<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Ubuntu Maker Canonical Generated Nearly $300M In Revenue Last Year</title><link>https://www.phoronix.com/news/Canonical-2024-Annual-Report</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 18:22:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A decade ago Canonical did around $81 million in revenue (2014) with a head count of around 337 at the company behind Ubuntu Linux while their Linux desktop efforts were still gaining a footing with OEMs/ODMs pre-loads, within enterprise desktop environments, and the lucrative server/cloud space. Canonical recently filed their 2024 annual report and they are now up to almost $300 million USD in revenue and a headcount of more than 1,100...]]></content:encoded></item><item><title>In just 3 months, CoreWeave CEO, once a crypto-mining bro, becomes a deca-billionaire</title><link>https://techcrunch.com/2025/06/26/in-just-3-months-coreweave-ceo-once-a-crypto-mining-bro-becomes-a-deca-billionaire/</link><author>Julie Bort</author><category>tech</category><pubDate>Thu, 26 Jun 2025 18:12:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[CoreWeave remains a symbol of the AI industry in 2025: Massive revenue, investor enthusiasm built on an insatiable need for more. ]]></content:encoded></item><item><title>Windows is Getting Rid of the Blue Screen of Death After 40 Years</title><link>https://tech.slashdot.org/story/25/06/26/1737218/windows-is-getting-rid-of-the-blue-screen-of-death-after-40-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 18:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Blue Screen of Death (BSOD) has held strong in Windows for nearly 40 years, but that's about to change. From a report: Microsoft revealed earlier this year that it was overhauling its BSOD error message in Windows 11, and the company has now confirmed that it will soon be known as the Black Screen of Death. The new design drops the traditional blue color, frowning face, and QR code in favor of a simplified black screen. 

The simplified BSOD looks a lot more like the black screen you'd see during a Windows update. But it will list the stop code and faulty system driver that you wouldn't always see during a crash dump. IT admins shouldn't need to pull crash dumps off PCs and analyze them with tools like WinDbg just to find out what could be causing issues. The company will roll out this new BSOD design in an update to Windows 11 "later this summer."]]></content:encoded></item><item><title>Threads now lets you manage Hidden Words separately from Instagram, set time limits</title><link>https://techcrunch.com/2025/06/26/threads-now-lets-you-manage-hidden-words-separately-from-instagram-set-time-limits/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 26 Jun 2025 17:57:46 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Threads, Meta’s competitor to X, now has its own Hidden Words settings that operates separately from Instagram.]]></content:encoded></item><item><title>Google Photos merges classic search with AI to speed up results</title><link>https://techcrunch.com/2025/06/26/google-photos-merges-classic-search-with-ai-to-speed-up-results/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 26 Jun 2025 17:55:05 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The AI feature, first introduced at Google's I/O developer conference last year, allows users to search across their collection of digital photos using natural language queries.]]></content:encoded></item><item><title>Malaysia Will Stop Accepting US Plastic Waste</title><link>https://news.slashdot.org/story/25/06/26/1726216/malaysia-will-stop-accepting-us-plastic-waste?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 17:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Malaysia will ban plastic waste imports from the U.S. starting Tuesday because of America's failure to abide by the Basel Convention treaty on international waste transfers, in a move that could have significant consequences for California. 

Malaysia emerged as a major destination for U.S. waste after China banned American waste imports in 2018. California shipped 864 shipping containers, or more than 10 million pounds of plastic waste, to Malaysia in 2024, according to the Basel Action Network, an advocacy group. That was second only to Georgia among U.S. states. 

Under Malaysian waste guidelines announced last month, the country will no longer accept plastic waste and hazardous waste from nations that didn't ratify the Basel Convention, the international treaty designed to reduce the international movement of hazardous and other waste. The U.S. is one of just a handful of countries, including Fiji and Haiti, that hasn't signed the pact.]]></content:encoded></item><item><title>Uber has Atlanta’s autonomous ride-hailing and delivery market on lock</title><link>https://techcrunch.com/2025/06/26/uber-has-atlantas-autonomous-ride-hailing-and-delivery-market-on-lock/</link><author>Rebecca Bellan</author><category>tech</category><pubDate>Thu, 26 Jun 2025 17:10:26 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Uber Eats customers in Atlanta can now opt in to have their food delivered via sidewalk delivery robots, following partner Serve Robotics’s launch on Thursday. The move comes just two days after Uber and Waymo launched a commercial robotaxi service in the city. ]]></content:encoded></item><item><title>Mesa 25.2 RADV Driver Merges Support For AV1 Vulkan Video Encode</title><link>https://www.phoronix.com/news/RADV-Merges-AV1-Encode</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 17:01:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Published last November as part of Vulkan 1.3.302 was the VK_KHR_video_encode_av1 extension for adding AV1 video encoding to the Vulkan Video API. Ahead of next quarter's Mesa 25.2 release, the open-source Radeon Vulkan driver "RADV" has merged its AV1 encode support...]]></content:encoded></item><item><title>Rivian cuts dozens on manufacturing team ahead of R2 launch</title><link>https://techcrunch.com/2025/06/26/rivian-cuts-dozens-on-manufacturing-team-ahead-of-r2-launch/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:58:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The company laid off around 1% of its total workforce, or around 140 employees. It's the third round of cuts since the start of 2024. ]]></content:encoded></item><item><title>Elon Musk reportedly fires Tesla’s top sales exec</title><link>https://techcrunch.com/2025/06/26/elon-musk-reportedly-fires-teslas-top-sales-exec/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:50:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Omead Afshar, one of Musk's top lieutenants, is out after more than seven years at the company. He was recently elevated to a VP role in late 2024.]]></content:encoded></item><item><title>Microsoft Moves Antivirus Software Out of Windows Kernel To Prevent CrowdStrike-Style Crashes</title><link>https://tech.slashdot.org/story/25/06/26/1650259/microsoft-moves-antivirus-software-out-of-windows-kernel-to-prevent-crowdstrike-style-crashes?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is preparing to release a private preview of Windows changes that will move antivirus and endpoint detection and response apps out of the Windows kernel, nearly a year after a faulty CrowdStrike update crashed 8.5 million Windows-based machines worldwide. 

The new Windows endpoint security platform is being developed in cooperation with CrowdStrike, Bitdefender, ESET, Trend Micro, and other security vendors. David Weston, Microsoft's vice president of enterprise and OS security, said dozens of partners have submitted papers detailing design requirements, some hundreds of pages long. The private preview will allow security vendors to request changes before the platform is finalized.]]></content:encoded></item><item><title>US, French authorities confirm arrest of BreachForums hackers</title><link>https://techcrunch.com/2025/06/26/us-french-authorities-confirm-arrest-of-breachforums-hackers/</link><author>Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:28:54 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Kai West was arrested in France, along with four other hackers, all suspected of being part of the well-known hacking forum, BreachForums.]]></content:encoded></item><item><title>Europe’s First Exascale Supercomputer Powers Up</title><link>https://spectrum.ieee.org/jupiter-exascale-supercomputer-europe</link><author>Michael Dumiak</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEwNjA1MS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3ODY2NzYyNX0.GYXAvGElUm_42AAuEc4HzE1Lsb-lbH1UjyW-5jEb2QU/image.jpg?width=600" length="" type=""/><pubDate>Thu, 26 Jun 2025 16:27:40 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Debuting at number 4 on the TOP500, JUPITER could open vast scientific vistas]]></content:encoded></item><item><title>Homeland Security warns of Iran-backed cyberattacks targeting US networks</title><link>https://techcrunch.com/2025/06/26/homeland-security-warns-of-iran-backed-cyberattacks-targeting-us-networks/</link><author>Zack Whittaker</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:20:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[DHS said low-level cyberattacks targeting U.S. networks are "likely" in the wake of military conflict between the U.S. and Israel and Iran.]]></content:encoded></item><item><title>Meta hires key OpenAI researcher to work on AI reasoning models</title><link>https://techcrunch.com/2025/06/26/meta-hires-key-openai-researcher-to-work-on-ai-reasoning-models/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:13:59 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[In recent months, Mark Zuckerberg has been on a hiring spree to build out Meta's new AI team, offering $100 million compensation packages to top researchers who join his company. ]]></content:encoded></item><item><title>As AI kills search traffic, Google launches Offerwall to boost publisher revenue</title><link>https://techcrunch.com/2025/06/26/as-ai-kills-search-traffic-google-launches-offerwall-to-boost-publisher-revenue/</link><author>Sarah Perez</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:04:10 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Offerwall lets publishers give their sites' readers a variety of ways to access their content, including through options like micropayments, taking surveys, watching ads, and more.]]></content:encoded></item><item><title>Who Needs Accenture in the Age of AI?</title><link>https://slashdot.org/story/25/06/26/165227/who-needs-accenture-in-the-age-of-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 16:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Accenture is facing mounting challenges as AI threatens to disrupt the consulting industry the company helped build. The Dublin-based firm, which made its fortune advising clients on adapting to new technologies from the internet to cloud computing, now confronts the same predicament as generative AI reshapes business operations. 

The company's new generative AI contracts slowed to $100 million in the most recent quarter, down from $200 million per quarter last year. Technology partners including Microsoft and SAP are increasingly integrating AI directly into their offerings, allowing systems to work immediately without extensive consulting support. Newcomers like Palantir are embedding their own engineers with customers, enabling clients to bypass traditional consultants. 

Between 2015 and 2024, Accenture generated a 370% total return by helping companies navigate technological transitions. The firm reached a $250 billion valuation in February before losing $60 billion in market value. CEO Julie Sweet insists that the company is reorganizing around "reinvention services." A recent survey found 42% of companies abandoned most AI initiatives, up from 17% a year ago.]]></content:encoded></item><item><title>Suno snaps up WavTool for its AI music editing tools amid ongoing dispute with music labels</title><link>https://techcrunch.com/2025/06/26/suno-snaps-up-wavtool-for-its-ai-music-editing-tools-amid-ongoing-dispute-with-music-labels/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:30:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[AI music company Suno acquired WavTool, a browser-based AI DAW. This aims to improve editing capabilities for musicians.]]></content:encoded></item><item><title>Why World&apos;s New &apos;Priority Lane&apos; for Humans on the Blockchain Could Change Everything</title><link>https://hackernoon.com/why-worlds-new-priority-lane-for-humans-on-the-blockchain-could-change-everything?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:28:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Can a Blockchain Truly Be for Humans First? World Is Betting on It.What if there were an internet where you, as a human, were always in the fast lane, never stuck in traffic behind armies of automated bots? This isn't a hypothetical question for World, the ambitious project formerly known as Worldcoin. With the launch of its blockchain, World Chain, the company has introduced a radical new concept: Priority Blockspace for Humans (PBH). It is a direct attempt to solve one of the most persistent problems in the crypto world, network congestion caused by bots, and to create a digital world where human users are the top priority. For its 13 million verified users, this could mean a fundamentally fairer and more reliable online experience.\
The core idea of blockchain technology was to create decentralized and open networks. However, anyone who has tried to make a transaction on popular blockchains like Ethereum during peak times knows the frustration. Transaction fees, known as gas fees, can skyrocket as users compete to have their transactions processed. This system often favors those with the most capital, including automated bots designed to snatch up limited digital goods or manipulate markets, pushing out regular users. World’s new system aims to flip this model on its head. Instead of a bidding war, it uses a person's verified humanity as the ultimate pass to the front of the line, ensuring that during high traffic, it is the human users who are served first, not the bots.\
This is made possible through World ID, the project's controversial yet core component. To get a World ID, a user must have their iris scanned by a physical device called an Orb. This process is designed to create a "proof of personhood," a digital verification that a user is a unique human being and not a bot or a duplicate account. With the launch of World Chain, this proof of personhood is now being used to grant privileged access. During periods of heavy network traffic, a specific portion of each block, the digital bundles of data that make up a blockchain, is reserved exclusively for transactions coming from these Orb-verified humans. This ensures that essential activities, from verifying one's identity to using applications on the network, remain smooth and inexpensive for people.A New Set of Rules for the Digital EconomyThe introduction of Priority Blockspace for Humans represents a significant technical and philosophical shift for blockchain infrastructure. It moves away from a purely economic model, where network priority is bought, to a model based on identity. "By granting real human transactions built-in priority during block production, Priority Blockspace for Humans effectively makes World Chain fairer and more efficient," said Steven Smith, Vice President of Engineering and Protocol at Tools for Humanity, a key contributor to the World project. This baked-in priority means that verified users are shielded from the wild fee swings that plague other networks, creating a more predictable and user-friendly environment.\
To understand the impact, consider the example of claiming a digital grant or purchasing a popular, limited-edition item on a traditional blockchain. In such a scenario, bots can be programmed to flood the network with transactions, driving up fees and making it nearly impossible for a human user to succeed without paying an exorbitant price. World Chain’s PBH system is designed to neutralize this threat. The reserved blockspace acts as a dedicated lane for humans, allowing their transactions to be processed quickly and without the extra cost, regardless of bot activity. "We firmly believe that humans and AI can coexist harmoniously," Smith added, "and this development reflects our commitment to building user-friendly systems that benefit humanity while also making the most of bleeding-edge technology.”\
The technology behind this was not built in a vacuum. It was tested on an incentivized testnet, a trial environment that rewards participants for stress-testing the system, with input from leading blockchain research and development firms like Flashbots and Alchemy. The core components, including PBH and Rollup Boost, a tool that allows for this custom ordering of transactions, have been made open-source. This means anyone can inspect the code, a move intended to build trust and encourage wider adoption. Furthermore, the entire system underwent an independent security audit by Nethermind, a respected blockchain auditing firm, to ensure its integrity and security.Building on a Foundation for a Billion UsersWorld Chain itself is what is known as a Layer-2 network. In simple terms, think of it as a highway built on top of a more secure, but slower, main road. The main road, in this case, is Ethereum, one of the largest and most secure blockchains. By processing transactions on its own "layer" and then bundling them to be secured by Ethereum, World Chain can offer significantly faster and cheaper transactions than Ethereum itself could handle alone. This architecture is built using the OP Stack, a standardized and open-source framework that is part of a larger vision for a "Superchain" ecosystem, an interconnected network of many Layer-2 blockchains that can communicate with each other.\
The ultimate goal for World is ambitious, to build a network that serves over a billion people. To achieve this, the platform is designed to support applications with real-world utility, focusing on areas like stablecoin-based finance, international remittances, and commerce. The integration with World ID's proof of personhood is central to this vision, as it aims to create a system where developers can build applications for a network of verified humans, reducing the risks of fraud and spam that are common in the digital world. Anyone with a compatible digital wallet can start exploring and using these applications, with the project’s own World App serving as the primary gateway.World's launch of Priority Blockspace for Humans is a genuinely novel experiment in the ongoing effort to make blockchain technology more equitable and accessible. The project addresses a real and frustrating problem for everyday users who are often outmaneuvered and outspent by bots. By creating a system where human identity, not economic power, dictates network priority, World is making a bold statement about the kind of digital future it wants to build. The focus on a fairer system is a commendable goal that could, if successful, set a new standard for how public blockchains operate.\
However, the project is not without its controversies, primarily centered around the privacy implications of its iris-scanning World ID system. While the Worldcoin Foundation maintains that the biometric data is handled securely and used only to generate a unique, anonymous verification, concerns about the centralization of such sensitive personal data persist. The success of World Chain is therefore intrinsically linked to its ability to earn and maintain public trust, not just in its technology but also in its stewardship of user data. The decision to make its core technology open-source and subject it to independent audits is a positive step in this direction.\
Ultimately, World is forcing a necessary conversation about who our digital infrastructure should serve. Is it for the highest bidder, or is it for the individual human user? While the road to a billion users is long and fraught with challenges, the launch of World Chain and its human-first priority system is a significant and fascinating step. It's an attempt to technically enforce a more level playing field, and the entire technology world will be watching to see if this radical idea can deliver on its promise of a fairer internet for all.Don’t forget to like and share the story! ]]></content:encoded></item><item><title>Study Finds LLM Users Have Weaker Understanding After Research</title><link>https://slashdot.org/story/25/06/26/1526210/study-finds-llm-users-have-weaker-understanding-after-research?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Researchers at the University of Pennsylvania's Wharton School found that people who used large language models to research topics demonstrated weaker understanding and produced less original insights compared to those using Google searches. 

The study, involving more than 4,500 participants across four experiments, showed LLM users spent less time researching, exerted less effort, and wrote shorter, less detailed responses. In the first experiment, over 1,100 participants researched vegetable gardening using either Google or ChatGPT. Google users wrote longer responses with more unique phrasing and factual references. A second experiment with nearly 2,000 participants presented identical gardening information either as an AI summary or across mock webpages, with Google users again engaging more deeply and retaining more information.]]></content:encoded></item><item><title>Mir 2.21 Released With Cursor Scaling &amp; Mouse Keys Support</title><link>https://www.phoronix.com/news/Mir-2.21-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:18:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Mir 2.21 is out today for this Ubuntu/Canonical project to serve as a set of libraries used to simplify the development of Wayland-based shells/environments...]]></content:encoded></item><item><title>Deribit And SignalPlus Launch “The Summer Chase” Trading Competition With a $300,000 USDC Prize Pool</title><link>https://hackernoon.com/deribit-and-signalplus-launch-the-summer-chase-trading-competition-with-a-$300000-usdc-prize-pool?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:05:39 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA["" is a five-week trading contest offering a total prize pool of over $300,000 in USDC, with additional prizes including a Tesla, a Rolex, a Hawaii vacation, and more. From leaderboard battles to referral contests and daily reward events, the campaign aims to bring together a combination of skill, strategy, and summer fun for traders around the world.Users who  by June 30th will receive 3 free Deribit options.By  a trader, users will have a chance to win a Tesla.Luuk Strijers, Chief Executive Officer at Deribit, shared: "We’re excited to launch the third edition of our Options Competition in partnership with SignalPlus. Following last year’s massive success and incredible momentum, we knew we had to take things to the next level. This year, we’re raising the stakes with bigger rewards, fresh challenges, and even more opportunities for traders to showcase their skills."Chris Yu, Co-Founder of SignalPlus, added: “We're proud to partner with Deribit to welcome traders into the next exciting iteration of our summer trading competition. At SignalPlus, we are firm believers of crypto options being at the forefront of the next innovation wave, granting users ever expanding control of their digital asset portfolios. Together with Deribit's leading platform, we're delivering an experience that rewards skill, fosters learning, and energizes derivatives trading for the betterment of the community. As with our past collaboration successes, we are excited to see what this round of seasoned professionals and newcomers will achieve in the hot summer months, and look forward to interacting with all participants in the very near future.”Attractive Prize Pool: Compete on both individual and team leaderboards for a chance to win USDC cash rewards and additional prizes. From Tesla to Rolex to Hawaii travel to physical gold, 'The Summer Chase' should have something in store for everyone!Palm Throne Tournament: Climb the rankings each week by trading options and futures on Deribit with SignalPlus. Top traders earn weekly rewards with a twist - 1st place & odd-numbered rankers will receive bonus rewards!Island League Showdown - Team Competition: Team up and compete to win up to 1,000 USDC weekly, with 20% going to team leaders. All team members must meet the individual minimum trading volume for overall qualification.Big Referral Prizes: Invite and refer top-performing traders to the competition for a chance to win a Tesla or the equivalent of 30,000 in USDC.Upsize your referral bonuses by inviting additional registered users to earn up to 3,500 USDC, as well as jackpot prizes including a Rolex Submariner, Skydive Dubai entry, F1 Grand Prix tickets, and more.Participation Rewards & Daily Lucky Draws: Activity-based rewards will be granted to participants for making your 1st trade, social media contests, weekend boosts, and other challenges. Earn participation rewards without worrying about your rankings! Furthermore, daily events will be held to incentivize active traders with guaranteed USDC awards, while qualified teams will have a chance to win a trip to Hawaii through regular lucky draws.Cassette Rewards & Symbol Game: Daily trading activity unlocks cassette tokens for guaranteed and limited USDC rewards. Special symbol-based mechanics offer the chance to win a Hawaii trip and extra bonuses for teams.Lifestyle Prizes: From designer sunglasses and Apple Watches to travel giveaways and high-end electronics, The Summer Chase offers something for everyone.Learning & Engagement: Traders will have the opportunity to upskill their trading knowledge through weekly engagement events, prediction challenges, and social activities. Learn while you earn! invites participants into a dynamic trading competition that combines strategic trading with seasonal celebration. With every trade, contenders edge closer to exclusive summer rewards. The event begins today. is a centralized, institutional-grade provider of crypto derivatives ecosystem, specializing in Bitcoin and Ethereum options and futures. With state-of-the-art infrastructure, Deribit offers instantaneous price discovery, low-latency execution, advanced risk mitigation tools, and deep liquidity through a network of top-tier market makers. Deribit facilitates the majority of global crypto options volume and upholds rigorous proof-of-reserves practices to maintain the highest standards of integrity and transparency. delivers a comprehensive options trading suite tailored for crypto derivatives traders. Access an advanced suite of tools to perform delta hedging, risk analysis, P&L attribution, and multi-leg execution with minimal slippage via our dashboard. Tap into exchange liquidity via listed order books or bilateral block-trade requests with institutional counterparties. Users can manage portfolio risk through an integrated suite of scenario-based risk simulations, supported by automated hedging functions and real-time Telegram alerts to help monitor exposures.Deribit FZE is licensed by the VARA to provide Virtual Asset Exchange Services and does not accept or service retail clients. DRB Panama Inc. is not regulated and services both retail and non-retail clients. Virtual Assets are subject to extreme market volatility, involve a high degree of risk, and can lose value, in part or in full. Investing in Virtual Assets may result in you losing your entire investment.Virtual Assets are not insured against potential losses and are not protected by any form of financial protection whatsoever. Participants onboarded as clients under Deribit FZE (Qualified Investors) are eligible to win up to 5,000 USD in total throughout the competition period. They are not eligible for high-value rewards such as the Tesla or Rolex prizes. Clients registered via DRB Panama Inc. are eligible for the full prize pool, including the Tesla, Rolex, and all other promotional rewards.:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>PepeDollar: GameFi and Real Use Rewards</title><link>https://hackernoon.com/pepedollar-gamefi-and-real-use-rewards?source=rss</link><author>Flash PR</author><category>tech</category><pubDate>Thu, 26 Jun 2025 15:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Here’s why  stands out as the meme coin that blends real use cases with community trust and scalable tech.GameFi and Real Use Rewards: Pepe Dollar (PEPD) Steps AheadPepe Dollar (PEPD)’s GameFi rewards system incentivizes holding and active participation, driving token velocity and user engagement. Brettcoin (BRETT) lacks such robust incentives, while Pepecoin (PEPE) offers limited utility beyond speculation. Explore the growing ecosystem and GameFi rewards at .Whale investors have started migrating from Brettcoin (BRETT) to Pepe Dollar (PEPD), seeking projects with clearer long-term value. Pepe Dollar (PEPD) presale gains have seen stronger community support and strategic partnerships, reinforcing investor trust. Secure your position now by joining the Pepe Dollar (PEPD) presale.Market experts forecast Pepe Dollar (PEPD) to reach $5 or more as its ecosystem expands, driven by game-changing technology and sound tokenomics. Brettcoin (BRETT) and Pepecoin (PEPE) may deliver short-term rallies but lack Pepe Dollar (PEPD)’s growth engine. Stay updated with the latest developments at the Pepe Dollar (PEPD) blog.While Brett coin (BRETT) exploded onto the scene with impressive rallies and a strong Base blockchain backing, its inflated token supply and speculative volatility have put long-term growth in question. Brettcoin (BRETT) has yet to deliver on mechanisms like token burns or meaningful GameFi rewards to sustain momentum.Pepecoin (PEPE) remains a beloved meme but struggles with its unlimited token supply and lack of integration with DeFi or payment infrastructure. This limits Pepecoin’s (PEPE) ability to convert hype into sustained investor profits.Pepe Dollar (PEPD): The Meme Coin Built for the FutureUnlike Brettcoin (BRETT) and Pepecoin (PEPE), Pepe Dollar (PEPD) incorporates a capped supply, a 29% federal burn to increase scarcity, and a Layer-2 Ethereum (ETH) base that offers scalability and low fees. These fundamentals attract profit-focused investors who value sustainability. Learn more about Pepe Dollar (PEPD) and its tokenomics on the official .For investors weighing their options in the meme coin space, Pepe Dollar (PEPD) offers a smarter balance of scarcity, utility, and community-driven growth. ]]></content:encoded></item><item><title>Sia x HackerNoon: Inviting Developers to Build the Future of Decentralized Cloud Storage</title><link>https://hackernoon.com/sia-x-hackernoon-inviting-developers-to-build-the-future-of-decentralized-cloud-storage?source=rss</link><author>Sia Foundation</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:51:57 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Through this collaboration, Sia becomes the decentralized storage backbone for over 100,000 HackerNoon stories—and counting.

By backing up every existing and future HackerNoon article on the Sia network, we’re helping ensure that tech journalism remains permanently accessible, censorship-resistant, and free from centralized control. Over 2,000 new stories will be added to Sia each month, showcasing just how scalable and reliable decentralized infrastructure can be.]]></content:encoded></item><item><title>D’CENT Wallet Launches Third ‘Tap That Drop’ Campaign With Taiko</title><link>https://hackernoon.com/dcent-wallet-launches-third-tap-that-drop-campaign-with-taiko?source=rss</link><author>BTCWire</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:50:44 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[has announced the launch of the third campaign in its seasonal Web3 initiative, Tap That Drop, featuring Taiko, an Ethereum Layer 2 protocol known for its based sequencing approach. The campaign began on May 27 and has already seen participation from tens of thousands of users across its first two rounds.Developed by Korean blockchain security company IoTrust, D’CENT is widely recognized as a pioneer in the hardware wallet space. It introduced biometric authentication to the market in 2018 and has since expanded its global user base to more than 200 countries. The wallet supports more than 85 blockchain networks and 4,500 tokens. Now, D’CENT is evolving beyond secure storage, aiming to become a full-featured and intuitive Web3 platform that integrates portfolio tools, ecosystem discovery, and reward-based participation.To support this shift,  launched Tap That Drop — the first seasonal quest campaign of its kind embedded in a wallet. Over several months, twelve blockchain projects will each run a two-week campaign featuring interactive quests, project-specific NFTs, and token-based rewards. Users who collect all twelve NFTs will qualify for a Mega Airdrop after season 1 ends. The 2024 edition received strong engagement from the Web3 community, with many participants highlighting the value of the rewards.The campaign has also served as a way for participating projects to introduce their ecosystems to new users and drive deeper interaction.The format has already proven effective. The first round with Skate attracted more than 20,000 participants, with rewards fully claimed within few hours of launch. The second round with MAP Protocol saw even faster response, with all rewards claimed in less than two hours. Across both campaigns, biometric hardware wallet users received higher-tier rewards, and completing a quest unlocked access to wallet discounts — creating additional value for both new and existing users.The third partner in the campaign is Taiko, an Ethereum-equivalent Layer 2 rollup focused on scalability without compromising decentralization. The protocol uses based sequencing for community-centric block production. Since its mainnet launch, where Ethereum co-founder Vitalik Buterin proposed the first block, Taiko has been listed on Binance Alpha as one of the platform’s top ten traded tokens and has kicked off Trailblazers Season 5 with 10,000 TAIKO in liquidity rewards.“We believe wallets are no longer just for holding crypto. They’re evolving into platforms for exploration and rewards,” said Sangsu Baek, CEO of IoTrust. “Through Tap That Drop and upcoming features like real-time portfolio tools, we’re building a new kind of user experience, where security and participation go hand in hand. That’s what sets D’CENT apart from other wallets in the space.”The Taiko campaign within Tap That Drop begins on June 25 in the D’CENT app. Participants can complete quests, mint a Taiko-themed NFT from the Star Collection, and claim token-based rewards. Hardware wallet users will continue to receive higher reward tiers and access to exclusive discounts.👉 Download D’CENT App and join the quest:  isn’t just a wallet—it’s where security meets opportunity.Developed by IoTrust, D’CENT is the world’s first biometric hardware wallet, built with SE (Secure Element) & TEE (Trusted Execution Environment) technology to ensure bank-grade security for your digital assets.But D’CENT goes beyond security. Unlike traditional wallets that simply store crypto, D’CENT empowers users to do more—unlocking rewards, accessing exclusive benefits, and seamlessly interacting with Web3 services. As the only wallet offering both hardware and software solutions, D’CENT provides the flexibility to secure, earn, and engage, all in one place.:::tip
This story was published as a press release by Btcwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Why is edwin Betting on Chat to Solve DeFi&apos;s Biggest Problem?</title><link>https://hackernoon.com/why-is-edwin-betting-on-chat-to-solve-defis-biggest-problem?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:47:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Can a Simple Conversation Unlock the Future of Decentralized Finance?What if interacting with the complex world of decentralized finance was as simple as sending a text message? For years, the promise of DeFi, a parallel financial system built on blockchain technology, has been available to a select few who could navigate its steep learning curve. Now, an emerging web3 startup edwin has publicly launched an AI terminal, a move that signals a potential shift in how people interact with this emerging financial landscape. The release aims to transform complex financial transactions into simple conversations, potentially opening the doors of DeFi to a much broader audience.The traditional gateway to DeFi has been the crypto wallet. While essential for holding assets, these wallets often present users with a daunting array of options for swapping tokens or earning yield, with interfaces that can be unintuitive for newcomers. This complexity has created a significant barrier to entry. AI is now being introduced as a new layer to simplify this experience. By integrating artificial intelligence, the goal is to translate human language into machine-executable commands on the blockchain. The edwin terminal is designed to be this bridge, giving users a conversational way to manage their investments through intelligent agents.A New Gateway for the Next Wave of UsersThe edwin AI terminal provides a user interface that connects to various DeFi applications through a chat window. After a user connects their existing web wallet, they can begin typing commands in natural language to a smart AI agent. This agent then interprets these commands to access a wide range of on-chain protocols for trading, lending, or generating yield. The process is designed to remove the multiple, often confusing steps typically required to perform such actions, replacing them with a single instruction.This approach is tailored for both beginner and intermediate users who understand the basic premise of DeFi but are often deterred by the intricate user experience. A key aspect of the design is that users retain full control over their digital assets. The AI agent acts as an intermediary that can only execute actions based on direct and explicit user commands. This is a crucial distinction in an industry where security and ownership are paramount. This model is often referred to as "non-custodial," meaning the user, and only the user, holds the cryptographic keys to their funds. edwin does not take possession of user assets at any point.Liran Markin, the Chief Executive Officer of edwin, articulated the vision behind the launch. "We believe that DeFi should be as easy as chatting," Markin stated. "The new edwin terminal is our way of making onchain finance finally accessible to the average user. It’s secure, powerful, and highly intuitive." This sentiment points to a broader industry challenge. Research indicates that a significant number of new users, approximately 65 percent, abandon decentralized applications after their first interaction. This high rate of disengagement is largely attributed to poor onboarding experiences and confusing interfaces, which has stalled mainstream adoption of DeFi.The Technology Behind the ConversationTo function, the terminal's AI agent utilizes a combination of technologies. It integrates LangChain, a framework for developing applications powered by language models, and Anthropic, an AI safety and research company. These are coupled with edwin’s proprietary MCP tooling, which is specifically designed for executing DeFi transactions. This technological stack allows the AI to understand user requests and interact with a curated list of established DeFi services. It aggregates fundamental financial activities like trading, lending, and yield farming into one unified interface.Gal Wiernik, the Chief Technology Officer at edwin, explained the technical goal. "We’ve built this terminal for the next wave of crypto users," Wiernik said. "It combines wallet security, AI reasoning, and real DeFi tools, all from your browser." The intention is to abstract away the complexity without sacrificing functionality. Instead of needing to visit multiple websites, understand different user interfaces, and manually connect a wallet to each one, a user could theoretically ask the edwin agent to find the best yield for a specific asset, and the agent would then present and execute the optimal strategy upon approval.The core value proposition is the simplification of a fragmented and often intimidating ecosystem. By creating a single point of interaction that speaks a user's language, the terminal aims to empower individuals to explore financial opportunities across different blockchains, a concept known as omnichain, with greater clarity. The AI is not just a chatbot, but an execution tool that leverages large language models to reason and act on financial instructions within the secure environment of the user's own wallet.The launch of the edwin AI terminal represents a logical and perhaps necessary evolution in the DeFi space. The technology's core promise has always been one of democratization, yet its practical application has been anything but democratic. The user experience has been the single greatest impediment to widespread adoption. While early adopters were willing to tolerate clunky interfaces and a high degree of technical knowledge, the next hundred million users will not.Edwin's approach of using a conversational AI interface is a compelling solution. It leans into a user behavior that is already universal: chatting. If the execution is as seamless as the concept, it could significantly lower the barrier to entry. The non-custodial design is critical; without it, the product would be a non-starter for the security-conscious crypto community. The true test will be in the AI's reliability and the breadth of its integrations. Can it truly understand the nuances of a user's intent and execute transactions flawlessly and optimally across a wide range of protocols? The potential for error, misunderstanding, or exploitation in an AI-mediated financial transaction must be infinitesimally small.Ultimately, the success of edwin and similar platforms will depend on building trust. Users must trust not only the security of the underlying code but also the reasoning capabilities of the AI. The company's challenge will be to prove its robustness and safety over time. If it succeeds, the concept of a DeFi "agent" could become the standard, moving the industry away from manual clicks and toward goal-oriented instructions. This is a meaningful step toward fulfilling DeFi's original mission of creating a more open and accessible financial system. The journey is far from over, but this is a significant development to watch.:::tip
Vested Interest Disclosure:This author is an independent contributor publishing via our . HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO]]></content:encoded></item><item><title>CareerBuilder + Monster, Which Once Dominated Online Job Boards, File For Bankruptcy</title><link>https://slashdot.org/story/25/06/26/1356207/careerbuilder--monster-which-once-dominated-online-job-boards-file-for-bankruptcy?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[CareerBuilder + Monster, which once dominated the online recruitment industry, filed for Chapter 11 bankruptcy protection this week and said it plans to sell its businesses. From a report: Created through the September merger of CareerBuilder and Monster, the Chicago-based company said it agreed to sell its job board operations, its most recognizable business, to JobGet, which has an app for so-called gig workers.]]></content:encoded></item><item><title>Oracle Linux 10 Reaches GA, Available With Unbreakable Enterprise Kernel 8.1</title><link>https://www.phoronix.com/news/Oracle-Linux-10-GA</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:39:07 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Building off the release of Red Hat Enterprise Linux 10 (RHEL 10.0) just over one month ago, Oracle today announced the general availability of Oracle Linux 10.0...]]></content:encoded></item><item><title>Jeff Bezos is reportedly courting Trump after public spat with Musk</title><link>https://techcrunch.com/2025/06/26/jeff-bezos-is-reportedly-courting-trump-after-public-spat-with-musk/</link><author>Sean O&apos;Kane</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:35:21 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Bezos allegedly sees an opening for his space company, Blue Origin, now that Musk has been diminished in the eyes of the president.]]></content:encoded></item><item><title>People use AI for companionship much less than we’re led to believe</title><link>https://techcrunch.com/2025/06/26/people-use-ai-for-companionship-much-less-than-were-led-to-believe/</link><author>Ram Iyer</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:21:28 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A report by Anthropic reveals that people rarely seek companionship from AI, and turn to AI for emotional support or advice only 2.9% of the time. ]]></content:encoded></item><item><title>YouTube adds an AI Overviews-like search results carousel</title><link>https://techcrunch.com/2025/06/26/youtube-adds-an-ai-overviews-like-search-results-carousel/</link><author>Aisha Malik</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:08:38 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[YouTube is rolling out new AI-powered features to help users find content and information more easily, the company announced on Thursday. ]]></content:encoded></item><item><title>5 Ways to Automate Security and Compliance in a Cloud-First World</title><link>https://hackernoon.com/5-ways-to-automate-security-and-compliance-in-a-cloud-first-world?source=rss</link><author></author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:06:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Most breaches happen due to delayed response. This article outlines 5 essential ways to automate security and compliance to protect your business continuously.]]></content:encoded></item><item><title>The Violinist Who Fell in Love With Machine Learning</title><link>https://spectrum.ieee.org/violinist-to-software-engineer</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEwNjI1NS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxMTE5NDgxOH0.CRr1Ii2b4e1LshGhO8SZqQBKhjyqTfDKxiuBJfamzJo/image.png?width=600" length="" type=""/><pubDate>Thu, 26 Jun 2025 14:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Those with nontechnical backgrounds can succeed in software]]></content:encoded></item><item><title>Meet Pepe Dollar: DeFi Integration and Institutional Appeal</title><link>https://hackernoon.com/meet-pepe-dollar-defi-integration-and-institutional-appeal?source=rss</link><author>Flash PR</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Ethereum (ETH) remains the backbone of the decentralized finance revolution, hosting everything from lending protocols to NFTs. Yet, the network faces a fundamental challenge—token inflation and scalability—impacting the potential for meme coins to gain serious institutional traction. Enter , a Layer-2 meme coin designed to solve these issues with a fixed supply and real DeFi utilities, offering what even Ethereum struggles to provide.The Inflation Problem Ethereum and Meme Coins FaceEthereum’s growing ecosystem has brought explosive innovation but also growing pains. Native ETH inflation, coupled with meme coins like Pepecoin (PEPE) having uncapped supplies, dilutes value and raises concerns among institutional investors who prioritize economic discipline and long-term stability. directly addresses this by imposing a fixed max supply of 3.695 billion tokens—mirroring a satirical critique of the $36.95 trillion US debt. This hard cap makes PEPD a deflationary asset, appealing to institutions wary of inflationary risk prevalent in both ETH and most meme coins, including Pepecoin (PEPE).DeFi Integration and Institutional AppealBeyond tokenomics,  integrates real DeFi functionality and payment solutions:Staking and Yield: Token holders can stake PEPD for rewards, aligning incentives with ecosystem growth and investor returns.PayFi Payments: Combining decentralized finance with user-friendly payment infrastructure, Pepe Dollar enables real-world, censorship-resistant transactions.On-Chain Governance: Institutions favor projects where governance is transparent and community-driven, and PEPD’s roadmap includes voting mechanisms empowering holders.These utilities set Pepe Dollar apart from meme coins like Pepecoin (PEPE), which remain largely speculative without layered financial products.Layer-2 Scaling and Cross-Chain ReadinessEthereum’s congestion and gas fees are notable barriers for institutional adoption.  operates as a Layer-2 payment infrastructure, drastically lowering transaction costs and improving speed—critical factors for high-volume trading and institutional participation.Additionally,  is designed for cross-chain expansion across EVM-compatible chains such as Arbitrum and Polygon, enhancing liquidity and accessibility in a multi-chain DeFi landscape.Why Institutions Are Eyeing Pepe Dollar Over Pepecoin and EthereumFixed Supply vs. Inflation: Institutions prize predictable supply economics. PEPD’s capped supply contrasts with Ethereum’s native inflation and Pepecoin’s (PEPE) uncapped tokens.Real Use Cases vs. Hype: Pepe Dollar combines meme culture with payment infrastructure and DeFi tools, making it a practical asset rather than just a viral token.Community & Transparency: Robust tokenomics and audited contracts elevate trust, crucial for institutional capital.Get Involved with Pepe Dollar TodayInvestors seeking exposure to the future of meme-driven institutional DeFi should act now. The  is open, offering early access and rewards. Joining the official website and Telegram community ensures timely information on ecosystem developments.]]></content:encoded></item><item><title>Salesforce CEO Says 30% of Internal Work Is Being Handled by AI</title><link>https://slashdot.org/story/25/06/26/1316242/salesforce-ceo-says-30-of-internal-work-is-being-handled-by-ai?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Salesforce chief executive Marc Benioff said Thursday his company has automated a significant chunk of work with AI, another example of a firm touting labor-replacing potential of the emerging technology. From a report: "AI is doing 30% to 50% of the work at Salesforce now," Benioff said in an interview, pointing at job functions including software engineering and customer service. 

[...] Salesforce has said that use of AI internally has allowed it to hire fewer people. The San Francisco-based software company is focused on selling an AI product that promises to handle tasks such as customer service without human supervision. Benioff said that tool has reached about 93% accuracy, including for large customers such as Walt Disney.]]></content:encoded></item><item><title>Jon McNeill brings the operator’s playbook to TechCrunch All Stage</title><link>https://techcrunch.com/2025/06/26/jon-mcneill-brings-the-operators-playbook-to-techcrunch-all-stage/</link><author>TechCrunch Events</author><category>tech</category><pubDate>Thu, 26 Jun 2025 14:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[At TechCrunch All Stage 2025, Jon McNeill, CEO and co-founder of DVx Ventures, will take the Scale Stage to flip the script on conventional startup growth advice.]]></content:encoded></item><item><title>AI Improves at Improving Itself Using an Evolutionary Trick</title><link>https://spectrum.ieee.org/evolutionary-ai-coding-agents</link><author>Matthew Hutson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTA5ODI3OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTc2MzU3NH0.vGFSx4mspqLVTKLAyJIpHbXbsXx1MeIkCl8Bk2qfEnE/image.jpg?width=600" length="" type=""/><pubDate>Thu, 26 Jun 2025 13:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Researchers use evolutionary algorithms to enhance AI coding skills]]></content:encoded></item><item><title>Fannie Mae, Freddie Mac Ordered To Consider Crypto As an Asset When Buying Mortgages</title><link>https://slashdot.org/story/25/06/26/0048249/fannie-mae-freddie-mac-ordered-to-consider-crypto-as-an-asset-when-buying-mortgages?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Jun 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from the Associated Press: The head of the federal government agency that oversees Fannie Mae and Freddie Mac wants the mortgage giants to consider accepting a homebuyer's cryptocurrency holdings in their criteria for buying mortgages from banks. William Pulte, director of the Federal Housing Finance Agency, which oversees Fannie and Freddie, ordered the agencies Wednesday to prepare a proposal for consideration of crypto as an asset for reserves when they assess risks in single-family home loans.
 
Pulte also instructed the agencies that their mortgage risk assessments should not require cryptocurrency assets to be converted to U.S. dollars. And only crypto assets that "can be evidenced and stored on a U.S.-regulated centralized exchange subject to all applicable laws" are to be considered by the agencies in their proposal, Pulte wrote in a written order, effective immediately. Pulte was sworn in as the head of FHFA in March. Public records show that as of January 2025, Pulte's spouse owned between $500,000 and $1 million of bitcoin and a similar amount of Solana's SOL token. [...]
 
The policy change is meant to encourage banks to expand how they gauge borrowers' creditworthiness, in hopes that more aspiring homebuyers can qualify for a home loan. It also recognizes that cryptocurrencies have grown in popularity as an alternative to traditional investments, such as bonds and stocks. The agencies have to come up with their proposals "as soon as reasonably practical," according to the order. "This is a big win for advocates of cryptocurrencies who want crypto to be treated the same way as other assets are," said Daryl Fairweather, chief economist at Redfin.
 
Currently, stock investments are treated as qualifying assets that count toward reserves that banks want borrowers to have. But assets that are more volatile, like individual stocks or crypto, may be discounted by lenders, Fairweather noted. "As long as lenders are appropriately discounting crypto based on volatility, it's fine that crypto investments count toward reserves," she said.
 
Danielle Hale, chief economist at Realtor.com, added: "If Fannie and Freddie are going to accept cryptocurrency as collateral, that's a strong incentive for banks to shift their practices. Because people who might otherwise have to sell cryptocurrency to qualify -- and maybe that's a deal-breaker for them now -- under this new policy, they can qualify. It sort of expands the potential pool of eligible buyers."]]></content:encoded></item><item><title>Firefox 141 Beta Lowering RAM Use On Linux But Still Benchmarking Behind Chrome</title><link>https://www.phoronix.com/review/firefox-141-linux-ram</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 12:41:31 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Following this week's release of Firefox 140, Firefox 141 was promoted to beta. Most exciting for Linux users with next month's Firefox 141 release is finally lowering system RAM use! I've been running some benchmarks looking at the impact.]]></content:encoded></item><item><title>Superconducting Motor Could Propel Electric Aircraft</title><link>https://spectrum.ieee.org/electric-aircraft-motor-hinetics</link><author>Glenn Zorpette</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTA5MDc1OS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NDM2ODY5Nn0.jSv_WVuS4E1S5V0iq1xXRsMDALvP4U3fksuBmU_awlY/image.jpg?width=600" length="" type=""/><pubDate>Thu, 26 Jun 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Prototype unit from startup Hinetics uses a high-temperature superconductor]]></content:encoded></item><item><title>Ubuntu 25.10 Snapshot 2 Released</title><link>https://www.phoronix.com/news/Ubuntu-25.10-Snapshot-2</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 11:47:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Last month Canonical announced plans for releasing monthly Ubuntu Linux development snapshots and was followed by the Questing Snapshot 1 release in the road toward Ubuntu 25.10. Out today is the Questing Snapshot 2 release for incorporating the latest Ubuntu 25.10 development changes...]]></content:encoded></item><item><title>An AI Agent That Interprets Papers So You Don’t Have To: Full Build Guide</title><link>https://hackernoon.com/an-ai-agent-that-interprets-papers-so-you-dont-have-to-full-build-guide?source=rss</link><author>Superlinked</author><category>tech</category><pubDate>Thu, 26 Jun 2025 11:45:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Learn how to build an AI agent for Research Paper Retrieval, Search, and SummarizationFor researchers, staying updated with the latest findings is akin to finding a needle in a haystack. Imagine an AI-powered assistant that not only retrieves the most relevant papers but also summarizes key insights and answers your specific questions, all in real-time.This article delves into constructing such an AI research agent using Superlinked's complex document embedding capabilities. By integrating semantic and temporal relevance, we eliminate the need for complex reranking, ensuring efficient and accurate retrieval of information.Build a real-time AI research agent using Superlinked's vector search. It skips complex RAG pipelines by embedding and querying documents directly—making research faster, simpler, and smarter.(Want to jump straight to the code? Check out the open source on GitHub . Ready to try semantic search for your own agentic use case? )This article shows how to build an agent system using a Kernel agent to handle queries. If you want to follow along and run the code in-browser, Where to start building a research assistant system?Traditionally, building such a system involves complexity and considerable resource investment. Search systems typically retrieve an initial broad set of documents based on relevance and subsequently apply a secondary reranking process to refine and reorder results. While reranking enhances accuracy, it significantly increases computational complexity, latency, and overhead due to the extensive data retrieval initially required. Superlinked addresses this complexity by combining structured numeric and categorical embeddings with semantic text embeddings, providing comprehensive multimodal vectors. This method significantly enhances search accuracy by preserving attribute-specific information within each embedding.Build an agentic system with SuperlinkedThis AI agent can do three main things:Find Papers: Search for research papers by topic (e.g. “quantum computing”) and then rank them by relevance and recency.Summarize papers: Condense the retrieved papers into bite-sized insights.Answer questions: Extract answers directly from the specific research papers based on targeted user queries.Superlinked eliminates the need for re-ranking methods as it improves the vector search relevance. Superlinked's RecencySpace will be used which specifically encodes temporal metadata, prioritizing recent documents during retrieval, and eliminating the need for computationally expensive reranking. For example, if two papers have the same relevance - the one that is most recent will rank higher.To make things easier and more modular, I created an Abstract Tool class. This will simplify the process of building and adding toolsimport pandas as pd
import superlinked.framework as sl
from datetime import timedelta
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import os
from abc import ABC, abstractmethod
from typing import Any, Optional, Dict
from tqdm import tqdm
from google.colab import userdata

# Abstract Tool Class
class Tool(ABC):
    @abstractmethod
    def name(self) -> str:
        pass

    @abstractmethod
    def description(self) -> str:
        pass

    @abstractmethod
    def use(self, *args, **kwargs) -> Any:
        pass


# Get API key from Google Colab secrets
try:
    api_key = userdata.get('OPENAI_API_KEY')
except KeyError:
    raise ValueError("OPENAI_API_KEY not found in user secrets. Please add it using Tools > User secrets.")

# Initialize OpenAI Client
api_key = os.environ.get("OPENAI_API_KEY", "your-openai-key")  # Replace with your OpenAI API key
if not api_key:
    raise ValueError("Please set the OPENAI_API_KEY environment variable.")

client = OpenAI(api_key=api_key)
model = "gpt-4"
Step 2 : Understanding the DatasetThis example uses a dataset containing approximately 10,000 AI research papers available on Kaggle. To make it easy, simply run the cell below, and it will automatically download the dataset to your working directory. You may also use your own data sources, such as research papers or other academic content. If you decide to do so, all you need to do is adjust the schema design slightly and update the column names.import pandas as pd

!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1FCR3TW5yLjGhEmm-Uclw0_5PWVEaLk1j' -O arxiv_ai_data.csv
\
For now, to make things run a bit quicker, we will use a smaller subset of the papers just to speed things up but feel free to try the example using the full dataset. An important technical detail here is that the timestamps from the dataset will be converted from string timestamps (like '1993-08-01 00:00:00+00:00') into pandas datetime objects. This conversion is necessary because it allows us to perform date/time operations.df = pd.read_csv('arxiv_ai_data.csv').head(100)

# Convert to datetime but keep it as datetime (more readable and usable)
df['published'] = pd.to_datetime(df['published'])

# Ensure summary is a string
df['summary'] = df['summary'].astype(str)

# Add 'text' column for similarity search
df['text'] = df['title'] + " " + df['summary']
Debug: Columns in original DataFrame: ['authors', 'categories', 'comment', 'doi', 'entry_id', 'journal_ref' 'pdf_url', 'primary_category', 'published', 'summary', 'title', 'updated']
Understanding the Dataset ColumnsBelow is a brief overview of the key columns in our dataset, which will be important in the upcoming steps:: The publication date of the research paper.: The abstract of the paper, providing a concise overview.: The unique identifier for each paper from arXiv.For this demonstration, we specifically focus on four columns: , , , and . To optimize retrieval quality, the title and summary are combined into a single, comprehensive text column, which forms the core of our embedding and search process.A Note on Superlinked’s In-Memory Indexer : Superlinked’s in-memory indexing stores our dataset directly in RAM, making retrieval exceptionally fast which is ideal for real-time searches and rapid prototyping. For this proof-of-concept with 1,000 research papers, leveraging an in-memory approach significantly enhances query performance, eliminating delays associated with disk access.Step 3 : Defining the Superlinked SchemaTo move ahead, there is a need for schema to map our data. We have set up  with key fields:lass PaperSchema(sl.Schema):
    text: sl.String
    published: sl.Timestamp  # This will handle datetime objects properly
    entry_id: sl.IdField
    title: sl.String
    summary: sl.String

paper = PaperSchema()
Defining Superlinked Spaces for Effective RetrievalAn essential step in organizing and effectively querying our dataset involves defining two specialized vector spaces: TextSimilaritySpace and RecencySpace.The  is designed to encode textual information—such as the titles and abstracts of research papers into vectors. By converting text into embeddings, this space significantly enhances the ease and accuracy of semantic searches. It is optimized specifically to handle longer text sequences efficiently, enabling precise similarity comparisons across documents.text_space = sl.TextSimilaritySpace(
    text=sl.chunk(paper.text, chunk_size=200, chunk_overlap=50),
    model="sentence-transformers/all-mpnet-base-v2"
)
The  captures temporal metadata, emphasizing the recency of research publications. By encoding timestamps, this space assigns greater significance to newer documents. As a result, retrieval results naturally balance content relevance with publication dates, favoring recent insights.recency_space = sl.RecencySpace(
    timestamp=paper.published,
    period_time_list=[
        sl.PeriodTime(timedelta(days=365)),      # papers within 1 year
        sl.PeriodTime(timedelta(days=2*365)),    # papers within 2 years
        sl.PeriodTime(timedelta(days=3*365)),    # papers within 3 years
    ],
    negative_filter=-0.25
)
\
Think of RecencySpace as a time-based filter, similar to sorting your emails by date or viewing Instagram posts with the newest ones first. It helps answer the question, 'How fresh is this paper?'Smaller timedeltas (like 365 days) allow for more granular, yearly time-based rankings.Larger timedeltas (like 1095 days) create broader time periods.The  penalizes very old papers. To explain it more clearly, consider the following example where two papers have identical content relevance, but their rankings will depend on their publication dates.Paper A: Published in 1996 
Paper B: Published in 1993

Scoring example:
- Text similarity score: Both papers get 0.8
- Recency score:
  - Paper A: Receives the full recency boost (1.0)
  - Paper B: Gets penalized (-0.25 due to negative_filter)

Final combined scores:
- Paper A: Higher final rank
- Paper B: Lower final rank
\
These spaces are key to making the dataset more accessible and effective. They allow for both content-based and time-based searches, and really helpful in understanding the relevance and recency of research papers. This provides a powerful way to organize and search through the dataset based on both the content and the publication time.Step 4 : Building the indexNext, the spaces are fused into an index which is the search engine's core:paper_index = sl.Index([text_space, recency_space])Then the DataFrame is mapped to the schema and loaded in batches (10 papers at a time) into an in-memory store:# Parser to map DataFrame columns to schema fields
parser = sl.DataFrameParser(
    paper,
    mapping={
        paper.entry_id: "entry_id",
        paper.published: "published",
        paper.text: "text",
        paper.title: "title",
        paper.summary: "summary",
    }
)

# Set up in-memory source and executor
source = sl.InMemorySource(paper, parser=parser)
executor = sl.InMemoryExecutor(sources=[source], indices=[paper_index])
app = executor.run()

# Load the DataFrame with a progress bar using batches
batch_size = 10
data_batches = [df[i:i + batch_size] for i in range(0, len(df), batch_size)]
for batch in tqdm(data_batches, total=len(data_batches), desc="Loading Data into Source"):
    source.put([batch])
\
The in-memory executor is why Superlinked shines here—1,000 papers fit snugly in RAM, and queries fly without disk I/O bottlenecks.Step 5 : Crafting the queryNext is the query creation. This is where the template for crafting queries is created. To manage this, we need a query template that can balance both relevance and recency. Here’s what that would look like:# Define the query
knowledgebase_query = (
    sl.Query(
        paper_index,
        weights={
            text_space: sl.Param("relevance_weight"),
            recency_space: sl.Param("recency_weight"),
        }
    )
    .find(paper)
    .similar(text_space, sl.Param("search_query"))
    .select(paper.entry_id, paper.published, paper.text, paper.title, paper.summary)
    .limit(sl.Param("limit"))
)
\
This allows us to pick whether to prioritize the content (relevanceweight) or the recency (recencyweight) - a very useful combo for our agent's needs.Now comes the tooling part.We will be creating three tools … : This tool is crafted by plugging into Superlinked’s index, letting it pull the top 5 papers based on a query. It balances relevance (1.0 weight) and recency (0.5 weight) to accomplish the “find papers” goal. What we want is to find the papers which are relevant to the query. So, if the query is: “What quantum computing papers were published between 1993 and 1994?”, then the retrieval tool will retrieve those papers, summarize them one by one, and return the results.class RetrievalTool(Tool):
    def __init__(self, df, app, knowledgebase_query, client, model):
        self.df = df
        self.app = app
        self.knowledgebase_query = knowledgebase_query
        self.client = client
        self.model = model

    def name(self) -> str:
        return "RetrievalTool"

    def description(self) -> str:
        return "Retrieves a list of relevant papers based on a query using Superlinked."

    def use(self, query: str) -> pd.DataFrame:
        result = self.app.query(
            self.knowledgebase_query,
            relevance_weight=1.0,
            recency_weight=0.5,
            search_query=query,
            limit=5
        )
        df_result = sl.PandasConverter.to_pandas(result)
        # Ensure summary is a string
        if 'summary' in df_result.columns:
            df_result['summary'] = df_result['summary'].astype(str)
        else:
            print("Warning: 'summary' column not found in retrieved DataFrame.")
        return df_result
\
Next up is the . This tool is designed for cases where a concise summary of a paper is needed. In order to use it, it will be provided with , which is the ID of the paper that needs to be summarized. If a  is not provided, the tool will not work as these IDs are a requirement in order to find the corresponding papers in the dataset.class SummarizationTool(Tool):
    def __init__(self, df, client, model):
        self.df = df
        self.client = client
        self.model = model

    def name(self) -> str:
        return "SummarizationTool"

    def description(self) -> str:
        return "Generates a concise summary of specified papers using an LLM."

    def use(self, query: str, paper_ids: list) -> str:
        papers = self.df[self.df['entry_id'].isin(paper_ids)]
        if papers.empty:
            return "No papers found with the given IDs."
        summaries = papers['summary'].tolist()
        summary_str = "\n\n".join(summaries)
        prompt = f"""
        Summarize the following paper summaries:\n\n{summary_str}\n\nProvide a concise summary.
        """
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
        return response.choices[0].message.content.strip()
\
Finally, we have the . This tool chains the  to fetch the relevant papers and then uses them to answer the questions. If no relevant papers are found to answer the questions, it will provide an answer based on general knowledgeclass QuestionAnsweringTool(Tool):
    def __init__(self, retrieval_tool, client, model):
        self.retrieval_tool = retrieval_tool
        self.client = client
        self.model = model

    def name(self) -> str:
        return "QuestionAnsweringTool"

    def description(self) -> str:
        return "Answers questions about research topics using retrieved paper summaries or general knowledge if no specific context is available."

    def use(self, query: str) -> str:
        df_result = self.retrieval_tool.use(query)
        if 'summary' not in df_result.columns:
            # Tag as a general question if summary is missing
            prompt = f"""
            You are a knowledgeable research assistant. This is a general question tagged as [GENERAL]. Answer based on your broad knowledge, not limited to specific paper summaries. If you don't know the answer, provide a brief explanation of why.

            User's question: {query}
            """
        else:
            # Use paper summaries for specific context
            contexts = df_result['summary'].tolist()
            context_str = "\n\n".join(contexts)
            prompt = f"""
            You are a research assistant. Use the following paper summaries to answer the user's question. If you don't know the answer based on the summaries, say 'I don't know.'

            Paper summaries:
            {context_str}

            User's question: {query}
            """
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
        return response.choices[0].message.content.strip()
Step 7 : Building the Kernel AgentNext is the Kernel Agent. It functions as the central controller, ensuring smooth and efficient operation. Acting as the core component of the system, the Kernel Agent coordinates communication by routing queries according to their intent when multiple agents operate concurrently. In single-agent systems, such as this one, the Kernel Agent directly uses the relevant tools to manage tasks effectively.class KernelAgent:
    def __init__(self, retrieval_tool: RetrievalTool, summarization_tool: SummarizationTool, question_answering_tool: QuestionAnsweringTool, client, model):
        self.retrieval_tool = retrieval_tool
        self.summarization_tool = summarization_tool
        self.question_answering_tool = question_answering_tool
        self.client = client
        self.model = model

    def classify_query(self, query: str) -> str:
        prompt = f"""
        Classify the following user prompt into one of the three categories:
        - retrieval: The user wants to find a list of papers based on some criteria (e.g., 'Find papers on AI ethics from 2020').
        - summarization: The user wants to summarize a list of papers (e.g., 'Summarize papers with entry_id 123, 456, 789').
        - question_answering: The user wants to ask a question about research topics and get an answer (e.g., 'What is the latest development in AI ethics?').

        User prompt: {query}

        Respond with only the category name (retrieval, summarization, question_answering).
        If unsure, respond with 'unknown'.
        """
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=10
        )
        classification = response.choices[0].message.content.strip().lower()
        print(f"Query type: {classification}")
        return classification

    def process_query(self, query: str, params: Optional[Dict] = None) -> str:
        query_type = self.classify_query(query)
        if query_type == 'retrieval':
            df_result = self.retrieval_tool.use(query)
            response = "Here are the top papers:\n"
            for i, row in df_result.iterrows():
                # Ensure summary is a string and handle empty cases
                summary = str(row['summary']) if pd.notna(row['summary']) else ""
                response += f"{i+1}. {row['title']} \nSummary: {summary[:200]}...\n\n"
            return response
        elif query_type == 'summarization':
            if not params or 'paper_ids' not in params:
                return "Error: Summarization query requires a 'paper_ids' parameter with a list of entry_ids."
            return self.summarization_tool.use(query, params['paper_ids'])
        elif query_type == 'question_answering':
            return self.question_answering_tool.use(query)
        else:
            return "Error: Unable to classify query as 'retrieval', 'summarization', or 'question_answering'."
\
At this stage, all components of the Research Agent System have been configured. The system can now be initialized by providing the Kernel Agent with the appropriate tools, after which the Research Agent System will be fully operational.retrieval_tool = RetrievalTool(df, app, knowledgebase_query, client, model)
summarization_tool = SummarizationTool(df, client, model)
question_answering_tool = QuestionAnsweringTool(retrieval_tool, client, model)

# Initialize KernelAgent
kernel_agent = KernelAgent(retrieval_tool, summarization_tool, question_answering_tool, client, model)
\
Now let's test the system..# Test query print(kernel_agent.process_query("Find papers on quantum computing in last 10 years"))Running this activates the . It will fetch the relevant papers based on both relevance and recency, and return the relevant columns. If the returned result includes the summary column (indicating the papers were retrieved from the dataset), it will use those summaries and return them to us.Query type: retrieval
Here are the top papers:
1. Quantum Computing and Phase Transitions in Combinatorial Search 
Summary: We introduce an algorithm for combinatorial search on quantum computers that
is capable of significantly concentrating amplitude into solutions for some NP
search problems, on average. This is done by...

1. The Road to Quantum Artificial Intelligence 
Summary: This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provi...

1. Solving Highly Constrained Search Problems with Quantum Computers 
Summary: A previously developed quantum search algorithm for solving 1-SAT problems in
a single step is generalized to apply to a range of highly constrained k-SAT
problems. We identify a bound on the number o...

1. The model of quantum evolution 
Summary: This paper has been withdrawn by the author due to extremely unscientific
errors....

1. Artificial and Biological Intelligence 
Summary: This article considers evidence from physical and biological sciences to show
machines are deficient compared to biological systems at incorporating
intelligence. Machines fall short on two counts: fi...
\
Let's try one more query, this time, let's do a summarization one..print(kernel_agent.process_query("Summarize this paper", params={"paper_ids": ["http://arxiv.org/abs/cs/9311101v1"]}))Query type: summarization
This paper discusses the challenges of learning logic programs that contain the cut predicate (!). Traditional learning methods cannot handle clauses with cut because it has a procedural meaning. The proposed approach is to first generate a candidate base program that covers positive examples, and then make it consistent by inserting cut where needed. Learning programs with cut is difficult due to the need for intensional evaluation, and current induction techniques may need to be limited to purely declarative logic languages.
\
I hope this example has been helpful for developing AI agents and agent-based systems. Much of the retrieval functionality demonstrated here was made possible by Superlinked, so please consider starring the repository for future reference when accurate retrieval capabilities are needed for your AI agents!Combining semantic and temporal relevance eliminates complex reranking while maintaining search accuracy for research papers.Time-based penalties (negative_filter=-0.25) prioritize recent research when papers have similar content relevance.Modular tool-based architecture allows specialized components to handle distinct tasks (retrieval, summarization, question-answering) while maintaining system cohesion.Loading data in small batches (batch_size=10) with progress tracking improves system stability when processing large research datasets.Adjustable query weights let users balance relevance (1.0) and recency (0.5) based on specific research needs.The question-answering component gracefully degrades to general knowledge when paper-specific context is unavailable, preventing dead-end user experiences.Keeping up-to-date with the vast number of research papers published regularly can be challenging and time-consuming. An agentic AI assistant workflow capable of efficiently locating relevant research, summarizing key insights, and answering specific questions from these papers could significantly streamline this process.]]></content:encoded></item><item><title>Your Digital Self Is Not Your Own: How Moca Foundation&apos;s New Chain Plans to Give You Back Control</title><link>https://hackernoon.com/your-digital-self-is-not-your-own-how-moca-foundations-new-chain-plans-to-give-you-back-control?source=rss</link><author>Ishan Pandey</author><category>tech</category><pubDate>Thu, 26 Jun 2025 10:43:40 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Is your online identity truly yours? Every click, every login, and every piece of data you generate across countless apps and platforms are collected and controlled by centralised entities. Now, the Moca Foundation is launching a direct challenge to this status quo with the announcement of Moca Chain, a new blockchain built from the ground up to restore ownership of your digital life to its rightful owner: you.\
The Moca Foundation unveiled its plans for a Layer 1 blockchain dedicated entirely to identity and user data. The Moca Chain, with its testnet and mainnet slated for launch in the third and fourth quarters of 2025 respectively, aims to create a new infrastructure where individuals, devices, and even AI agents can manage their digital credentials without relying on the corporate platforms that dominate the web today. This initiative is not just about privacy; it is about establishing a new paradigm for how user data is verified, shared, and valued in the digital economy.\
The core of the Moca Chain's design is to provide a secure and interoperable foundation for digital identity. It will function as a modular, EVM-compatible chain, which means it is designed to work seamlessly with the vast ecosystem of applications built on Ethereum and other compatible networks. At the heart of this new ecosystem will be the MOCA Coin, which will be used for all network activities, including transaction fees, staking by validators, and payments for data storage and verification services. This creates a self-sustaining economy around the management of digital identity.The Problem with a Centralized Digital WorldIn the current web landscape, convenience often comes at the cost of control. Services like single sign-on (SSO), which allow you to use one account (like Google or Facebook) to log into multiple applications, have become ubiquitous. While they simplify the user experience, they also create a significant vulnerability. As Yat Siu, the co-founder and executive chairman of Animoca Brands, points out, this model has deep-seated flaws. "Billions of users today go online using single sign-on (SSO), which contains the keys to a user’s data, services, and digital lives," Siu stated. "While convenient, SSO represents a centralized point of failure that compromises security while also allowing operators to aggressively extract value from users’ digital selves."\
This centralization means that large technology companies hold the keys to our digital kingdoms. Our data is siloed within their "walled gardens," where it is monetized for their benefit. This model not only exposes users to massive-scale data breaches but also prevents them from having a unified, portable identity that they can use across different services without compromising their privacy. The fragmentation of our digital identities across numerous platforms also makes it difficult to build a comprehensive and verifiable reputation that can be leveraged in various contexts, from professional networking to accessing financial services.\
Moca Chain directly confronts this issue by proposing a decentralized alternative. "Moca Chain seeks to solve this problem by giving users decentralized true ownership of their data, ensuring the sovereignty of users’ digital identity without a single point of failure," Siu explained. This vision aligns with the broader mission of Animoca Brands to champion digital property rights, empowering individuals to not only control their online activities and personal data but also to share more equitably in the value they generate.A New Infrastructure for Trust\
So, how does Moca Chain propose to solve these complex issues? The project is building a multi-faceted infrastructure designed for what it calls "self-sovereign identity." This concept revolves around the idea that individuals should have ultimate control over their own digital identities, just as they do with their physical identification documents. To achieve this, Moca Chain will allow both on-chain (data stored on a blockchain) and off-chain (data stored elsewhere) user data to be verified by any application on any blockchain. This is made possible through a combination of decentralized data storage, a cross-chain identity oracle, and advanced cryptographic methods.\
One of the key technologies Moca Chain will employ is web proof data generation, specifically through a method known as zkTLS. In simple terms, this allows for the verification of data from a web session without revealing the data itself, a powerful application of zero-knowledge proofs. For example, you could prove that you have a certain educational credential from a university's portal without actually sharing the certificate itself with a third-party application. This has profound implications for privacy across numerous industries, from healthcare, where patient data could be verified without being exposed, to finance, where know-your-customer (KYC) requirements could be met without sensitive documents being shared repeatedly.\
This technological stack will work in conjunction with the AIR Kit, a software development kit (SDK) from Moca Network, the identity ecosystem of Animoca Brands. The AIR Kit is already being integrated into a host of consumer applications, including those with massive user bases like SK Planet’s OK Cashbag, with its 28 million verified users, and One Football, which has over 200 million users. Kenneth Shek, the project lead of Moca Network, sees this as a pivotal move to disrupt the current model of data ownership. “Moca Chain and AIR Kit are a one-of-a-kind infrastructure for verified identity data to empower consumer apps and their users," Shek said. "By adopting Moca Chain and MOCA Coin, we believe we can disrupt current models of data ownership and break down the dominance of walled garden ecosystems, returning value to the users who generate it and making ecosystem growth more scalable.”\
The launch of Moca Chain represents a significant and ambitious step toward solving one of the most persistent problems of the digital age: the ownership and control of our personal data. The vision articulated by the Moca Foundation and its partners at Animoca Brands is not merely a technological one, it is fundamentally about rebalancing power in the digital world. The idea of a user-centric identity layer that is both private and interoperable has been a long-held goal for many in the Web3 space, but the practical implementation has always been the primary hurdle.\
What makes Moca Chain a particularly noteworthy project is its pragmatic approach to adoption. By integrating its AIR Kit into established Web2 platforms with hundreds of millions of users, it is building a bridge for mainstream adoption rather than expecting users to jump headfirst into a completely new and unfamiliar ecosystem. This strategy of meeting users where they are could be the key to achieving the network effects necessary for a new identity standard to take hold.\
However, the path ahead will not be without its challenges. The project will need to navigate a complex regulatory landscape, particularly in sensitive areas like finance and healthcare. Furthermore, it will have to convince both developers and end-users that its solution is not only more secure and equitable but also just as convenient as the centralized systems they are accustomed to. The success of Moca Chain will ultimately depend on its ability to deliver a seamless user experience that makes the benefits of self-sovereign identity tangible to the average person. If it succeeds, it could indeed lay the groundwork for a more equitable and user-empowered internet.\
Don’t forget to like and share the story!]]></content:encoded></item><item><title>AMD CPUID Faulting Support Looks To Be Coming For Linux 6.17</title><link>https://www.phoronix.com/news/AMD-CPUID-Faulting-Linux-6.17</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 10:30:33 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a follow-up to the article a few weeks ago about AMD enabling User CPUID Faulting support for Linux, that code looks like it's ready to go for being introduced in the upcoming Linux 6.17 kernel...]]></content:encoded></item><item><title>Bochs DRM Panic Support, Panfrost Adds Mediatek MT8370 SoC For Linux 6.17</title><link>https://www.phoronix.com/news/Linux-6.17-Bochs-Panic-MT8370</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 10:18:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Maxime Ripard at Red Hat sent out the latest weekly pull of "drm-misc-next" changes to DRM-Next for queuing of these kernel graphics/display driver changes ahead of the Linux 6.17 merge window opening up in about one month's time...]]></content:encoded></item><item><title>New Datacenter In Italy Captures Heat Waste</title><link>https://hardware.slashdot.org/story/25/06/26/0032258/new-datacenter-in-italy-captures-heat-waste?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Jun 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Italian utility A2A and French tech firm Qarnot have launched a data center in Brescia, Italy, that captures waste heat from servers and redirects it to a local district heating system. "The Brescia project is expected to meet the heating needs of more than 1,350 apartments and cut carbon dioxide emissions by 3,500 tons annually -- equivalent to the absorption capacity of over 22,000 trees," reports Reuters. From the report: "The rapid spread of data centers and the growing electrification of consumption require major investments in power grids. But data centers also offer a remarkable opportunity for cities with district heating networks," A2A CEO Renato Mazzoncini said at the inauguration. "In (the Italian region of) Lombardy alone, with projects already in the pipeline, we estimate that 150,000 apartments could be heated this way," Mazzoncini added.]]></content:encoded></item><item><title>Blender 5.0 Introducing HDR Support On Linux With Vulkan + Wayland</title><link>https://www.phoronix.com/news/Blender-5.0-HDR-Linux-Wayland</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 09:55:06 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The upcoming Blender 5.0 3D modeling software application is introducing High Dynamic Range (HDR) display support on Linux when making use of Wayland -- no X11 support for HDR -- and Vulkan graphics accelerator...]]></content:encoded></item><item><title>Neo Pepe $NEOP Presale Passes $2M Raised With Stellar CertiK Audit</title><link>https://hackernoon.com/neo-pepe-$neop-presale-passes-$2m-raised-with-stellar-certik-audit?source=rss</link><author>Chainwire</author><category>tech</category><pubDate>Thu, 26 Jun 2025 09:42:51 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Road Town, Tortola, June 25th, 2025/Chainwire/-- ($NEOP) has crossed past the $2 million milestone in record-breaking speed, propelling it toward Stage Four of its anticipated presale. Launching less than a week ago, the project’s early growth reflects strong interest, with investors rallying behind its revolutionary approach to decentralization, governance, and liquidity.Within days, Neo Pepe has hit a notable $2 million raised and strong participation in the project’s presale.This achievement not only underscores investor confidence but highlights the project's compelling narrative—a serious, thematic rebellion against traditional financial centralization, aptly branded as the Memetrix.Neo Pepe Coin recently achieved a 71.96 score on its , validating its credibility as a legitimate and secure project.Neo Pepe’s presale is meticulously structured across 16 dynamic stages, progressively increasing the token price to reward early supporters. Now, as it rapidly approaches Stage Four, the window to secure tokens at advantageous pricing is narrowing swiftly.Further differentiating Neo Pepe Coin is its innovative 2.5% auto-liquidity mechanism. Each transaction enhances liquidity pools, with LP tokens permanently burned, creating sustained price stability and growth potential.Complementing this powerful feature is a fully decentralized governance model, empowering token holders with real decision-making power on strategic listings and treasury allocations.Neo Pepe Coin’s early performance has been marked by steady presale participation and a structured rollout strategy. With its ongoing stage-based pricing model, thematic framing, and auto-liquidity mechanics, $NEOP continues to progress through its planned presale phases.With Stage Four approaching, Neo Pepe Coin continues through its presale schedule, supported by consistent participation and structured pricing mechanics. The project’s distinctive theme and token model remain central to its current phase of growth.Users can secure a spot now and discover why Neo Pepe Coin is setting new standards in crypto innovation.For more information, users can join the Neo Pepe community on socials or visit the official website today. ($NEOP) is a decentralized cryptocurrency designed to challenge centralization, regulatory overreach, and market manipulation. Leveraging the thematic narrative of the Memetrix, Neo Pepe Coin symbolizes a bold movement towards financial democratization and innovation.The project features a structured 16-stage presale, robust community-driven governance, and an auto-liquidity mechanism ensuring sustainable growth and stability.To Get Started with $NEOP:::tip
This story was published as a press release by Chainwire under HackerNoon’s Business Blogging .]]></content:encoded></item><item><title>Deadly Risks: 5 Crypto Entrepreneurs and Investors Who Paid the Ultimate Price</title><link>https://hackernoon.com/deadly-risks-5-crypto-entrepreneurs-and-investors-who-paid-the-ultimate-price?source=rss</link><author>Michael Jerlis</author><category>tech</category><pubDate>Thu, 26 Jun 2025 08:36:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Crypto promises wealth and freedom. But sometimes, it costs people their lives…These are five real stories where being involved in crypto wasn’t just financially risky — it was fatal. Not because of bad trades, but because of kidnappings, threats, and murderSince 2018, there have been over 150 documented cases of crypto-related kidnappings, “wrench attacks,” and violent extortion attempts across North America, Europe, and Asia. In 2025 alone, 7 high-profile abductions were reported in France, including cases involving physical mutilation and ransom demands of up to €10 million.Financially, the numbers tell an alarming story. In 2023, total crypto ransom payments reached , nearly double the $567 million paid the year before. In 2024, the figure dropped to $814 million, but still reflects a massive sum of money used to fund coercion, violence, and organized crime.Let’s take a closer look at five high-profile tragedies that expose the deadly intersection of money, privacy, and risk in the crypto world.Kidnapped and Killed for Bitcoin in Kyiv (July 2024)In July 2024, news broke that a 29-year-old crypto investor had been kidnapped and murdered in Kyiv. According to BeInCrypto Russia, the victim was lured through social engineering — a fake meeting involving a car rental, where attackers posed as clients or partners. Once inside the vehicle, he was taken to an unknown location.His captors demanded access to his crypto holdings — about 3 BTC. That’s not a fortune by crypto standards, but in Ukraine, it’s well above the average yearly salary. The attackers were prepared. When the victim refused to give up access to his wallet, he was killed.The killers were arrested within a few days. But what stands out is the method: premeditated, organized, and directly targeting a crypto user. This wasn't a random mugging. It’s a reminder that once someone knows you're holding crypto, especially self-custodied assets, you're exposed — not digitally, but physically.Murder of Antonio Vinicius Lopes Gritzbach (Brazil, November 2024)Antonio Lopes Gritzbach was a Brazilian entrepreneur working with Bitcoin and reportedly linked to over-the-counter crypto operations. These kinds of OTC businesses are often used to convert large amounts of crypto into fiat discreetly. And in Brazil, the line between OTC and money laundering is often razor-thin.According to The Guardian, Gritzbach had been cooperating with prosecutors on a high-profile money laundering case. His death wasn’t random. He was shot multiple times in broad daylight, near the Guarulhos International Airport in São Paulo. Surveillance footage showed a car slowly trailing him before someone opened fire.Some local sources suggested Gritzbach had inside knowledge of crypto flows used by drug cartels and political interests. While these claims were never confirmed, it’s clear he wasn’t just an average Bitcoiner. His death shows the dangerous overlap between crypto and organized crime, especially in regions where the legal framework is still catching up.Death of Nikolai Mushegian — DeFi Pioneer or Target? (Puerto Rico, October 2022)Nikolai Mushegian was no stranger to crypto insiders. He co-founded , one of the biggest decentralized finance protocols. He also contributed to Balancer Labs and early stablecoin designs. A deeply technical person, Nikolai was also known for his paranoia — and for good reason.In the months leading up to his death, Mushegian had been tweeting erratically about being followed, bugged, and targeted by intelligence agencies. The tweet that shocked everyone came just hours before his death: he claimed the CIA, Mossad, and others were running blackmail operations and were planning to torture him. A few hours later, he drowned near Condado Beach, Puerto Rico. No foul play was found — but the beach was known for strong rip currents, and he was a strong swimmer.BeInCrypto and others covered the story, sparking endless speculation. Was it suicide? Was it paranoia turned real? Mushegian had been working on privacy-preserving tools and alternative financial systems — things that do challenge state power. Whether or not there’s more to the story, the crypto world lost one of its most brilliant and troubled minds.Tiantian Kullander — Young Billionaire, Sudden Death (Hong Kong, November 2022)Tiantian “TT” Kullander was just 30 years old when he passed away in his sleep, with no clear medical explanation. He co-founded , a major crypto trading and infrastructure company based in Hong Kong. Valued at over $3 billion at one point, the firm handled institutional trades, offered yield products, and had ties to major players like Temasek.TT was also a former trader at Morgan Stanley and a rising star in global finance. His death shocked the industry. Changelly reported the cause as “unexpected” with no further details.What spooked many wasn’t just the timing, but the pattern: his death followed the collapse of FTX by just days. Several other crypto executives died within the same 12-month window — including Nikolai Mushegian and Vyacheslav Taran. Online forums and Telegram groups exploded with theories, though no link has ever been confirmed.This case shows how little we know about what happens at the top of crypto finance — where money moves at speed and under intense pressure. Whether TT died of natural causes or something more complex, it left a massive hole in one of Asia’s biggest Web3 firms.The Stabbing of Bob Lee — Tech Icon with Crypto Ties (San Francisco, April 2023)Bob Lee was a Silicon Valley legend. He helped build Android at Google, launched Square’s Cash App, and was an early backer of many startups. In recent years, he got deep into crypto, joining the MobileCoin project — a privacy-focused coin aimed at messaging apps like Signal.In April 2023, Bob was stabbed to death in downtown San Francisco. The case made headlines because of who he was — not just a tech guy, but a powerful connector in both crypto and finance. Initial theories included robbery or a possible crypto deal gone wrong. But police later arrested a suspect known to Bob personally.Even though the motive turned out to be personal, the crypto world didn’t ignore the context. BeInCrypto and others noted how the killing fueled wider concerns: if someone like Bob Lee isn’t safe, how safe is anyone building cutting-edge tech that challenges traditional systems?How to Stay Safe in CryptoCrypto puts you in control, but that control comes with visibility — and visibility attracts risk. Criminals, extortionists, even governments are watching the space more closely than ever. If you hold significant assets or work on high-profile projects, you need to think about your security the way a public figure or whistleblower would.Don’t talk publicly about your holdings, not even in passing on Telegram or Twitter. Avoid connecting your real name to wallets or exchanges. Use aliases where possible, and treat your online identity as something separate from your real life. Store most of your assets in cold wallets, and never in one place. Use techniques like splitting keys or multisig to limit exposure. Backups should be offline, encrypted, and stored in more than one secure location.Be cautious with who you meet in the crypto space. Some attacks have started with what looked like harmless business conversations. Assume your messages could be read and your devices compromised. Use end-to-end encrypted tools like Signal or Session, and stay alert in both physical and digital environments.==Most importantly, remember this: blockchain isn’t anonymous==. Even if you’re operating legally, once you're in the public eye — as a developer, founder, or even a well-known trader — the risk of being targeted increases dramatically.Is Crypto Worth the Risk?The five stories mentioned earlier aren’t rare exceptions. They’re warning signs. In crypto, people have been extorted, abducted, and in some cases, killed — not because they ran scams, but simply because they were visible and valuable.This space offers freedom, but that freedom comes with real-world threats. As crypto becomes more traceable and more mainstream, your personal security needs to evolve just as fast.]]></content:encoded></item><item><title>Offline-First JavaScript for Space Missions: How JS Thrives in Low-Connectivity Environments</title><link>https://hackernoon.com/offline-first-javascript-for-space-missions-how-js-thrives-in-low-connectivity-environments?source=rss</link><author>Raju Dandigam</author><category>tech</category><pubDate>Thu, 26 Jun 2025 08:35:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Whenever we think about javascript we imagine websites, web applications, browser based games but javascript has quietly evolved beyond the web.Today, it's even orbiting our planet—and in some cases, headed to Mars. In environments where internet connectivity is a luxury, JavaScript is proving its worth as an offline-first language.This article explores how JavaScript is used in space missions, the challenges it solves in remote environments, and how developers can draw inspiration from these high-stakes use cases to build more resilient apps.Now, let’s understand why offline-first is important in Space?There are Spacecraft, satellites, and interplanetary rovers operate in extreme conditions: (up to 40 minutes delay betweenEarth and Mars)Power and memory constraintsNow, any of the software running in these systems must beResilient to connection dropsCapable of local decision-makingAble to sync data when possibleIn such a scenario, JavaScript’s lightweight nature, ecosystem, and offline-capable design patterns make it surprisingly useful.Let’s talk about some real use case of the space flow,Real Use Case: NASA's Open MCTOne of the most prominent JavaScript-based tools in space exploration is (Mission Control Technologies), developed by NASA.Now let’s understand what is that?A web-based platform for mission planning, telemetry visualization, and data monitoring.Built with JavaScript, HTML, and CSS.Designed to work offline, allowing ground operators to continue monitoring spacecraft without internet access.NASA's Ames Research CenterESA (European Space Agency)Commercial space startupsIts modular design allows teams to customize the interface and logic for specific missions—whether you're watching a Mars orbiter or a CubeSat in low Earth orbit.Now, let’s understand why we are using JS in space?Why Use JavaScript in Space?We all have wondered why they chose JS instead of C/C++ in the tech space?Fast prototyping for dashboards and tooling.Runs in browsers and Node.js environments.Use existing libraries to handle logs, charts, caching, etc.Service workers, IndexedDB, and localStorage**.**While low-level systems in spacecraft often use C or Ada, mission control tools, simulators, and dashboards are increasingly JS-powered.Here is the quick comparison with C with JS:|  |  |  |
|----|----|----|
|  |  |  |
|  |  | Slower, requires more setup |
|  |  |  |
|  |  |  |
|  |  |  |Can you tell me what techniques can be use in space?So, there are many techniques they can use in space but we will mainly focus on 5 techniques.Offline-First JS Techniques for SpaceService Workers for CachingLet’s understand what is that?Service workers allow caching files and API responses for offline access.self.addEventListener("fetch", (event) => {
  event.respondWith(
    caches.match(event.request).then((response) => {
      return response || fetch(event.request);
    })
  );
});
Perfect for telemetry dashboards where continuous internet isn’t guaranteed.\
Now, let’s see how offline data logging is doing in JS.Intermittent Sync StrategiesHey What’s this Intermittent sync strategy?let me explain to you about it so we all know that Space has very limited communication ways correct? Now in this case there are some strategies which can help here so that delta sync, data compression, and edge caching can reduce the time spent transferring data once a connection is re-established and it’s very crucial for high-latency environments like deep space.3. Fault Tolerance and RedundancySpace systems must anticipate failures. JavaScript applications can:
Use  for failed API calls.Implement  to prevent system overloads.Use  to maintain a record of all changes, allowing recovery after failures.4. Data Prioritization and CompressionNot all data is equally important. JS systems can prioritize critical telemetry and compress non-essential data to ensure the most important information reaches mission control.// Example: Compressing JSON data before sending
const data = { status: 'OK', battery: '85%', signal: 'strong' };
const compressed = JSON.stringify(data).replace(/\s+/g, '');
This approach minimizes data size, reducing transmission costs and time.When bandwidth is limited, not all data is equal. Systems tag and queue data by priority:Life-support > scientific readings > diagnosticsHigh-priority packets are synced firstJavascript on Edge Devices in SpaceIn Space missions often include sensors or edge devices with low computing power. Surprisingly, JS fits here too:Node.js on Raspberry Pi Zero (used in CubeSats)JavaScript runtimes on microcontrollers (e.g., Espruino, Moddable)Buffering telemetry for future sync A Raspberry Pi in a weather balloon logs atmospheric data with a Node.js app, stores readings in IndexedDB, and dumps to SD card for later analysis.Lessons Earth Developers Can LearnOffline-first JS isn't just for space—it benefits any environment with unreliable internet:Games with offline progress trackingIf it works on Mars, it'll work anywhere.Progressive Web Apps (PWAs)Caching and sync patternsThese ideas improve reliability, performance, and UX—whether you're in orbit or on a bus
The development of JavaScript from web page scripting to space mission technology represents an impressive transformation. The language demonstrates its growth and adaptability through this development. The study of JS operations in space enables us to develop applications that handle unpredictable conditions while delivering user satisfaction and creating a more resilient web and beyond.]]></content:encoded></item><item><title>Educational Byte: How to Buy Crypto for the First Time</title><link>https://hackernoon.com/educational-byte-how-to-buy-crypto-for-the-first-time?source=rss</link><author>Obyte</author><category>tech</category><pubDate>Thu, 26 Jun 2025 08:34:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[New to crypto? Want to know what the fuss is about? Well, it’s easier than you think. If you barely know a thing about the industry, buying cryptocurrency for the first time might feel a bit like learning a new language or stepping into a sci-fi movie, but don’t worry, you’re not alone. In fact, over 400 million people around the world now own crypto, and that number keeps growing.Crypto is digital money that doesn’t need banks or governments and works 24/7 from a simple app on your device (desktop, mobile, or other hardware). You can send it worldwide, without limits, and, depending on the decentralization level of the coin involved, no one, not even your government, can stop your transactions or seize your funds. Let’s see some other things you should know to start.Before you buy your first crypto, there are a few things you need to consider. Not to scare you, but to help you avoid headaches later. First off, yes,  in most countries, and yes, it’s real money with real market value. People all over the world use it for trading, sending money abroad, and even saving. But it’s not like putting cash under your mattress or buying a stock and forgetting about it. Depending on the coin, prices can jump or crash in hours. You should never invest more than you’re willing to lose.And please, don’t just trust anyone who talks a good game. Your favorite influencer? They might be getting paid to promote something shady. That random message on Telegram promising a 200% return? Probably . Even friends can get tricked and drag you into it without meaning to. If something sounds too good to be true, it probably is. Always double-check, read reviews, and compare sources.Also, think about why you’re doing this. Are you curious? Want to invest a bit? Planning to send money abroad? Different goals might lead you to different coins. Bitcoin, Ethereum, , stablecoins, , DeFi tokens —they all work differently and offer different things. Bitcoin and stablecoins mainly work for payments (stablecoins being more, well, stable in price). Ethereum and Obyte have smart contracts and other advanced features. Privacy tokens like Monero or  will obscure your transactions. DeFi tokens may help you make . It's really up to you, so do your research on the project you’re interested in first.Buying Crypto for the First TimeAfter you do your own research (), you’ll only need your phone or device of choice, pen and paper, Internet, and an email. Your ID may be required too, if you choose the easiest way to buy (through regulated companies). Overall, here are some simple steps to get started:\Choose your first crypto: This decision must be based on what you want to use it for, and not just what’s trending. Want to send money? Any crypto will do, especially stablecoins. Curious about Dapps? Look into Ethereum or Obyte. It’s also advisable to start small. \n Pick a trustworthy and/or regulated crypto exchange: If you only have fiat money (USD, EUR, etc.), you’ll need to visit a crypto exchange. They’re websites, mostly beginner-friendly, where you can create an account with an email and password, and start buying crypto with fiat. They may ask for your real ID, but again, that depends on the volume you want to handle. Popular exchanges include Binance, Coinbase, and Biconomy.As a side note: yes, you can also buy crypto directly from someone else. No need for middlemen. However, if you pick this option, be sure to select someone reliable. Crypto transactions can’t be reversed. Buying crypto physically from strangers, using cash, could be particularly dangerous.Make the purchase and store it safely: crypto exchanges may work as wallets, but they’re not wallets at all. While your money is there, it’s still controlled by the exchange, and if it goes under, so does your money. The best option is to select a  (under your full control), install it on your device of choice, and write down your private keys physically. Then, move most of your crypto funds to this wallet, and leave little or nothing in the exchange.The Obyte wallet has an easy option for learners: “,” available in the “Receive” menu. This button leads to a beginner-friendly website where it’s possible to  (Obyte’s native currency) and other Obyte-compatible tokens with ether (ETH), USD Coin (USDC) or Wrapped Bitcoin (WBTC). All of them can be acquired with fiat in popular crypto exchanges.\ In case you only have fiat and you want to carry out the process from a single place, GBYTE is listed on the crypto exchange Biconomy against Tether (USDT).** Biconomy has  to buy stablecoins like USDT with payment methods like debit and credit cards, Banxa, Apple Pay, and Alchemy Pay. Once you’ve bought some USDT, you can exchange it for GBYTE on the same platform. Afterward, remember that those GBYTEs are on the Polygon network, so, if you want to use them in Obyte, you’ll need to import them through the .There’s another option as well: . This exchange doesn’t handle fiat, but it doesn’t ask for ID verification (KYC) either. They have the trading pairs GBYTE/BTC and GBYTE/USDT available. This is likely not the most beginner-friendly alternative, but if you’re concerned about your privacy, it may be the best path. Featured Vector Image by storyset / ]]></content:encoded></item><item><title>AI in Fundraising Hits Ethical Speed Bump, New Report Finds</title><link>https://hackernoon.com/ai-in-fundraising-hits-ethical-speed-bump-new-report-finds?source=rss</link><author>Eli Grid</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:55:06 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Inside the Findings: Ethics Over EfficiencyA new report jointly released by the Chartered Institute of Fundraising (CIOF) and the University of York’s Centre for Digital Innovation in Philanthropy and Fundraising (CDIPF) has spotlighted a major hesitation in the adoption of AI among UK fundraisers. The core reason? Ethical unease.The research surveyed 78 organizations and conducted in-depth interviews with 21 fundraising professionals. While participants acknowledged that AI could streamline operations and enhance engagement, many remained wary of its risks — particularly around , transparency, and tool reliability.Dr. Marta Herrero, the report’s lead researcher and Director at CDIPF, emphasized the gap between technical potential and emotional readiness. “AI-driven technology is capable of offering novel and compelling possibilities for charitable fundraising,” she said. “However, first we need to build trust among fundraising professionals.”Herrero stressed the importance of fundraisers understanding both the benefits and limitations of AI. Beyond technical training, she said, the profession needs cultural readiness — confidence in using the tools responsibly and ethically.The report was formally launched at the CIOF’s annual Fundraising Convention, under the theme Shaping the Future of Fundraising with AI.Ceri Edwards, executive director of engagement at CIOF, reinforced the message: “As AI technologies become increasingly accessible, they offer unparalleled opportunities for our sector — however, we must navigate these advancements carefully.”She added that ethical clarity and donor trust must remain at the center of AI integration efforts.This study adds to a growing body of global concern around the unchecked use of AI in sensitive sectors. For fundraisers, whose work is built on human connection and trust, the path to AI adoption may require more than just innovation — it needs ethical alignment.What’s at Stake for the Nonprofit SectorUnlike in commercial marketing, where rapid adoption of AI can offer immediate ROI, nonprofits face a different calculation. Here, missteps come with reputational risk. A botched AI rollout that mishandles donor data or personalizes outreach poorly could backfire, harming both donations and community trust.Imagine a donor receiving an automated message suggesting a gift based on income data they never consented to share — the loss of goodwill could outweigh any automation gain.At the same time, fundraising teams are increasingly resource-constrained. AI offers efficiency: chatbots to triage queries, models to prioritize donor outreach, even predictive tools to forecast campaign performance. But until ethical concerns are addressed head-on, those benefits may remain out of reach.“We want fundraisers to feel confident about using AI,” said Dr. Herrero, “feeling that they understand what it can do well, the challenges it poses, and — most importantly — that they trust in their own abilities to identify how they can use the tools responsibly for the benefit of the communities and donors they serve.”That balance of empowerment and ethical literacy may be the missing piece in the sector’s cautious approach.Where It Could Go From HereFundraising won’t escape AI — but it may humanize it first. Unlike sales-driven sectors, nonprofits are demanding trust by design, not as an afterthought. Expect to see hybrid models emerge, where AI supports but never replaces human judgment. The organizations that get this balance right could not only raise more, but also raise better.Should nonprofits slow down AI adoption until ethical frameworks are stronger — or risk falling behind? Tell us what you think.]]></content:encoded></item><item><title>AI Agents Can Now Pay Autonomously Thanks to x402 and HTTP 402</title><link>https://hackernoon.com/ai-agents-can-now-pay-autonomously-thanks-to-x402-and-http-402?source=rss</link><author>Stefano Amorelli</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:52:12 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[The specification introduces  : previously, every single HTTP request required a fresh TCP connection. Persistent connections resolve this, allowing multiple HTTP requests to flow through a single, long-lived TCP connection. No more establishing separate connections for every image, CSS file, or JavaScript snippet on a web page.There's also chunked transfer encoding , a new way for web servers to stream content without knowing the full size beforehand. No longer does a server need to calculate the total size of dynamically generated content upfront, it's now free to deliver data incrementally, as it's produced.But RFC 2068 quietly introduces something intriguing , a new status code:HTTP 402 Payment Required
This code is reserved for future use.
This shows how the founding fathers of world wide web predicted how money would eventually become a big part of internet, even if they had no clear path on how it would actually play out.Today, 2025, nearly three decades and multiple HTTP versions later ( in 2015,  in 2022), still sits there with the exact same note: 'reserved for future use.' Despite the fintech revolution, the rise of online payments, and an entire economy built on internet transactions, nobody had figured out what to do with it.Last month (May 2025), Coinbase released , an open source protocol that gives  its first real job: enabling native  payments within HTTP requests.Nowadays AI agents need to make  (machine-to-machine) payments across the web with reduced  (human-in-the-loop) interventions, but traditional payment flows dont work well in this case. They require multiple human interactions, redirects, and manual steps that simply don't work when an AI agent needs to make a transaction autonomously. fills this gap. It proposes an automated on-chain payment flow implemented natively within the HTTP protocol, making them as seamless as any other web request.But what does this look like in practice? is built around four core components:A  acts as the payment initiator, discovering what's required for access and constructing the appropriate payment payload. Put simply, this is whatever is making the HTTP request to a pay-walled resource. It could be a browser making a request for premium content, an AI agent purchasing API access, or a mobile app unlocking features. The client handles the cryptographic signing using the user's private key and automatically retries requests when payment is required.The  enforces payment policies for its endpoints while remaining focused on its core business logic. This is the web server or API that hosts the content or service being purchased. It maintains simple pricing tables that map endpoints to costs, but delegates the payment verification logic to the facilitator.Blockchain logic is implemented in the  component: verifying cryptographic signatures, preventing replay attacks through nonce tracking, and managing the actual on-chain settlement. It allows both clients and servers to work with on-chain payments without understanding the blockchain implementation details.On  resides the final settlement layer, ensuring payments are immutable and transparent. It enables programmable money through smart contracts and stable-coins, but its complexity is completely hidden from the application layer by the facilitator.Primary Responsibility: Payment initiationKey Features:EIP-712 signing, automatic retries, payment discoveryWhat it does: Makes requests, handles wallets, retries with paymentPrimary Responsibility: Payment enforcementKey Features: Pricing tables, HTTP 402 responses, middleware integrationWhat it does: Sets prices, checks payments, serves contentPrimary Responsibility: Payment verificationKey Features: Signature verification, nonce tracking, gas abstractionWhat it does: Verifies signatures, talks to blockchainPrimary Responsibility: Payment settlementKey Features:USDC transfers, smart contracts, immutable recordsWhat it does: Settles payments on chainThis architecture demonstrates several fundamental software engineering principles. The most important is . Each component has a single, well-defined responsibility. Resource servers focus purely on business logic, facilitators handle payment complexity, and clients manage user interaction.The system achieves  by having components interact only through standardized HTTP and REST interfaces. A resource server doesn't need to understand how blockchain transactions work, and a client doesn't need to know the server's internal implementation. This isolation means you can swap out components (for example, use a different blockchain, change facilitator providers, or modify server logic) without affecting the rest of the system.The facilitator embodies the single responsibility principle by isolating all blockchain complexity into one specialized service. This prevents payment logic from leaking into business applications and keeps concerns properly separated.Last but not least this architecture follows . High-level components depend on abstractions rather than concrete implementations. Servers and clients depend on HTTP interfaces, not specific blockchain APIs. This allows the same application code to work across different blockchains and payment schemes without modification.When an AI agent or user hits an -enabled API, here's the four-step flow that happens: : The client makes a standard HTTP request to access some resourcePayment required response : If no payment is attached, the server responds with  and includes payment details : The client creates a cryptographically signed payment and retries the request : The server validates the payment, broadcasts it to the blockchain, and grants accessWhat makes this powerful is that it all happens at the HTTP protocol level. No redirects to third-party payment processors, no  flows, no account creation. Just standard HTTP with extra headers: flows from client to server and contains the . This includes the payment details (amount, recipient, token) plus a cryptographic signature proving the client authorized the payment. flows from server to client after successful payment and contains transaction receipt information , providing transparency about what happened on-chain.Server-side implementationPayment middleware architectureThe core server-side implementation revolves around a payment filter (AKA middle-ware) that intercepts HTTP requests and enforces payment requirements. When integrated into your web server, this middle-ware checks incoming requests against a price table that maps endpoints to their costs.The middle-ware follows a simple decision tree: if a request hits a protected endpoint without payment, it responds with  and detailed payment instructions. If payment is included in the  header, it verifies the payment with a facilitator service before allowing the request to proceed.Here's the essential structure from the Java implementation:public class PaymentFilter implements Filter {
    private final String payTo;
    private final Map<String, BigInteger> priceTable; // path → amount
    private final FacilitatorClient facilitator;

    public void doFilter(ServletRequest request, ServletResponse response, 
                        FilterChain chain) throws IOException, ServletException {
        String path = req.getRequestURI();
        String paymentHeader = req.getHeader("X-PAYMENT");

        if (!priceTable.containsKey(path)) {
            chain.doFilter(request, response);  // Free endpoint
            return;
        }

        if (paymentHeader == null) {
            send402Response(resp, path);  // Request payment
            return;
        }

        // Verify payment, process request, then settle
        VerificationResponse verification = facilitator.verify(paymentHeader, requirements);
        if (verification.valid) {
            chain.doFilter(request, response);
            facilitator.settle(paymentHeader, requirements);
        }
    }
}
The beauty of this approach is that it requires minimal changes to existing applications. You simply add the payment filter to your middle-ware stack and define which endpoints require payment. ResponseWhen a client hits a protected endpoint without payment, the server constructs a detailed payment requirements object. This includes the payment amount, accepted tokens (like ), the receiving wallet address, blockchain network, and an expiration time to prevent replay attacks.private void send402Response(HttpServletResponse response, String path) throws IOException {
    response.setStatus(HttpStatus.PAYMENT_REQUIRED);
    response.setContentType("application/json");

    PaymentRequirements requirements = PaymentRequirements.builder()
        .paymentRequirement(List.of(
            PaymentRequirement.builder()
                .kind(new Kind("exact", "base-sepolia"))             // Payment scheme + blockchain network
                .receiver(payTo)                                     // Wallet address to receive payment
                .amount(priceTable.get(path))                        // Cost for this specific endpoint
                .asset("<USDC_TOKEN_CONTRACT>")                      // USDC token contract
                .expiry(Instant.now().plus(Duration.ofMinutes(5)))   // Payment window
                .nonce(UUID.randomUUID().toString())                 // One-time use identifier
                .build()
        ))
        .build();

    response.getWriter().write(Json.MAPPER.writeValueAsString(requirements));
}
Each field in the  is described as follows:: Defines the payment scheme ( for fixed amounts) and target blockchain network ( for Base testnet). This tells the client exactly how to structure and execute the payment.: The wallet address where payment should be sent. This is your business wallet that will receive the funds.: The cost for accessing this specific endpoint, retrieved from your price table. For USDC, this is typically expressed in  (smallest unit).: The smart contract address of the token to be used for payment. The example shows USDC on Base Sepolia testnet.: A timestamp after which this payment requirement becomes invalid. This prevents old payment requests from being reused and adds security against replay attacks.: A unique identifier (UUID) that ensures each payment requirement can only be fulfilled once, even if the same client makes multiple requests to the same endpoint.Client-side implementationAutomatic payment handlingClient libraries wrap standard HTTP clients to automatically handle 402 responses. When a client receives a payment requirement, (1) it constructs a payment payload, (2) signs it with the user's private key, and (3) retries the original request with the payment attached.public HttpResponse<String> makeRequest(String url, String method) throws Exception {
    HttpRequest request = HttpRequest.newBuilder()
        .uri(URI.create(url))
        .method(method, HttpRequest.BodyPublishers.noBody())
        .build();

    HttpResponse<String> response = httpClient.send(request, 
        HttpResponse.BodyHandlers.ofString());

    // Handle 402 Payment Required
    if (response.statusCode() == 402) {
        PaymentRequirements requirements = Json.MAPPER.readValue(
            response.body(), PaymentRequirements.class);

        // Create payment payload matching the requirements
        PaymentPayload payment = PaymentPayload.builder()
            .receiver(requirements.getPaymentRequirement().get(0).getReceiver())
            .amount(requirements.getPaymentRequirement().get(0).getAmount())
            .asset(requirements.getPaymentRequirement().get(0).getAsset())
            .nonce(requirements.getPaymentRequirement().get(0).getNonce())
            .expiry(requirements.getPaymentRequirement().get(0).getExpiry())
            .build();

        // Sign using EIP-712 structured data signing
        String signature = signer.sign(payment.toSigningMap());

        // Retry with payment header
        String paymentHeader = encodePaymentHeader(payment, signature);
        HttpRequest paidRequest = HttpRequest.newBuilder()
            .uri(URI.create(url))
            .method(method, HttpRequest.BodyPublishers.noBody())
            .header("X-PAYMENT", paymentHeader)
            .build();

        return httpClient.send(paidRequest, HttpResponse.BodyHandlers.ofString());
    }

    return response;
}
The signing process uses the  standard, which creates a structured, human-readable representation of the payment data before hashing and signing it. This ensures the payment is cryptographically secure and tied to the specific request.Payment verification flowThe facilitator service is where the blockchain complexity lives, abstracting it away from both clients and servers. When a server receives a payment, it forwards the payment payload to the facilitator for verification.public class HttpFacilitatorClient implements FacilitatorClient {
    private final HttpClient http;
    private final String baseUrl;

    @Override
    public VerificationResponse verify(String paymentHeader, PaymentRequirements requirements) 
        throws Exception {

        // Construct verification request with payment and requirements
        VerifyRequest body = VerifyRequest.builder()
            .paymentHeader(paymentHeader)     // The X-PAYMENT header from client
            .requirements(requirements)       // What the server expects
            .build();

        HttpRequest request = HttpRequest.newBuilder()
            .uri(URI.create(baseUrl + "/verify"))
            .POST(HttpRequest.BodyPublishers.ofString(Json.MAPPER.writeValueAsString(body)))
            .header("Content-Type", "application/json")
            .build();

        String json = http.send(request, HttpResponse.BodyHandlers.ofString()).body();
        return Json.MAPPER.readValue(json, VerificationResponse.class);
    }

    @Override
    public SettlementResponse settle(String paymentHeader, PaymentRequirements requirements) 
        throws Exception {

        // Settlement happens after successful verification
        SettleRequest body = SettleRequest.builder()
            .paymentHeader(paymentHeader)
            .requirements(requirements)
            .build();

        HttpRequest request = HttpRequest.newBuilder()
            .uri(URI.create(baseUrl + "/settle"))
            .POST(HttpRequest.BodyPublishers.ofString(Json.MAPPER.writeValueAsString(body)))
            .header("Content-Type", "application/json")
            .build();

        String json = http.send(request, HttpResponse.BodyHandlers.ofString()).body();
        return Json.MAPPER.readValue(json, SettlementResponse.class);
    }
}
The facilitator checks several things:Does the payment amount match the requirements?Has this payment been used before?Is the payment still within its expiration window?If verification passes, the facilitator also handles settlement by broadcasting the transaction to the blockchain of choice. The  interface defines the contract that any facilitator client must implement:public interface FacilitatorClient {
    VerificationResponse verify(String paymentHeader, PaymentRequirements requirements);
    SettlementResponse settle(String paymentHeader, PaymentRequirements requirements);
}
Your application needs to provide a concrete implementation of this interface.Now that we've seen the individual components (payment filters, facilitator integration, and client handling) let's look at how these pieces come together in a real application. Here's a minimal  example that demonstrates the complete flow.First, create a  annotation:@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface PaymentRequired {
    String price();
    String currency() default "USDC";
    String network() default "base-sepolia";
}
Then modify the  to scan for these annotations at startup:@Component
public class PaymentFilter implements Filter {
    private final Map<String, BigInteger> priceTable;
    private final String payTo;
    private final FacilitatorClient facilitator;

    public PaymentFilter(ApplicationContext context, String payTo, FacilitatorClient facilitator) {
        this.payTo = payTo;
        this.facilitator = facilitator;
        this.priceTable = buildPriceTableFromAnnotations(context);
    }

    private Map<String, BigInteger> buildPriceTableFromAnnotations(ApplicationContext context) {
        Map<String, BigInteger> prices = new HashMap<>();

        // Scan all @RestController beans for @PaymentRequired annotations
        Map<String, Object> controllers = context.getBeansWithAnnotation(RestController.class);

        for (Object controller : controllers.values()) {
            Method[] methods = controller.getClass().getMethods();
            for (Method method : methods) {
                PaymentRequired payment = method.getAnnotation(PaymentRequired.class);
                if (payment != null) {
                    String path = extractPathFromMapping(method);
                    BigInteger amount = new BigInteger(payment.price().replace(".", ""));
                    prices.put(path, amount);
                }
            }
        }
        return prices;
    }
}
Now you can annotate your controller methods directly:@RestController
public class WeatherController {

    @GetMapping("/weather")
    @PaymentRequired(price = "0.001", currency = "USDC", network = "base-sepolia")
    public WeatherData getWeather(@RequestParam String city) {
        // Your existing business logic
        return weatherService.getWeatherForCity(city);
    }

    @GetMapping("/premium-forecast")
    @PaymentRequired(price = "0.01", currency = "USDC", network = "base-sepolia")
    public ExtendedForecast getPremiumForecast(@RequestParam String city) {
        return weatherService.getExtendedForecast(city);
    }
}

@Configuration
public class PaymentConfig {

    @Bean
    public PaymentFilter paymentFilter(ApplicationContext context) {
        return new PaymentFilter(
            context,
            "<WALLET_ADDRESS>", // Your wallet address
            new HttpFacilitatorClient("<FACILITATOR_URL>")
        );
    }

    @Bean
    public FilterRegistrationBean<PaymentFilter> paymentFilterRegistration(PaymentFilter filter) {
        FilterRegistrationBean<PaymentFilter> registration = new FilterRegistrationBean<>();
        registration.setFilter(filter);
        registration.addUrlPatterns("/*");
        registration.setOrder(1);
        return registration;
    }
}
The  annotation handles pricing configuration declaratively, while the  automatically discovers these annotations at startup and builds the price table. Your existing business logic in the controller methods remains completely unchanged. The configuration wires everything together by registering the payment filter and connecting it to a facilitator service. Once deployed, requests to  cost 0.001 USDC and  costs 0.01 USDC, with all payment handling happening transparently at the HTTP layer.Security and production considerations is simple and elegant, it hides complexity. This is a pro and a con. It makes integration easy, but it also hides an important aspect: putting AI agents in charge of money creates new attack vectors.While  signatures and nonce management handle replay attacks, what happens when an agent gets compromised? Traditional fraud detection relies on human behavioral patterns, but AI agents don't follow human spending habits. A compromised agent could drain funds faster than any human fraudster.The facilitator component becomes another high-value target since it's verifying signatures and managing nonces. Unlike traditional payment processors that can reverse transactions, blockchain settlements are final.Since  is based on on-chain transactions, it inherits the operational risks of blockchain transactions. Gas fees fluctuate wildly, sometimes making micropayments economically inconvenient. Network congestion can delay transactions. What's an AI agent supposed to do when it needs real-time data but the payment is stuck in a mempool?Another important aspect is regulation. Compliance varies across jurisdictions with different rules about automated payments, cryptocurrency usage, and data retention. An AI agent making a large volume of micro-transactions across borders might trigger AML alerts or violate local regulations without anyone realizing it.What's interesting about  is the timing. AI agents need autonomous payment capabilities, stablecoins provide a programmable money layer, and blockchain infrastructure has matured enough to handle scalable applications. These pieces haven't aligned before. Internet now needs a new form of payments, and traditional flows weren't built for autonomous agents or micropayments. is compelling thanks to its pragmatic approach. Instead of reinventing payments from scratch, it extends existing HTTP infrastructure. Instead of requiring new integrations, it works with standard patterns that we already understand.The security and operational challenges are real, but they're engineering problems with possible solutions.After nearly three decades,  might finally do what it was designed for: make paying for things on the internet as simple as requesting them.The foundation is set. Now it's time to build.Resources and Further Reading]]></content:encoded></item><item><title>Invisible Filters in Power BI? Here&apos;s the Deep Fix That Worked</title><link>https://hackernoon.com/invisible-filters-in-power-bi-heres-the-deep-fix-that-worked?source=rss</link><author>Grigory Koftaylov</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:49:19 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Long story short, once I found that a report I had previously created showed strange data. I can’t show you real visuals as they contain data I can’t disclose, so they are just blank in the article or are mocked. Please, imagine you are expecting to see a few hundred thousand but seeing only hundreds.\
All visible slicers were not set up, but the numbers were too low. Thankfully, the Filters feature helps us understand what filters are applied to the visual:The second filter is legitimate, so there is no problem there. But another one showed one value of the “National name” field. This filtration was applied to all visuals on the page. The is such a slicer on the page, however, it was not set up (it showed All), and the Filters pane showed this filter — so slicers have nothing to do with it. To prove this point, I removed all visuals from the list except the table — the filter was still applied.All suggestions I found while surfing the internet led to checking hidden slicers, which I had already done. It seems to me like a bug or glitch, which appeared somehow while I was editing other pages and the data model.One option is to rebuild the whole page from scratch or at least copy and paste all visuals onto a new page and set them up. However, there were many tricky interaction relationships between them, and this approach would require manual setup and is time-consuming. It would be a problem since there were four dozen visuals, and recreating the correct relationships would be a nightmare. Roughly 1500 clicks would be needed to change the size of visuals and their locations to be able to click on an interaction icon, and then return everything to its place. No, thanks. If there’s a chance to avoid that, I’ll take it.Attempts to delete or rename either the value or field did not help.So the only way left is to edit the internal files of the report.Well, here we come to the main point of the article. There are two parts. The first part will tell you how to get to internal files and a few things about their structure, if you never edit them probably you should read it first. The second part will provide the final short solution that worked well for me, so if you have some experience you can just jump to it.I suggest making a copy of your report to work with in case something goes wrong. If you work in the standard Windows Explorer, you might need to turn on “Show file extensions” in its settings.Rename the file, changing .pbix to .zip, and agree with the warning.Extract the files and go to the resulting folder.Here you can see the files in which Power BI stores all report data. For an explanation of these files, refer to this article.The file we are looking for is in the Report folder and is called Layout. You can use any app you prefer to navigate and edit it. Standard MS Notepad lacks advanced features, so I prefer Notepad++ for editing text files. It has syntax highlighting and different plugins. For working with JSON, I used JSONTools. If you know exactly what and where to edit, you might use just Notepad, but for the first time, it makes sense to use advanced tools.This is how it looks in plain text view. To improve readability, use the “Pretty-print current JSON file” function of the JSONTools plugin.If your file is well-structured but lacks syntax highlighting, you can add it by selecting Language -> JSON.Now it looks clearer, however, some of the values contain strings of JSON, so instead of a full tree, we see only one block.To see all data in a tree view, replace some symbols. Warning! The following actions will break the initial structure of the file and make it unusable for reassembling the Power BI report. Use this only for learning and investigation.It might be helpful to create a Macro in Notepad++ and save these actions for future use.After replacing, use the “Pretty-print current JSON file” of the JSONTools plugin again.Now it looks much better, and we can use it for exploration. For easier navigation, you can open the JSON Tree viewer from the JSONTools plugin. It looks similar to the Notepad view but shows the number of values for each key and data type.Now we can go deeper into the structure of the Layout file.The clause here is exactly what we have in the test report:If a visual has a filter set up through the Filter pane, we find the clause in the Filters value as well:This is some theory obtained from parsing the Layout file of a simple test report. It’s time to look at the Layout from the broken live report. Open the file, transform it for better exploration, and find the required filter in the data query block. Also, there is a non-empty Filter block, which filters by another field, so it’s valid.Now we understand the structure of the file and I can try to look at the initial untransformed file. The only way to find the places we are looking for is to search by names used in the report. Fortunately, we have the value of the problematic field. There were 3 dozen such places in the file. Here you can see the selected part of the Where clause with the problematic condition:Now we can delete the Condition section (in all clauses where they are met), or delete the whole Where section if there is only one condition. Notepad++ helps you to identify beginning and end brackets with highlights.After deleting, save the file. If you made a copy, move it to another folder — there should be only one Layout file in the Report folder.Now, there’s one last step which took me a lot of hours — if you skip it, your pbix report won’t open even if you made no mistakes editing the Layout file.You should delete the SecurityBindings file from the root folder of the report.According to the article mentioned earlier, this file contains Row-Level Security data. Keeping it before reassembling the report will prevent it from opening. Since I have no RLS in this report, it’s not a problem to delete it. At least, I haven’t found any issues in the report after doing so.Delete the SecurityBindings file and zip the report folder: select all files and folders, and send them to a Compressed (zipped) folder. Then change the extension back to .pbix and try to open it. If done correctly, your file will successfully open with Power BI Desktop, and you will find your report is fixed.If you get an error while opening the report, you might have made a mistake editing the files. Try to repeat all steps from scratch. You can make small changes step-by-step and check if there are errors after each step.]]></content:encoded></item><item><title>Tech Leaders Need First Principles Thinking Now More Than Ever</title><link>https://hackernoon.com/tech-leaders-need-first-principles-thinking-now-more-than-ever?source=rss</link><author>Archith</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:47:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[You're a leader, more specifically, one that works with technology, engineering, or product development—if you're reading this. You're faced with hard problems every day. As easy as it is to simply leap headfirst into the tried and true, I've learned over the years that one of the most effective mental models is to step back and apply First Principles Thinking.What Exactly is First Principles Thinking?First principles thinking is literally just breaking down complicated problems into their basic, underlying facts and rebuilding solutions afresh from scratch. Instead of analogizing solutions from one's own background (or worse, copying competitors), this process forces you to question assumptions, drill down, and generate true innovation.Why Does This Matter in Tech?First, a reality check: tech leaders are perpetually under pressure. Pressure from stakeholders, competitors, or even our own aggressive timetables. Under constant pressure, it's all too easy to fall back on generic playbooks or industry "best practices."Here's the problem: best practices are average solutions to typical challenges. They will seldom provide breakthrough outcomes.By taking a step back and applying first principles, you'll be able to find new solutions. Like when you're scaling a platform, it's easy to rush in and just throw more infrastructure or conventional scaling techniques at the problem. But consider this, what's really the limitation here? Is it compute, is it latency, or maybe it's a suboptimal data architecture that's inherently broken?Practical Application: Scaling Without Blindly Adding CostsRecently, my team faced frenetic scaling problems after a product launch that led to excessive traffic. GUT reaction: give more AWS resources—everyone knows that's the easiest thing to do. However, I waited and wore first principles thinking instead:Pinpoint the root constraints: We plunged into performance metrics. Was the server load? Database contention? API inefficiencies?Challenge every assumption: We had initially presumed that we needed more resources. But was our data structure efficient enough? Were our caching techniques maximized? Were certain features too resource-intensive when they did not have to be?Rebuild the solution from fundamentals: We realized that the real problem wasn't server capacity; it was how often we cached and repeatedly called expensive database queries. By revisiting our caching strategy in its most basic form, we greatly reduced costs and improved performance.A helpful technique to help with first principles thinking application is the "Five Whys" technique, notoriously created by Toyota:Because the servers are overloaded.Why are the servers overloaded?Because there's too much traffic from repeated database calls.Why are there repeated database calls?Because the cache refreshes too often.Why does the cache refresh often?Because it is based on a badly made update interval choice.Why was this update interval choice made?Due to an old assumption about user behavior.By repeating "why" several times, we found the root cause and fixed the fundamental issue rather than giving a superficial solution.First Principles Framework for PracticeHere's a simple-to-use but effective framework that you can apply to practice first principles thinking: Clearly articulate what you're trying to solve. Define and break down the problem elements. Boldly challenge each element and assumption. Use the Five Whys or other methods to drill down to the root causes. Build new solutions from these absolute truths.Beyond Technical ProblemsFirst principles aren't exclusive to technology. They're also robust in leadership contexts—hiring, organizational structure, and even communication strategy.With a consistent drip of employee turnover, rather than showering employees with more benefits or bonuses, you wonder: Why are people actually quitting? Is it compensation, chances for growth, or perhaps lack of meaningful work?By cutting it down to first principles, you'll be more likely to find simpler, but better answers.Quick Tips to Practice First Principles ThinkingAttack assumptions aggressively: Assume nothing is holy. Drill deep to an anchor truth. If it looks too complex, you probably haven't cut it thinly enough.First principles thinking won't always be the simplest or fastest route—at first. But the clarity and imagination it provides are unmatchable. In a rapidly shifting tech world, that clarity isn't just worth its weight in gold; it's mandatory.Remember, average strategies have average results. The next time you find yourself faced with complexity, try applying first principles—it may very well change how you lead.What's your experience been with first principles thinking? Have you uncovered some unexpected epiphanies by pushing back against assumptions? Tell us your tale in the comments below ?]]></content:encoded></item><item><title>They Promised a Bonus. Then They Vanished. The Upwork Scam No One Sees Coming</title><link>https://hackernoon.com/they-promised-a-bonus-then-they-vanished-the-upwork-scam-no-one-sees-coming?source=rss</link><author>Shayam Thomas Mohan Ram</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:45:27 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Upwork is supposed to be a place of opportunity. But for many freelancers, it becomes a minefield of scams, manipulation, and wasted time. Below, we bring you real titles and stories from actual freelancers who got burned — so you don’t have to.“I was invited to a voice recording job. The catch? The payment would be sent as a bonus after task completion. Turns out bonus payments aren’t protected if you haven’t worked with the client before. Upwork did nothing when the client vanished.”Lesson: Never accept jobs where payment is offered solely as a bonus. Use milestone or hourly contracts with tracked time to stay protected.“The client filed a chargeback after Upwork ruled in my favor during arbitration. Upwork took the money back anyway and locked my withdrawals.”Lesson: Even “won” disputes can go south. Learn how chargebacks work and avoid clients who seem to look for loopholes. If possible, prefer hourly contracts with work diaries.“After delivering the design files, the client went silent. Then they suddenly disputed the milestone and claimed I overbilled.”Lesson: Always send preview-only versions of your work until payment is received. Document all client approvals in messages.“Got a message offering a copywriting job. They sent a Telegram handle, then a shady form with personal questions and a bonus-based payment model.”✅ Lesson: Any job that pulls you off-platform early is usually a scam. Stay inside Upwork’s ecosystem until trust is built — and  give personal info outside contracts.“Not kidding. They wanted a urine test and a psychological profile. For a part-time remote assistant gig. Absolutely unhinged.”Lesson: You don’t owe anyone your bodily fluids. This is an extreme red flag. Decline and report such behavior immediately.“I submitted my ID and did a video call, but Upwork still banned me. I lost access to all my pending payments.”Lesson: Keep backups of everything: contracts, work files, and withdrawal receipts. Don’t rely 100% on a single platform — have an exit strategy.“Spent a ridiculous amount boosting proposals. Got ignored or ghosted. Some jobs looked real but never had any intention of hiring.”Lesson: Don’t assume more Connects = more chances. Look for red flags: clients with zero hires, vague descriptions, or ‘too good to be true’ budgets.“Initially it was one email. Then two. Then landing pages. All under the same milestone. I’m being squeezed dry.”Lesson: Scope creep is real. Define hard boundaries in your contract, and pause work the moment deliverables go beyond what was agreed.“Client didn’t leave feedback. But my JSS dropped immediately after. Did I just get silently punished?”Lesson: Clients can leave private feedback even if they say nothing publicly. Always aim to wrap projects with clear deliverables and satisfaction.“I was about to accept the contract, then noticed the client’s account was brand new. The next day it was suspended. I could’ve been trapped.”Lesson: Always check client history: hiring rate, reviews, and time on the platform. If it’s too new, it might be fake.These are not just stories — they are  from real people trying to earn a living. Every freelancer should:Protect deliverables until payment.Avoid off-platform communication.💬 “If it feels off, it probably is. You’re better off walking away than working for free.”Stay sharp out there. The freedom of freelancing is powerful — but only if you protect it.\
Tired of scams, ghost clients, and wasted connects on Upwork?You’re not alone — and you don’t have to fly blind anymore.I’ve started using , a smart tool that scans and scores Upwork jobs  you apply.Flags suspicious or scammy postsHighlights legit, high-quality clientsSaves your connects and your sanityHelps you spot red flags like the stories above — before you live themIf you’re serious about protecting your time, money, and mental health on Upwork, .]]></content:encoded></item><item><title>AI Didn’t Replace Me—It Supercharged My Phpstorm Workflow</title><link>https://hackernoon.com/ai-didnt-replace-meit-supercharged-my-phpstorm-workflow?source=rss</link><author>Lala</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:43:22 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[I was scared like anyone else that “AI will replace me.”But with my experience using AI tools daily, I’ve embraced the cliché: \n AI doesn’t replace me — it speeds me up.But  because I’m learning how to use it properly. Let’s start with Github Co-pilot.As a frontend developer, my job involves building features, debugging, and constantly improving things across large codebases. Copilot has become a tool I rely on almost every day — not to do the work for me, but to help me start faster, move smarter, and catch things earlier.Today, I’m breaking down how I actually use GitHub Copilot inside PhpStorm — where it works , and where you still need to lead.What makes Copilot inside PhpStorm so useful is this:It understands the file, the code, and the project context.This means I can write a comment like:// Service to validate student assignment submissions
…and Copilot will generate a solid starting point — not just random boilerplate, but something relevant to the structure of my project.This kind of head start makes a huge difference when starting something from scratch. It saves me from staring at a blank file, remembering syntaxes and starting empty functions. Instead, I’m editing and evolving, which is much faster than writing from zero.But here’s the thing: You need a strong mental model of what you’re building before AI can be helpful.How I Use GitHub Copilot Daily Let me walk you through some real-life use cases that have made the most impact:Kickstarting new features I often create starter code for new features by prompting Copilot based on what the file or module is meant to do. It gives me a first draft, and from there, I build step-by-step toward the final version.It’s not about skipping thinking — it’s about saving time on repetitive scaffolding.Reusing patterns from past work A while ago, I experimented with dnd-kit to build a custom drag-and-drop interface. When I later needed to build sortable question lists for a live feature, I pointed Copilot to the earlier implementation.It recognized the logic and followed the same pattern — helping me repurpose tested ideas in seconds.That saved me hours of research and fiddling.Code reviews — before anyone else sees my PR One of my favorite ways to use Copilot is to run a pre-review on my code. I have a simple custom prompt that I use for this.It flags inconsistencies, suggests better naming, logic issues — even potential edge cases. It also suggests how I can improve accessibility and how the code can be more maintainable.This gives me confidence before sharing my work with the team. I treat it as my private reviewer.Debugging: Smarter logging suggestions Copilot can be a game-changer to help debug code. One great trick is asking:“Where should I put console logs to help you debug this error?”It often points me to helpful log spots. Then I run the app, collect logs, and ask Copilot what they mean.Sometimes this saves a lot of time. Sometimes… not so much. AI can go in circles — and you need to know when to stop and rely on your instincts.The Truth About AI-Generated Code Let me be clear:You can’t just accept what Copilot gives you.AI can hallucinate. It can suggest things that look right — but aren’t. If you don’t understand the micro-details of your system, it’ll slow you down.Using Copilot well requires skill. You have to guide it, check its work, and sometimes work around its wrong turns.But when you do — when you know how to prompt it smartly, use context, and course-correct — it makes you faster, not just lazier.Final Thoughts I didn’t learn this overnight. It came from trying, failing, and figuring out what works over the past year.But today, GitHub Copilot is one of the few tools I’d be reluctant to work without. Not because it’s perfect — but because it makes me better.And that’s what good tools are supposed to do.]]></content:encoded></item><item><title>James Webb Space Telescope Discovers Its First Exoplanet</title><link>https://science.slashdot.org/story/25/06/25/2112219/james-webb-space-telescope-discovers-its-first-exoplanet?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Jun 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The James Webb Space Telescope has discovered its first new exoplanet, TWA 7b -- a young, low-mass planet about 100 times the mass of Earth, making it the lightest planet ever directly imaged beyond the solar system. Space.com reports: TWA 7b was discovered in the debris rings that surround the low-mass star CE Antilae, also known as TWA 7, located around 111 light-years from Earth. CE Antilae is a very young star, estimated to be around just a few million years old. If that seems ancient, consider the sun, a "middle-aged" star, is around 4.6 billion years old.
 
[...] The disk of CE Antilae is divided into three distinct rings, one of which is narrow and bounded by two empty "lanes" mostly devoid of matter. When imaging this ring, the JWST spotted an infrared-emitting source, which the team of astronomers determined is most likely a young exoplanet. They then used simulations that confirmed the formation of a thin ring and a "hole" exactly where this planet is positioned, corresponding to JWST observations. The research has been published in the journal Nature.]]></content:encoded></item><item><title>Meta’s recruiting blitz claims three OpenAI researchers</title><link>https://techcrunch.com/2025/06/25/metas-recruiting-blitz-claims-three-openai-researchers/</link><author>Connie Loizos</author><category>tech</category><pubDate>Thu, 26 Jun 2025 04:50:12 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Zuckerberg has been dangling $100+ million compensation packages in an effort to lure top talent from OpenAI, with mixed results.]]></content:encoded></item><item><title>Swarms of Tiny Nose Robots Could Clear Infected Sinuses, Researchers Say</title><link>https://hardware.slashdot.org/story/25/06/25/218250/swarms-of-tiny-nose-robots-could-clear-infected-sinuses-researchers-say?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Thu, 26 Jun 2025 03:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Guardian: Swarms of tiny robots, each no larger than a speck of dust, could be deployed to cure stubborn infected sinuses before being blown out through the nose into a tissue, researchers have claimed. The micro-robots are a fraction of the width of a human hair and have been inserted successfully into animal sinuses in pre-clinical trials by researchers at universities in China and Hong Kong. Swarms are injected into the sinus cavity via a duct threaded through the nostril and guided to their target by electromagnetism, where they can be made to heat up and catalyze chemical reactions to wipe out bacterial infections. There are hopes the precisely targeted technology could eventually reduce reliance on antibiotics and other generalized medicines.
 
[...] The latest breakthrough, based on animal rather than human trials, involves magnetic particles "doped" with copper atoms which clinicians insert with a catheter before guiding to their target under a magnetic field. The swarms can be heated up by reacting to light from an optical fibre that is also inserted into the body as part of the therapy. This allows the micro-robots to loosen up and penetrate viscous pus that forms a barrier to the infection site. The light source also prompts the micro-robots to disrupt bacterial cell walls and release reactive oxygen species that kill the bacteria.
 
The study, published in Nature Robotics, showed the robots were capable of eradicating bacteria from pig sinuses and could clear infections in live rabbits with "no obvious tissue damage." The researchers have produced a model of how the technology could work on a human being, with the robot swarms being deployed in operating theatre conditions, allowing doctors to see their progress by using X-rays. Future applications could include tackling bacterial infections of the respiratory tract, stomach, intestine, bladder and urethra, they suggested. "Our proposed micro-robotic therapeutic platform offers the advantages of non-invasiveness, minimal resistance, and drug-free intervention," they said.]]></content:encoded></item><item><title>Indian drone startup Raphe mPhibr raises $100M as military UAV demand soars</title><link>https://techcrunch.com/2025/06/25/indian-drone-startup-raphe-mphibr-raises-100m-as-military-uav-demand-soars/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Thu, 26 Jun 2025 02:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[The startup aims to boost its R&D and local production capabilities amid growing demand for drones in battlefields and for border surveillance.]]></content:encoded></item><item><title>Meta Beats Copyright Suit From Authors Over AI Training on Books</title><link>https://tech.slashdot.org/story/25/06/25/2127222/meta-beats-copyright-suit-from-authors-over-ai-training-on-books?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 01:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader shares a report: Meta escaped a first-of-its-kind copyright lawsuit from a group of authors who alleged the tech giant hoovered up millions of copyrighted books without permission to train its generative AI model called Llama. 

San Francisco federal Judge Vince Chhabria ruled Wednesday that Meta's decision to use the books for training is protected under copyright law's fair use defense, but he cautioned that his opinion is more a reflection on the authors' failure to litigate the case effectively. "This ruling does not stand for the proposition that Meta's use of copyrighted materials to train its language models is lawful," Chhabria said.]]></content:encoded></item><item><title>Microsoft Sued By Authors Over Use of Books in AI Training</title><link>https://news.slashdot.org/story/25/06/25/2137220/microsoft-sued-by-authors-over-use-of-books-in-ai-training?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 00:50:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft has been hit with a lawsuit by a group of authors who claim the company used their books without permission to train its Megatron artificial intelligence model. From a report: Kai Bird, Jia Tolentino, Daniel Okrent and several others alleged that Microsoft used pirated digital versions of their books to teach its AI to respond to human prompts. Their lawsuit, filed in New York federal court on Tuesday, is one of several high-stakes cases brought by authors, news outlets and other copyright holders against tech companies including Meta Platforms, Anthropic and Microsoft-backed OpenAI over alleged misuse of their material in AI training. 

[...] The writers alleged in the complaint that Microsoft used a collection of nearly 200,000 pirated books to train Megatron, an algorithm that gives text responses to user prompts.]]></content:encoded></item><item><title>NVIDIA Engineer Now Co-Maintainer Of &quot;NOVA&quot; Open-Source Rust GPU Driver</title><link>https://www.phoronix.com/news/NOVA-Core-Co-Maintainer</link><author>Michael Larabel</author><category>tech</category><pubDate>Thu, 26 Jun 2025 00:37:04 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The NOVA-Core driver as the basis for a modern, Rust-written open-source NVIDIA GPU driver for the upstream Linux kernel and eventual successor to the reverse-engineered Nouveau DRM driver has a new co-maintainer...]]></content:encoded></item><item><title>Aaron Sorkin&apos;s The Social Network Sequel Officially in Development</title><link>https://entertainment.slashdot.org/story/25/06/25/2144252/aaron-sorkins-the-social-network-sequel-officially-in-development?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Thu, 26 Jun 2025 00:10:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Aaron Sorkin is officially working on a sequel to The Social Network. From a report: Last year, the Oscar-winning writer revealed he was working on a film that would revisit the subject of Facebook, and Deadline has now reported that The Social Network Part II is in development at Sony Pictures yet isn't a "straight sequel." 

The original film, which traced the early days of Facebook and its creator Mark Zuckerberg, was directed by David Fincher. Sorkin is rumoured to be directing the follow-up. "I blame Facebook for January 6," he said in 2024 on a special edition of The Town podcast, live from Washington DC. When asked to explain why, he responded: "You're gonna need to buy a movie ticket." 

The Social Network was an adaptation of Ben Mezrich's book The Accidental Billionaires, and the sequel will be based on the Wall Street Journal series The Facebook Files. The 2021 investigation examined the damage caused by the social networking site and how internal findings had been buried. Subjects included the influence on the January 6 riot and the mental health of teenage users.]]></content:encoded></item><item><title>Federal judge sides with Meta in lawsuit over training AI models on copyrighted books</title><link>https://techcrunch.com/2025/06/25/federal-judge-sides-with-meta-in-lawsuit-over-training-ai-models-on-copyrighted-books/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 25 Jun 2025 23:40:32 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A federal judge sided with Meta on Wednesday in a lawsuit brought against the company by 13 book authors, including Sarah Silverman, that alleged the company had illegally trained its AI models on their copyrighted works.]]></content:encoded></item><item><title>Brad Feld on ‘Give First’ and the art of mentorship (at any age)</title><link>https://techcrunch.com/2025/06/25/brad-feld-on-give-first-and-the-art-of-mentorship-at-any-age/</link><author>Connie Loizos</author><category>tech</category><pubDate>Wed, 25 Jun 2025 23:31:20 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Brad Feld has spent decades operating by a simple principle: Give without expecting anything in return. ]]></content:encoded></item><item><title>US Senators Push For American Version of EU&apos;s Digital Markets Act</title><link>https://news.slashdot.org/story/25/06/25/214209/us-senators-push-for-american-version-of-eus-digital-markets-act?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 25 Jun 2025 23:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[U.S. lawmakers have reintroduced the bipartisan Open App Markets Act, aiming to curb Apple and Google's control over mobile app stores by promoting competition, supporting third-party marketplaces and sideloading, and safeguarding developer rights. AppleInsider reports: The Open App Markets Act seeks to do a number of things, including:
- Protect developers' rights to tell consumers about lower prices and offer competitive pricing;
- Protect sideloading of apps;
- Promote competition by opening the market to third-party app stores, startup apps, and alternative payment systems;
- Make it possible for developers to offer new experiences that take advantage of consumer device features;
- Give consumers greater control over their devices;
- Prevent app stores from disadvantaging developers; and
- Establish safeguards to preserve consumer privacy, security, and safety.
 
This isn't the first time we've seen this bill, either. In 2021, Senators Blumenthal, Klobuchar, and Blackburn had attempted to put forth the original version of the Open App Markets Act.However, the initial bill never made it to the floor for an office vote. Thanks to last-minute efforts by lobbying groups and appearances from chief executives, the bill eventually stalled out.
 
While the two bills are largely similar, the revised version introduces several key differences. Notably, the new version includes new carve-outs aimed at protecting intellectual property and addressing potential national security concerns.There's also a new clause that would prohibit punitive actions against developers for enabling remote access to other apps. The clause addition harkens back to the debacle between Apple and most game streaming services -- though in 2024, Apple loosened its App Store guidelines to allow cloud gaming and emulation.
 
There are a few new platform-protective clauses added, too. For instance, it would significantly lower the burden of proof for either Apple or Google to block platform access to a third-party app.Additionally, it reinforces the fact that companies like Apple or Google will not need to provide support or refunds for third-party apps installed outside of first-party app marketplaces. 
The full bill can be found here.]]></content:encoded></item><item><title>Sam Altman comes out swinging at The New York Times</title><link>https://techcrunch.com/2025/06/25/sam-altman-comes-out-swinging-at-the-new-york-times/</link><author>Maxwell Zeff</author><category>tech</category><pubDate>Wed, 25 Jun 2025 20:54:57 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[From the moment OpenAI CEO Sam Altman stepped onstage, it was clear this was not going to be a normal interview.]]></content:encoded></item><item><title>Anthropic destroyed millions of print books to build its AI models</title><link>https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-1152x648.jpg" length="" type=""/><pubDate>Wed, 25 Jun 2025 20:00:03 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[On Monday, court documents revealed that AI company Anthropic spent millions of dollars physically scanning print books to build Claude, an AI assistant similar to ChatGPT. In the process, the company cut millions of print books from their bindings, scanned them into digital files, and threw away the originals solely for the purpose of training AI—details buried in a copyright ruling on fair use whose broader fair use implications we reported yesterday.The 32-page legal decision tells the story of how, in February 2024, the company hired Tom Turvey, the former head of partnerships for the Google Books book-scanning project, and tasked him with obtaining "all the books in the world." The strategic hire appears to have been designed to replicate Google's legally successful book digitization approach—the same scanning operation that survived copyright challenges and established key fair use precedents.While destructive scanning is a common practice among some book digitizing operations, Anthropic's approach was somewhat unusual due to its documented massive scale. By contrast, the Google Books project largely used a patentednon-destructive camera process to scan millions of books borrowed from libraries and later returned. For Anthropic, the faster speed and lower cost of the destructive process appears to have trumped any need for preserving the physical books themselves, hinting at the need for a cheap and easy solution in a highly competitive industry.]]></content:encoded></item><item><title>Ubuntu disables Intel GPU security mitigations, promises 20% performance boost</title><link>https://arstechnica.com/security/2025/06/ubuntu-disables-intel-gpu-security-mitigations-promises-20-performance-boost/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/06/graphics-1152x648.jpg" length="" type=""/><pubDate>Wed, 25 Jun 2025 19:39:19 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT – Ars Technica</source><content:encoded><![CDATA[Ubuntu users could see up to a 20 percent boost in graphics performance on Intel-based systems under a change that will turn off security mitigations for blunting a class of attacks known as Spectre.Spectre, you may recall, came to public notice in 2018. Spectre attacks are based on the observation that performance enhancements built into modern CPUs open a side channel that can leak secrets a CPU is processing. The performance enhancement, known as speculative execution, predicts future instructions a CPU might receive and then performs the corresponding tasks before they are even called. If the instructions never come, the CPU discards the work it performed. When the prediction is correct, the CPU has already completed the task.By using code that forces a CPU to execute carefully selected instructions, Spectre attacks can extract confidential data that the CPU would have accessed had it carried out the ghost instructions. Over the past seven years, researchers have uncovered multiple attack variants based on the architectural flaws, which are unfixable. CPU manufacturers have responded by creating patches in both micro code and binary code that restrict speculative execution operations in certain scenarios. These restrictions, of course, usually degrade CPU performance.]]></content:encoded></item><item><title>AVX-512&apos;s Enormous Advantage For AMD EPYC 4005 Series Performance</title><link>https://www.phoronix.com/review/amd-epyc-4005-avx512</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 19:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The AMD EPYC 4005 "Grado"" processors launched by AMD in May for entry-level servers offer downright amazing value, performance, and power efficiency over the Intel Xeon 6300 / Xeon E-2400 series competition. Intel's top-of-stack Xeon 6300 (Xeon 6369P) / Xeon E processors fail to compete with even the mid-tier EPYC 4005 series processors in either performance, power, or cost effectiveness. Among the many advantages to these budget-friendly EPYC processors is having AVX-512 support with a full 512-bit data path compared to the Xeon 6300 series only having AVX2. For providing more insight into the AVX-512 performance impact with the AMD EPYC 4005 series, here are some enabled/disabled comparison benchmarks and how they are positioned relative to the Xeon 6369P server processor.]]></content:encoded></item><item><title>Proposal To Ship XLibre As X11 Server Packages On Fedora Linux Is Withdrawn</title><link>https://www.phoronix.com/news/Fedora-XLibre-Proposal-Withdraw</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 18:33:02 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The controversial proposal to replace the upstream X.Org X11 server packages on Fedora Linux with XLibre is not going to happen... At least not for now. The change proposal has been withdrawn prior to being voted on by the Fedora Engineering and Steering Committee (FESCo)...]]></content:encoded></item><item><title>Mozilla Formally Discontinues Its DeepSpeech Project</title><link>https://www.phoronix.com/news/Mozilla-DeepSpeech-Discontinued</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 18:15:56 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[One of the interesting projects engaged in by Mozilla that directly wasn't related to their web browser efforts was DeepSpeech, an embedded/offline speech-to-text engine. To not much surprise given the lack of activity in recent years, last week they finally and formally discontinued the open-source project...]]></content:encoded></item><item><title>RADV Driver Introduces Limited Support For NVIDIA Cooperative Matrix Extension</title><link>https://www.phoronix.com/news/RADV-NV-Coperative-Matrix2</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 13:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Mesa Radeon Vulkan driver "RADV" has merged support for the VK_NV_cooperative_matrix2 NVIDIA Vulkan extension but it's hidden by default and only partially supported with a focus on helping FidelityFX Super Resolution 4 and VKD3D-Proton...]]></content:encoded></item><item><title>Estonia Debuts AI Chatbots for High School Classrooms</title><link>https://spectrum.ieee.org/estonia-ai-leap</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEwMzczNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc4ODQzODg4Nn0.MPutpQxbnQXwqa4Y9YhUq7sxHIhb9YmIb8JwISXFlGM/image.png?width=600" length="" type=""/><pubDate>Wed, 25 Jun 2025 12:33:36 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Students are meant to use AI for learning rather than cheating]]></content:encoded></item><item><title>Intel Preps Linux PTC &quot;Throttling Control Interface&quot; To Run Hotter For Better Performance</title><link>https://www.phoronix.com/news/Intel-PTC-Throttling-Control</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 12:10:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With the Linux 6.16 kernel Intel enabled the new Platform Temperature Control (PTC) interface as part of their int340x thermal driver. Now ahead of the Linux 6.17 kernel Intel PTC is being extended to support a Throttling Control Interface for those that may prefer running their system(s) hotter in order to enjoy better performance...]]></content:encoded></item><item><title>NIST Unveils a Verifiable Quantum Random-Number Beacon</title><link>https://spectrum.ieee.org/nist-quantum-random-number-generator</link><author>Edd Gent</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTEwNDg2OC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTgwOTgzMTgyNX0.VQwAF7Dhan81zccJrO2w6rKs9KXylJNl3rbRmVmPiUA/image.jpg?width=600" length="" type=""/><pubDate>Wed, 25 Jun 2025 12:00:04 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[CURBy uses quantum mechanics to create a more robust RNG than other options]]></content:encoded></item><item><title>Fairphone 6 Announced With Same-Day Linux Support Patches</title><link>https://www.phoronix.com/news/Fairphone-6-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 10:20:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Dutch electronics company Fairphone today announced their Fairphone Gen 6 smartphone as the successor to the Fairphone 5. Fairphone 6 continues to be repair-friendly and was just announced this morning while already the Linux support patches have hit the Linux kernel mailing list...]]></content:encoded></item><item><title>Intel Open-Source Software Begins Adding Notices For Generative AI Use</title><link>https://www.phoronix.com/news/Intel-OSS-Gen-AI-Usage</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 10:05:12 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Intel open-source software projects are beginning to relay notices that they may have been developed with support from Intel-operated generative AI solutions...]]></content:encoded></item><item><title>PNG Spec Updated For HDR Images &amp; Animated PNGs</title><link>https://www.phoronix.com/news/PNG-Specification-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 09:42:29 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[While WebP and AVIF generate much of the interest these days from a tech perspective for modern image formats, the PNG image format was just updated with new features...]]></content:encoded></item><item><title>Former Btrfs Lead Developer Chris Mason Announces New &quot;rsched&quot; Tool</title><link>https://www.phoronix.com/news/rsched</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 25 Jun 2025 00:27:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Open-source Linux developer Chris Mason who is known for being the original lead developer of the Btrfs file-system recently began hacking on a new tool that he announced today, rsched...]]></content:encoded></item><item><title>Pokemon Scarlet &amp; Violet Switch 2 Review - More To Chewtle On</title><link>https://www.gamespot.com/reviews/pokemon-scarlet-violet-review-a-braviary-new-world/1900-6417994/?ftag=CAD-01-10abi2f</link><author>Jake Dekker</author><category>tech</category><enclosure url="https://www.gamespot.com/a/uploads/screen_medium/43/434805/4057910-8965418612-pokem.jpg" length="" type=""/><pubDate>Tue, 24 Jun 2025 18:56:00 +0000</pubDate><source url="https://www.gamespot.com/feeds/reviews/">GameSpot - Game Reviews</source><content:encoded><![CDATA[With the launch of the Nintendo Switch 2, Pokemon Scarlet and Violet have received a much-needed performance boost. The free update adds 4K visuals while docked and a rock-solid 60 frames per second for both docked and handheld play. After several hours of testing, I've found both Scarlet and Violet run and play  better.On the original Switch, a stormy Casseroya Lake in the northwestern part of Paldea was particularly taxing--even after a handful of updates, I dreaded going to that lake to collect items or shiny hunt. Now, on the Switch 2, it runs flawlessly regardless of where you are in the world. Additionally, the lengthy loading times have been reduced to a few seconds.Despite these improvements, Pokemon Scarlet and Violet still isn't the most visually appealing Pokemon game to date. The world feels bland and barren, and character models--outside of the wonderfully detailed and expressive Pokemon--are simplistic and wooden. Despite the 4k resolution, there are still plenty of low-quality textures and visual bugs can occur during battle. It's a shame given how strong the visual identity is for something like Pokemon Let's Go! Pikachu and Eevee or even Pokemon: Legends Arceus, which received a fair bit of criticism for its visuals as well.Continue Reading at GameSpot]]></content:encoded></item></channel></rss>