{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":13,"items":[{"title":"Introducing Gemma 3n","url":"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/","date":1750957423,"author":"bundie","guid":231,"unread":true,"content":"<img src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3n_Wagtial_RD1-V02.original.jpg\" alt=\"Introducing Gemma 3n: The developer guide\"><div><p data-block-key=\"0lwbc\">The <a href=\"https://blog.google/technology/developers/gemma-open-models/\">first Gemma model</a> launched early last year and has since grown into a thriving <a href=\"https://deepmind.google/models/gemma/gemmaverse/\">Gemmaverse</a> of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like <a href=\"https://deepmind.google/models/gemma/gemmaverse/roboflow/\">Roboflow</a> building enterprise computer vision to the <a href=\"https://deepmind.google/models/gemma/gemmaverse/gemma-2-llama-swallow/\">Institute of Science Tokyo</a> creating highly-capable Japanese Gemma variants, your work has shown us the path forward.</p><p data-block-key=\"8lqqe\">Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While <a href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\">last month's preview</a> offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It’s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.</p><p data-block-key=\"10f0e\">Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.</p></div><div><ul><li data-block-key=\"b4rlm\"> Gemma 3n natively supports image, audio, video, and text inputs and text outputs.</li></ul><ul><li data-block-key=\"belt8\"> Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on <a href=\"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/#per-layer-embeddings-(ple):-unlocking-more-memory-efficiency\"></a> parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory.</li></ul><ul><li data-block-key=\"19kb\"><b>Groundbreaking architecture:</b> At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases.</li></ul><ul><li data-block-key=\"83m4e\"> Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.</li></ul></div><div><p data-block-key=\"0lwbc\">Achieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n’s unique mobile-first architecture, and it all starts with MatFormer.</p><p data-block-key=\"26ehe\">At the core of Gemma 3n is the <a href=\"https://arxiv.org/abs/2310.07707\"></a><b> (🪆Matryoshka Transformer)</b>, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of <a href=\"https://huggingface.co/papers/2205.13147\">Matryoshka Representation Learning</a> from just embeddings to all transformer components.</p></div><div><p data-block-key=\"0lwbc\">During the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today:</p><p data-block-key=\"2r57\">1: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference.</p><p data-block-key=\"dsd6a\">2:<b> Custom sizes with Mix-n-Match:</b> For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the <a href=\"https://goo.gle/gemma3n-matformer-lab\">MatFormer Lab</a>, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.</p></div><div><p data-block-key=\"0lwbc\">Looking ahead, the MatFormer architecture also paves the way for. While not part of today’s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.</p><h2 data-block-key=\"xe7xu\">Per-Layer Embeddings (PLE): Unlocking more memory efficiency</h2><p data-block-key=\"2ilaf\">Gemma 3n models incorporate <b>Per-Layer Embeddings (PLE)</b>. This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU).</p><p data-block-key=\"i1dh\">While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).</p></div><div><h2 data-block-key=\"5alsx\">KV Cache sharing: Faster long-context processing</h2><p data-block-key=\"6472b\">Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications.</p><p data-block-key=\"1tdep\">KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.</p><h2 data-block-key=\"pctk9\">Audio understanding: Introducing speech to text and translation</h2><p data-block-key=\"uijl\">Gemma 3n uses an advanced audio encoder based on the <a href=\"https://arxiv.org/abs/2303.01037\">Universal Speech Model (USM)</a>. The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context.</p><p data-block-key=\"dlc74\">This integrated audio capability unlocks key features for on-device development, including:</p><ul><li data-block-key=\"6mapr\"><b>Automatic Speech Recognition (ASR):</b> Enable high-quality speech-to-text transcription directly on the device.</li></ul><ul><li data-block-key=\"caq44\"><b>Automatic Speech Translation (AST):</b> Translate spoken language into text in another language.</li></ul><p data-block-key=\"doduj\">We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here’s an example:</p></div><div><pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user\nTranscribe the following speech segment in Spanish, then translate it into English: \n&lt;start_of_audio&gt;&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model</code></pre></div><div><p data-block-key=\"7ree3\">At launch time, the Gemma 3n encoder is implemented to process audio clips up to 30 seconds. However, this is not a fundamental limitation. The underlying audio encoder is a streaming encoder, capable of processing arbitrarily long audios with additional long form audio training. Follow-up implementations will unlock low-latency, long streaming applications.</p><h2 data-block-key=\"m5455\">MobileNet-V5: New state-of-the-art vision encoder</h2><p data-block-key=\"3r73m\">Alongside its integrated audio capabilities, Gemma 3n features a new, highly efficient vision encoder, , delivering state-of-the-art performance for multimodal tasks on edge devices.</p><p data-block-key=\"1qj1b\">Designed for flexibility and power on constrained hardware, MobileNet-V5 gives developers:</p><ul><li data-block-key=\"c1qs0\"><b>Multiple input resolutions</b>: Natively supports resolutions of 256x256, 512x512, and 768x768 pixels, allowing you to balance performance and detail for your specific applications.</li></ul><ul><li data-block-key=\"648dt\"><b>Broad visual understanding</b>: Co-trained on extensive multimodal datasets, it excels at a wide range of image and video comprehension tasks.</li></ul><ul><li data-block-key=\"k1qf\">: Processes up to 60 frames per second on a Google Pixel, enabling real-time, on-device video analysis and interactive experiences.</li></ul><p data-block-key=\"1a8ge\">This level of performance is achieved with multiple architectural innovations, including:</p><ul><li data-block-key=\"48us4\">An advanced foundation of MobileNet-V4 blocks (including Universal Inverted Bottlenecks and Mobile MQA).</li></ul><ul><li data-block-key=\"12p8\">A significantly scaled up architecture, featuring a hybrid, deep pyramid model that is 10x larger than the biggest MobileNet-V4 variant.</li></ul><ul><li data-block-key=\"73gq1\">A novel Multi-Scale Fusion VLM adapter that enhances the quality of tokens for better accuracy and efficiency.</li></ul><p data-block-key=\"49ved\">Benefiting from novel architectural designs and advanced distillation techniques, MobileNet-V5-300M substantially outperforms the baseline SoViT in Gemma 3 (trained with SigLip, no distillation). On a Google Pixel Edge TPU, it <b>delivers a 13x speedup with quantization (6.5x without), requires 46% fewer parameters, and has a 4x smaller memory footprint</b>, all while providing significantly higher accuracy on vision-language tasks</p><p data-block-key=\"e7u3j\">We’re excited to share more about the work behind this model. Look out for our upcoming MobileNet-V5 technical report, which will deep dive into the model architecture, data scaling strategies, and advanced distillation techniques.</p><p data-block-key=\"emlv5\">Making Gemma 3n accessible from day one has been a priority. We're proud to partner with many incredible open source developers to ensure broad support across popular tools and platforms, including contributions from teams behind AMD, Axolotl, Docker, Hugging Face, llama.cpp, LMStudio, MLX, NVIDIA, Ollama, RedHat, SGLang, Unsloth, and vLLM.</p><p data-block-key=\"354di\">But this ecosystem is just the beginning. The true power of this technology is in what you will build with it. That’s why we’re launching the <a href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\">Gemma 3n Impact Challenge.</a> Your mission: use Gemma 3n's unique on-device, offline, and multimodal capabilities to build a product for a better world. With $150,000 in prizes, we're looking for a compelling video story and a \"wow\" factor demo that shows real-world impact. <a href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\">Join the challenge</a> and help build a better future.</p><h2 data-block-key=\"tra7k\">Get started with Gemma 3n today</h2><p data-block-key=\"9t8b1\">Ready to explore the potential of Gemma 3n today? Here's how:</p><ul><li data-block-key=\"cvr4m\"> Use <a href=\"https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it\">Google AI Studio</a> to try Gemma 3n in just a couple of clicks. Gemma models can also be deployed directly to Cloud Run from AI Studio.</li></ul><ul><li data-block-key=\"1va1s\"> Dive into our <a href=\"https://ai.google.dev/gemma/docs/gemma-3n\">comprehensive documentation</a> to quickly integrate Gemma into your projects or start with our inference and fine-tuning guides.</li></ul></div>","contentLength":10319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44389202"},{"title":"Launch HN: Issen (YC F24) – Personal AI language tutor","url":"https://news.ycombinator.com/item?id=44387828","date":1750948348,"author":"mariano54","guid":230,"unread":true,"content":"Hey HN, we're Mariano and Anton from ISSEN (<a href=\"https://issen.com\">https://issen.com</a>), a foreign language voice tutor app that adapts to your interests, goals, and needs.<p>We started this company after struggling to find great tools to practice speaking Japanese and French. Having a tutor can be awesome, but there are downsides: they can be expensive (since you pay by the hour), difficult to schedule, and have a high upfront cost (finding a tutor you like often forces you to cycle through a few that you don’t).</p><p>We wanted something that would talk with us — realistically, in full conversations — and actually help us improve. So we built it ourselves.\nThe app relies on a custom voice AI pipeline combining STT (speech-to-text), TTS (text-to-speech), LLMs, long term memory, interruptions, turn-taking, etc. Getting speech-to-text to work well for learners was one of the hardest parts — especially with accents, multi-lingual sentences, and noisy environments. We now combine Gemini Flash, Whisper, Scribe, and GPT-4o-transcribe to minimize errors and keep the conversation flowing.</p><p>We didn’t want to focus too much on gamification. In our experience, that leads to users performing well in the app, achieving long streaks and so on, without actually getting fluent in the language you're wanting to learn.</p><p>With ISSEN you instantly speak and immerse yourself in the language, which, while not easy, is a much more efficient way to learn.</p><p>We combine this with a word bank and SRS flashcards for new words learned in the AI voice chats, which allows very rapid improvement in both vocabulary and speaking skills. We also create custom curriculums for each student based on goals, interests, and preferences, and fully customizable settings like speed, turn taking, formality, etc.</p><p>App: <a href=\"https://issen.com\">https://issen.com</a> (works on web, iOS, Android)\nPricing: 20 min free trial, $20–29/month (depending on duration and specific geography)</p><p>We’d love your feedback — on the tech, the UX, or what you’d wish from a tool like this. Thanks!</p>","contentLength":2009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44387828"},{"title":"Google DeepMind Releases AlphaGenome","url":"https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/","date":1750947360,"author":"i_love_limes","guid":229,"unread":true,"content":"<div><p data-block-key=\"vrizt\">Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.</p><p data-block-key=\"2ot99\">The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.</p><p data-block-key=\"3j9or\">Today, we introduce <a href=\"https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf\" rel=\"noopener\" target=\"_blank\">AlphaGenome</a>, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.</p><p data-block-key=\"166qa\">To advance scientific research, we’re making AlphaGenome available in preview via our <a href=\"https://github.com/google-deepmind/alphagenome\" rel=\"noopener\" target=\"_blank\">AlphaGenome API</a> for non-commercial research, and planning to release the model in the future.</p><p data-block-key=\"9n1f9\">We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.</p><p data-block-key=\"9pi95\">Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.</p><p data-block-key=\"6p84u\">Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including <a href=\"http://encodeproject.org/\" rel=\"noopener\" target=\"_blank\">ENCODE</a>, <a href=\"https://www.gtexportal.org/\" rel=\"noopener\" target=\"_blank\">GTEx</a>, <a href=\"https://4dnucleome.org/\" rel=\"noopener\" target=\"_blank\">4D Nucleome</a> and <a href=\"https://fantom.gsc.riken.jp/5/\" rel=\"noopener\" target=\"_blank\">FANTOM5,</a> which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.</p></div><div><p data-block-key=\"vrizt\">The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.</p><p data-block-key=\"dl5ep\">This model builds on our previous genomics model, <a href=\"https://deepmind.google/discover/blog/predicting-gene-expression-with-ai/\" rel=\"noopener\" target=\"_blank\">Enformer</a> and is complementary to <a href=\"https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/\" rel=\"noopener\" target=\"_blank\">AlphaMissense</a>, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.</p></div><div><h2 data-block-key=\"vrizt\">AlphaGenome’s distinctive features</h2><p data-block-key=\"ev8si\">AlphaGenome offers several distinctive features compared to existing DNA sequence models:</p><h3 data-block-key=\"2728b\">Long sequence-context at high resolution</h3><p data-block-key=\"7d57b\">Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.</p><p data-block-key=\"24mqo\">Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.</p><h3 data-block-key=\"86rg9\">Comprehensive multimodal prediction</h3><p data-block-key=\"bft7s\">By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.</p><h3 data-block-key=\"8hmh5\">Efficient variant scoring</h3><p data-block-key=\"b84ad\">In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.</p><h3 data-block-key=\"81hcn\">Novel splice-junction modeling</h3><p data-block-key=\"860mh\">Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.</p><h2 data-block-key=\"6vomo\">State-of-the-art performance across benchmarks</h2><p data-block-key=\"2hvqt\">AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.</p></div><div><p data-block-key=\"vrizt\">When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.</p><p data-block-key=\"4p9mo\">This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in <a href=\"https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf\" rel=\"noopener\" target=\"_blank\">our preprint</a>.</p><h2 data-block-key=\"f4akh\">The benefits of a unifying model</h2><p data-block-key=\"3ist1\">AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.</p><p data-block-key=\"13g3v\">Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.</p><p data-block-key=\"al5d1\">Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.</p></div><figure><blockquote><p data-block-key=\"o5onv\">It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.</p></blockquote><figcaption><p data-block-key=\"o3wzw\">Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center</p></figcaption></figure><div><p data-block-key=\"e5ukd\">AlphaGenome's predictive capabilities could help several research avenues:</p><ol><li data-block-key=\"8b7n9\"> By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.</li><li data-block-key=\"43h16\"> Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.</li><li data-block-key=\"efr5o\"> It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.</li></ol><p data-block-key=\"7gonn\">For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing <a href=\"https://www.science.org/doi/10.1126/science.1259037\" rel=\"noopener\" target=\"_blank\">study of patients with T-cell acute lymphoblastic leukemia (T-ALL)</a>, researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called <a href=\"https://alphafold.ebi.ac.uk/entry/P17542\" rel=\"noopener\" target=\"_blank\">TAL1</a> by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.</p></div><figure><blockquote><p data-block-key=\"o5onv\">AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.</p></blockquote><figcaption><p data-block-key=\"o3wzw\">Professor Marc Mansour, University College London</p></figcaption></figure><div><p data-block-key=\"9c6av\">AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.</p><p data-block-key=\"fhnk4\">Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.</p><p data-block-key=\"4lpo\">We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.</p><p data-block-key=\"7qtv9\">We’re continuing to improve our models and gathering feedback to help us address these gaps.</p><h2 data-block-key=\"cvanp\">Enabling the community to unlock AlphaGenome's potential</h2><p data-block-key=\"2ouf5\">AlphaGenome is now available for non-commercial use via our <a href=\"https://github.com/google-deepmind/alphagenome\" rel=\"noopener\" target=\"_blank\">AlphaGenome API</a>. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.</p><p data-block-key=\"2lg85\">Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the <a href=\"https://www.alphagenomecommunity.com/\" rel=\"noopener\" target=\"_blank\">community forum</a>.</p><p data-block-key=\"8fl9p\">We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.</p><p data-block-key=\"2p4vh\">Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.</p></div><section><div><div><p data-block-key=\"4d4h5\">We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.</p><p data-block-key=\"59d3j\">This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.</p><p data-block-key=\"75u3a\">We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.</p></div></div></section>","contentLength":12182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44387659"},{"title":"Learnings from building AI agents","url":"https://www.cubic.dev/blog/learnings-from-building-ai-agents","date":1750941904,"author":"pomarie","guid":228,"unread":true,"content":"<p>I’m Paul, cofounder of&nbsp;<a href=\"http://cubic.dev/\" rel=\"noopener\">cubic</a>—an \"AI-native GitHub.\" One of our core features is an AI code review agent that performs an initial review pass, catching bugs, anti-patterns, duplicated code, and similar issues in pull requests.</p><p>When we first released this agent back in April, the main feedback we got was straightforward: it was too noisy.</p><p>Even small PRs often ended up flooded with multiple low-value comments, nitpicks, or outright false positives. Rather than helping reviewers, it cluttered discussions and obscured genuinely valuable feedback.</p><img alt=\"\" height=\"236\" src=\"https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png\" srcset=\"https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=512 512w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png 2052w\" width=\"1026\" data-framer-original-sizes=\"\" sizes=\"(min-width: 1200px) 100vw, (min-width: 1000px) and (max-width: 1199px) 100vw, (max-width: 999px) 100vw\"><p>We decided to take a step back and thoroughly investigate why this was happening.</p><p>After three major architecture revisions and extensive offline testing, we managed to reduce false positives by  without sacrificing recall.</p><p>Many of these lessons turned out to be broadly useful—not just for code review agents but for designing effective AI systems in general.</p><h2>1. The Face‑Palm Phase: A Single, Do‑Everything Agent</h2><p>Our initial architecture was straightforward but problematic:</p><div><div><div><div><div><div aria-labelledby=\"/example.md-:Rb8na5b8m:-tab\" role=\"tabpanel\"><div aria-autocomplete=\"list\" aria-label=\"Code Editor for example.md\" aria-multiline=\"true\" role=\"textbox\" tabindex=\"0\" translate=\"no\"><pre>\n↓\n\n↓\n</pre></div></div></div></div></div></div></div><p>It looked clean in theory but quickly fell apart in practice:</p><ul><li data-preset-tag=\"p\"><p>Excessive false positives: The agent often mistook style issues for critical bugs, flagged resolved issues, and repeated suggestions our linters had already addressed.</p></li><li data-preset-tag=\"p\"><p>Users lost trust: Developers quickly learned to ignore the comments altogether. When half the comments feel irrelevant, the truly important ones get missed.</p></li><li data-preset-tag=\"p\"><p>Opaque reasoning: Understanding why the agent made specific calls was practically impossible. Even explicit prompts like \"ignore minor style issues\" had minimal effect.</p></li></ul><p>We tried standard solutions—longer prompts, adjusting the model's temperature, experimenting with sampling—but saw little meaningful improvement.</p><p>After extensive trial-and-error, we developed an architecture that significantly improved results and proved effective in real-world repositories. These solutions underpin the 51% reduction in false positives currently running in production.</p><h4>2.1 Explicit Reasoning Logs</h4><p>We required the AI to explicitly state its reasoning before providing any feedback:</p><div><div><div><div><div><div aria-labelledby=\"/example.js-:Rb97a5b8m:-tab\" role=\"tabpanel\"><div aria-autocomplete=\"list\" aria-label=\"Code Editor for example.js\" aria-multiline=\"true\" role=\"textbox\" tabindex=\"0\" translate=\"no\"><pre></pre></div></div></div></div></div></div></div><p>This approach provided critical benefits:</p><ul><li data-preset-tag=\"p\"><p>Enabled us to clearly trace the AI’s decision-making process. If reasoning was flawed, we could quickly identify and exclude the pattern in future iterations.</p></li><li data-preset-tag=\"p\"><p>Encouraged structured thinking by forcing the AI to justify its findings first, significantly reducing arbitrary conclusions.</p></li><li data-preset-tag=\"p\"><p>Created a foundation to diagnose and resolve root causes behind other issues we faced.</p></li></ul><p>Initially, the agent had extensive tooling—Language Server Protocol (LSP), static analysis, test runners, and more. However, explicit reasoning logs revealed most analyses relied on a few core tools, with extra complexity causing confusion and mistakes.</p><p>We streamlined the toolkit to essential components only—a simplified LSP and a basic terminal.</p><p>With fewer distractions, the agent spent more energy confirming genuine issues, significantly improving precision.</p><h4>2.3 Specialized Micro-Agents Over Generalized Rules</h4><p>Initially, our instinct was to continuously add more rules into a single large prompt to handle edge cases:</p><ul><li data-preset-tag=\"p\"><p>“Ignore unused variables in .test.ts files.”</p></li><li data-preset-tag=\"p\"><p>“Skip import checks in Python’s .py.”</p></li><li data-preset-tag=\"p\"><p>“Don't lint markdown files.”</p></li></ul><p>This rapidly became unsustainable and was largely ineffective as the AI frequently overlooked many rules.</p><p>Our breakthrough came from employing specialized micro-agents, each handling a narrowly-defined scope:</p><ul><li data-preset-tag=\"p\"><p>: Quickly assesses changes and identifies necessary checks.</p></li><li data-preset-tag=\"p\"><p>: Detects vulnerabilities such as injection or insecure authentication.</p></li><li data-preset-tag=\"p\"><p>: Flags repeated or copied code.</p></li><li data-preset-tag=\"p\"><p>: Handles typos and documentation consistency.</p></li></ul><p>Specializing allowed each agent to maintain a focused context, keeping token usage efficient and precision high. The main trade-off was increased token consumption due to overlapping context, managed through effective caching strategies.</p><p>These architecture and prompt improvements led to meaningful results across hundreds of real pull requests from active open-source and private repositories. Specifically, over the past six weeks:</p><ul><li data-preset-tag=\"p\"><p>51% fewer false positives, directly increasing developer trust and usability.</p></li><li data-preset-tag=\"p\"><p>Median comments per pull request cut by half, helping teams concentrate on genuinely important issues.</p></li><li data-preset-tag=\"p\"><p>Teams reported notably smoother review processes, spending less time managing irrelevant comments and more time effectively merging changes.</p></li></ul><p>Additionally, the reduced noise significantly improved developer confidence and engagement, making reviews faster and more impactful.</p><ol><li data-preset-tag=\"p\"><p>Explicit reasoning improves clarity. Require your AI to clearly explain its rationale first—this boosts accuracy and simplifies debugging.</p></li><li data-preset-tag=\"p\"><p>Simplify the toolset. Regularly evaluate your agent's toolkit and remove tools rarely used (less than 10% of tasks).</p></li><li data-preset-tag=\"p\"><p>Specialize with micro-agents. Keep each AI agent tightly focused on a single task, reducing cognitive overload and enhancing precision.</p></li></ol>","contentLength":4937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44386887"},{"title":"Snow - Classic Macintosh emulator","url":"https://snowemu.com/","date":1750928880,"author":"ColinWright","guid":227,"unread":true,"content":"<p>Snow emulates classic (Motorola 680x0-based) Macintosh computers. It features a graphical user interface to operate the emulated machine and provides extensive debugging capabilities. The aim of this project is to emulate the Macintosh on a hardware-level as much as possible, as opposed to emulators that patch the ROM or intercept system calls.</p><p>It currently emulates the Macintosh 128K, Macintosh 512K, Macintosh Plus, Macintosh SE, Macintosh Classic and Macintosh II.</p><p>There is a limited <a href=\"https://demo.snowemu.com/\">online demo</a> available (only the emulated machine, no user interface or other functionality from the full software).</p><p>Currently, only bleeding edge builds are available. These get generated automatically as work progresses\non the emulator.</p>","contentLength":724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44385562"},{"title":"LLM code generation may lead to an erosion of trust","url":"https://jaysthoughts.com/aithoughts1","date":1750918069,"author":"CoffeeOnWrite","guid":226,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44384610"},{"title":"Puerto Rico's Solar Microgrids Beat Blackout","url":"https://spectrum.ieee.org/puerto-rico-solar-microgrids","date":1750894876,"author":"ohjeez","guid":225,"unread":true,"content":"<p>When power went out across all of <a href=\"https://spectrum.ieee.org/tag/puerto-rico\">Puerto Rico</a> on 16 April, a lot of the lights in the town of Adjuntas stayed on. There, nestled in the mountains on the midwestern side of the island, a combination of experimental <a href=\"https://spectrum.ieee.org/microgrid\" target=\"_self\">microgrids</a>, <a href=\"https://spectrum.ieee.org/tag/solar-panels\">solar panels</a>, and storage kept power on for many businesses and residents. The rest of the island waited over 24 hours, and in some cases longer, for electricity to be restored.</p><p>The blackout was the latest in a series of power interruptions that have come to define Puerto Rico’s aging <a href=\"https://spectrum.ieee.org/tag/electrical-grid\">electrical grid</a>. Vegetation was to blame for April’s blackout, according to <a href=\"https://lumapr.com/?lang=en\" rel=\"noopener noreferrer\" target=\"_blank\">LUMA Energy</a>, the private company that <a href=\"https://spectrum.ieee.org/the-privatization-of-puerto-rico-power-grid-mired-in-controversy\" target=\"_self\">manages the island’s grid</a>. A faulty old cable triggered the near total blackout on New Year’s Eve 2024, the company said. Tropical storm Ernesto’s <a href=\"https://www.npr.org/2024/08/14/nx-s1-5075256/hurricane-ernesto-puerto-rico-power-outage\" rel=\"noopener noreferrer\" target=\"_blank\">strong winds </a>knocked out half of the island’s power in August 2024. </p><p>The problems are the result of decades of mismanagement and disinvestment in the island’s grid infrastructure. Neglecting to keep up with regular maintenance and failing to meet increasing demand for <a href=\"https://spectrum.ieee.org/tag/power-generation\">power generation</a> have contributed to the disarray. The long-standing issues set the stage for the grid to be crushed in 2017 by <a href=\"https://spectrum.ieee.org/tag/hurricane-maria\">Hurricane Maria</a>, the United States’ <a href=\"https://www.cbsnews.com/news/top-10-deadliest-hurricanes-in-us-history-katrina-maria-galveston/\" rel=\"noopener noreferrer\" target=\"_blank\">second deadliest,</a> which plunged Puerto Rico into months-long darkness and claimed nearly 3,000 lives. </p><p>After that <a href=\"https://spectrum.ieee.org/tag/hurricane\">hurricane</a>, the island’s state-run utility, Puerto Rico <a href=\"https://spectrum.ieee.org/tag/electric-power\">Electric Power</a> Authority (PREPA), contracted with private entities for power generation, transmission, and distribution in the hopes of fixing the grid. Over <a href=\"https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2025/01/10/fact-sheet-biden-harris-administrations-historic-investments-in-puerto-ricos-energy-grid/\" rel=\"noopener noreferrer\" target=\"_blank\">$20 billion</a> in U.S. federal <a href=\"https://spectrum.ieee.org/tag/disaster-relief\">disaster relief</a> was awarded by the Federal Emergency Management Agency (<a href=\"https://spectrum.ieee.org/tag/fema\">FEMA</a>) to improve the grid and boost its resilience. Yet bureaucratic red tape and politics in Puerto Rico and on the U.S. mainland have hindered much of that money from being spent.</p><p>Now, the U.S. <a href=\"https://spectrum.ieee.org/tag/department-of-energy\">Department of Energy</a> plans to redirect$365 million previously earmarked for <a href=\"https://spectrum.ieee.org/tag/rooftop-solar\">rooftop solar</a> toward infrastructure on Puerto Rico’s <a href=\"https://www.eia.gov/state/print.php?sid=RQ\" target=\"_blank\">majority</a> fossil-fuel-powered grid, according to an <a href=\"https://www.energy.gov/articles/energy-department-redirect-365-million-support-grid-resilience-efforts-puerto-rico\" rel=\"noopener noreferrer\" target=\"_blank\">announcement</a> from the agency on May 21. The money will  support “practical fixes and emergency activities that offer a faster, more impactful solution to the current crisis,” the agency said. This will include “system flexibility and response, power flow and control, component strength, supply security, and safety,” according to the announcement. </p><p>The move <a href=\"https://apnews.com/article/puerto-rico-federal-funds-energy-department-solar-379e4dc7b361268819bf60dd0cf9b0dc\" rel=\"noopener noreferrer\" target=\"_blank\">sparked an outcry</a> from Puerto Rico’s solar industry and U.S. Representative Nydia Velazquez of New York. Velazquez, who is from Puerto Rico, called the move “shameful” in a <a href=\"https://x.com/NydiaVelazquez/status/1925263284703248607\" rel=\"noopener noreferrer\" target=\"_blank\">post on X</a>, saying the money was designed to serve vulnerable communities on the island.</p><h2>Solar Energy’s Role in Puerto Rico’s Grid</h2><p>The ongoing political turmoil and bottlenecked federal funding have prompted the widespread development of solar-plus-storage systems across the island that are privately financed via leases, loans, or Power Purchase Agreements (PPAs). Each month, the island sees around 4,000 solar-plus-battery storage systems come online, Rúa-Jovet says. These installations are connected to the grid but can also operate during <a href=\"https://spectrum.ieee.org/tag/blackouts\">blackouts</a>.</p><p>At the end of March, LUMA <a href=\"https://energia.pr.gov/wp-content/uploads/sites/7/2025/05/Resumen-Metricas-Master_April2025-.xlsx\" rel=\"noopener noreferrer\" target=\"_blank\">reported</a> over 1.14 gigawatts of grid-connected distributed solar capacity, with an additional 2.34 gigawatt-hours of distributed <a href=\"https://spectrum.ieee.org/tag/batteries\">batteries</a> connected to the grid. <a href=\"https://spectrum.ieee.org/tag/solar-power\">Solar power</a> produces over 2 terawatt-hours of electricity each year, which accounts for more than 12.5 percent of Puerto Rico’s total residential electricity consumption annually. The majority of that power is generated from residential solar, and capacity continues to grow as more residents install systems with private financing. </p><p>Adjuntas, which has a population of about 18,000, took a more <a href=\"https://spectrum.ieee.org/microgrid\" target=\"_blank\">experimental approach</a>. The town’s local environmental nonprofit <a href=\"https://casapueblo.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Casa Pueblo</a> teamed up with researchers from the U.S. Department of Energy’s <a href=\"https://www.ornl.gov/\" rel=\"noopener noreferrer\" target=\"_blank\">Oak Ridge National Laboratory</a> in Oak Ridge, Tenn., to develop a way to connect multiple <a href=\"https://spectrum.ieee.org/tag/microgrids\">microgrids</a> to exchange power with one another, all without having to be hooked up to Puerto Rico’s grid. The strategy, called grid orchestration, ensures that if power is knocked out on one of the installations, the others aren’t compromised. It’s what kept multiple areas in Adjuntas electrified during April’s island-wide blackout.</p><p>During the blackout, Casa Pueblo and the Oak Ridge researchers were completing the testing of the orchestration strategy with three of the five microgrids connected in Adjuntas. These three microgrids are connected to the grid via <a href=\"https://spectrum.ieee.org/tag/net-metering\">net metering</a>. The remaining two grids are isolated.</p><p>“By decentralizing, it’s creating a more resilient and redundant energy setup,” says <a href=\"https://spectrum.ieee.org/u/arturo-massol-deya\" target=\"_self\">Arturo Massol-Deyá</a>, Casa Pueblo’s executive director. “Engineers will say: If you have redundancy, that’s more resilient; that’s better.”</p><p>The teams demonstrated trading energy from one microgrid to the other, and vice versa. This kind of transfer enables the system to overcome energy limitations during peak demand times and draw from additional storage at night when the sun is down. Together, the town’s five microgrids provide 228 kilowatts of <a href=\"https://spectrum.ieee.org/tag/photovoltaic\">photovoltaic</a> capacity and an additional 1.2 megawatt-hours of storage, which serve residences and 15 commercial businesses. It’s a small amount of power, but an example of a way for systems to operate independently from the grid. </p><h2>Expanding Microgrid Connections in Adjuntas</h2><p>Moving forward, Massol-Deyá’s plan is to continue improving and expanding the bottom-up approach to microgrid connections. On April 20, Casa Pueblo launched a lab in Adjuntas called the <a href=\"https://www.hasercambio.org/en/casapueblo/\" rel=\"noopener noreferrer\" target=\"_blank\">Community Laboratory for the Energy Transition</a> with the goal of bringing together academics and industry experts to test new microgrid technology as it develops. </p><p>The next milestone, Massol-Deyá says, will be successfully connecting microgrids that are not in close geographic proximity. “In Adjuntas, we’re bridging the gap between simulation and theoretical work with a real application,” he says.</p><p>As warmer months approach, Puerto Rico is gearing up island-wide for a <a href=\"https://www.sanjuandailystar.com/post/energy-czar-puerto-rico-likely-to-experience-blackouts-this-summer\" rel=\"noopener noreferrer\" target=\"_blank\">season of power failures</a> as energy demand will likely exceed Puerto Rico’s generation capacity. This will likely be compounded by a <a href=\"https://www.noaa.gov/news-release/noaa-predicts-above-normal-2025-atlantic-hurricane-season\" rel=\"noopener noreferrer\" target=\"_blank\">stronger-than-normal</a> Atlantic hurricane season. </p><p>Rúa-Jovet maintains that solar and batteries are an easily dispatchable resource that make a “good dent” in <a href=\"https://spectrum.ieee.org/tag/resiliency\">resiliency</a> against island-wide power failures. Massol-Deyá agrees and says that even with the government turning toward what he calls an “obsolete” model of fossil fuel power, the Puerto Rican people are embracing solar.</p><p>“It’s not top-down: It’s not by LUMA, it’s not by the government. It has been pushed by the people. You have a huge and significant investment by the people on solar,” Massol-Deyá says. </p>","contentLength":6776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44382834"},{"title":"Better Auth, by a self-taught Ethiopian dev, raises $5M from Peak XV, YC","url":"https://techcrunch.com/2025/06/25/this-self-taught-ethiopian-dev-built-an-authentication-tool-and-got-into-yc/","date":1750874822,"author":"bundie","guid":222,"unread":true,"content":"<p>It’s rare to see a solo founder building a widely adopted developer infrastructure tool. Even more so if the founder happens to be from Africa. <a href=\"https://et.linkedin.com/in/bekacru\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Bereket Engida</a>, a self-taught programmer from Ethiopia, is quietly building what some developers say is the best authentication tool they’ve ever used.</p><p>Engida’s startup, <a href=\"https://www.better-auth.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Better Auth</a>, offers an open source framework that promises to simplify how developers manage user authentication, and it’s caught the attention of some big name investors.&nbsp;It recently raised about $5 million in seed funding from Peak XV (formerly Sequoia India and Southeast Asia), Y Combinator, P1 Ventures, and Chapter One.&nbsp;</p><p>But the most interesting part here isn’t who’s on the startup’s cap table: Engida says he built the entire product back home in Ethiopia before he set foot in the U.S.</p><p>Engida told TechCrunch that he started programming at 18 after a friend declined to help him build an e-commerce search app, and he started working on the project himself. He went on to land some remote software jobs and eventually built a web analytics platform that lets developers monitor user behavior on their websites. </p><p>But throughout his various jobs, Engida says he kept seeing an issue popping up everywhere: authentication. Every app needs to manage how users sign in and out and reset passwords, and sometimes administrators need to handle permissions and user roles. But he found existing tools were either too limited or too rigid — companies like Auth0, Firebase, and NextAuth offer managed services, but they store user data externally, limit customization, and are expensive at scale.</p><p>“I remember needing an organization feature. It’s a very common use case for most SaaS applications, but it wasn’t available from these providers,” Engida told TechCrunch. “So I had to build it from scratch. It took me about two weeks, and I remember thinking, ‘This is crazy; there has to be a better way to solve this.’”</p><p>He then scrapped that project and began working on a TypeScript-based authentication framework that would let developers access user data via open source libraries, support common permissions use cases — like teams and roles — out of the box, and scale with plug-ins.</p><p>“The idea was that you could add advanced features in just two or three lines of code,” Engida said.</p><p>Over six months working mostly from his bedroom in Ethiopia, Engida built the first version of the library that would go on to become Better Auth. When he posted it to GitHub in September 2024, developers quickly saw the potential.&nbsp;</p><p>Since then, Better Auth has clocked 150,000+ weekly downloads, 15,000+ GitHub stars, and a community of over 6,000 Discord members, the startup claims.&nbsp;</p><p>Better Auth’s pitch is simple: Let developers implement everything from simple authentication flows to enterprise-grade systems directly on their databases and embed it all on the back end. Unlike hosted services, Better Auth is an open source library that developers can integrate directly into their codebase, keeping all user data on premise, in their database. For companies wary of handing over critical user information to third parties, this feature alone is a major point.</p><p>The library has also found unexpected traction among early-stage AI startups, which need to build custom authentication flows that integrate with proprietary APIs, manage tokens securely, and be able to scale without racking up high costs.</p><p>“We first heard about the product from numerous startups we’ve worked with,” said Arnav Sahu, partner at Peak XV and former principal at Y Combinator. “Their auth product has seen phenomenal adoption among the next generation of AI startups.” </p><p>Better Auth marks Peak XV’s first direct investment in an African founder.</p><p>Engida says Better Auth, currently free to use, will focus on improving its core features and launch a paid enterprise infrastructure that plugs into its open source base. This will give developers the flexibility to self-host or opt for Better Auth’s cloud add-ons as needed.</p><p>He’s also thinking about how to scale without trading away the product’s community-built feel. On the roadmap, therefore, is hiring a small team to help maintain the codebase, expand documentation, and support enterprise users. For now, though, Engida is still writing most of the code himself.</p><p>Better Auth, which just graduated from YC’s recent spring batch, is the third Ethiopian startup to pass through the accelerator, following drone-based digital health platform Avion, and food delivery platform BeU Delivery.&nbsp;</p><p>“Building this feels important not just because people love the product, but because of what it represents,” said Engida. “There aren’t many Ethiopian founders building global products. For many, it feels almost impossible. So seeing that traction gives hope for other people to try to be more ambitious.”</p>","contentLength":4889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44380185"},{"title":"Build and Host AI-Powered Apps with Claude – No Deployment Needed","url":"https://www.anthropic.com/news/claude-powered-artifacts","date":1750871675,"author":"davidbarker","guid":221,"unread":true,"content":"<p>Today, we’re introducing the ability to build, host, and share interactive AI-powered apps directly in the Claude app. Now developers can iterate faster on their AI apps without worrying about the complexity and cost of scaling for a growing audience.</p><h2>Build and host Claude-powered apps</h2><p>Here’s what we built: Claude can now create artifacts that interact with Claude through an API— turning these artifacts into AI-powered apps, where the economics actually work for sharing.</p><p>When someone uses your Claude-powered app:</p><ul><li>They authenticate with their existing Claude account</li><li>Their API usage counts against  subscription, not yours</li><li>You pay nothing for their usage</li><li>No one needs to manage API keys</li></ul><p>Claude writes real code that orchestrates complex AI functionality. You can see it, modify it, and share it freely.</p><p>Early users have already used interactive artifacts to build:</p><ul><li> with NPCs that remember conversations and adapt to player choices</li><li> that adjust to individual skill levels and provide personalized tutoring</li><li> where users upload CSVs and ask follow-up questions in natural language</li><li> that help with everything from scripts to technical documentation</li><li> that orchestrate multiple Claude calls for complex tasks</li></ul><p>Start building in the Claude app by enabling this new interactive capability. Simply describe what you want to create, and Claude will write the code for you.</p><p>As you work together, Claude can debug and improve its own code based on your feedback. Once your app is ready, you can share it instantly through a link—no deployment process needed. Claude takes care of the technical details like prompt engineering, error handling, and orchestration logic, allowing you to focus entirely on bringing your idea to life.</p><ul><li>Use a Claude API within your artifacts</li><li>Process files and create rich UIs with React</li><li>See, fork, and customize any artifact</li></ul><ul><li>No external API calls (yet)</li><li>Limited to a text-based completion API</li></ul><p>This capability is available in beta to Free, Pro, and Max plan users.</p>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44379673"},{"title":"What Problems to Solve (1966)","url":"http://genius.cat-v.org/richard-feynman/writtings/letters/problems","date":1750871324,"author":"jxmorris12","guid":220,"unread":true,"content":"<p><em>A former student, who was also once a student of Tomonaga’s, wrote to extend\nhis congratulations. Feynman responded, asking Mr. Mano what he was now doing.\nThe response: “studying the Coherence theory with some applications to the\npropagation of electromagnetic waves through turbulent atmosphere… a humble and\ndown-to-earth type of problem.”</em></p><pre><code>Dear Koichi,\n\nI was very happy to hear from you, and that you have such a position in the\nResearch Laboratories. Unfortunately your letter made me unhappy for you seem\nto be truly sad. It seems that the influence of your teacher has been to give\nyou a false idea of what are worthwhile problems. The worthwhile problems are\nthe ones you can really solve or help solve, the ones you can really contribute\nsomething to. A problem is grand in science if it lies before us unsolved and\nwe see some way for us to make some headway into it. I would advise you to take\neven simpler, or as you say, humbler, problems until you find some you can\nreally solve easily, no matter how trivial. You will get the pleasure of\nsuccess, and of helping your fellow man, even if it is only to answer a\nquestion in the mind of a colleague less able than you. You must not take away\nfrom yourself these pleasures because you have some erroneous idea of what is\nworthwhile.\n\nYou met me at the peak of my career when I seemed to you to be concerned with\nproblems close to the gods. But at the same time I had another Ph.D. Student\n(Albert Hibbs) was on how it is that the winds build up waves blowing over\nwater in the sea. I accepted him as a student because he came to me with the\nproblem he wanted to solve. With you I made a mistake, I gave you the problem\ninstead of letting you find your own; and left you with a wrong idea of what is\ninteresting or pleasant or important to work on (namely those problems you see\nyou may do something about). I am sorry, excuse me. I hope by this letter to\ncorrect it a little.\n\nI have worked on innumerable problems that you would call humble, but which I\nenjoyed and felt very good about because I sometimes could partially succeed.\nFor example, experiments on the coefficient of friction on highly polished\nsurfaces, to try to learn something about how friction worked (failure). Or,\nhow elastic properties of crystals depends on the forces between the atoms in\nthem, or how to make electroplated metal stick to plastic objects (like radio\nknobs). Or, how neutrons diffuse out of Uranium. Or, the reflection of\nelectromagnetic waves from films coating glass. The development of shock waves\nin explosions. The design of a neutron counter. Why some elements capture\nelectrons from the L-orbits, but not the K-orbits. General theory of how to\nfold paper to make a certain type of child’s toy (called flexagons). The energy\nlevels in the light nuclei. The theory of turbulence (I have spent several\nyears on it without success). Plus all the “grander” problems of quantum\ntheory.\n\nNo problem is too small or too trivial if we can really do something about it.\n\nYou say you are a nameless man. You are not to your wife and to your child. You\nwill not long remain so to your immediate colleagues if you can answer their\nsimple questions when they come into your office. You are not nameless to me.\nDo not remain nameless to yourself – it is too sad a way to be. now your place\nin the world and evaluate yourself fairly, not in terms of your naïve ideals of\nyour own youth, nor in terms of what you erroneously imagine your teacher’s\nideals are.\n\nBest of luck and happiness.  Sincerely, Richard P. Feynman.\n</code></pre>","contentLength":3580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44379606"},{"title":"Ambient Garden","url":"https://ambient.garden/","date":1750700288,"author":"fipar","guid":218,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44358148"},{"title":"Modeling the World in 280 Characters","url":"https://tympanus.net/codrops/2025/06/23/modeling-the-world-in-280-characters/","date":1750672930,"author":"OuterVale","guid":217,"unread":true,"content":"<p>\n  Hi, I’m Xor. As a graphics programmer, my job is essentially to make pixels prettier using math formulas. I work on\n  video effects like lighting, reflections, post-processing, and more for games and animated backgrounds in software.\n</p><p>\n  For fun, I like to unwind by writing compact little shader programs that fit in a “tweet” (280 characters or less).\n  You may have seen some of these posted on X/Twitter. The process of shrinking code while maintaining its functionality\n  is called “code golfing.”\n</p><p>Here’s an animated galaxy I wrote in just 197 characters of GLSL code:</p><p>\n  This little piece of code runs in real time for every pixel on the screen and generates a unique output color using\n  some fancy math and logic. I build these demos using a tool called\n  <a href=\"https://twigl.app\" target=\"_blank\">Twigl.app</a>\n  , an online shader editor designed for sharing mini-shaders. It makes exporting videos super easy, and in its\n  “geekiest” mode, it also takes care of the generic header code and shortens built-in variable names.\n</p><p>I even managed to fit a voxel DDA raytracer with edge detection into just 190 characters:</p><p>\n  Today, I’d like to explain why I make these, share my creation process, and show you how you can try it yourself if\n  you’re interested. Let’s start with the “why.”\n</p><p>\n  Why do I write these? Well, there are several factors. Since I like lists, I’ll go ahead and present them in order of\n  relevance:\n</p><ul><li>\n    : Sometimes I get struck by a new idea and just want to play around with it. I like Twigl because it helps lower my\n    expectations and lets me start doodling. There’s less room for overplanning, and it’s super easy to jump in.\n  </li><li>\n    : Working within constraints forces me to think through problems differently. By optimizing for code size, I often\n    find ways to simplify or approximate. It doesn’t always lead to more performant code (but often it does) and I’ve\n    learned how to squeeze the most out of every byte. Having very little code makes it easier to experiment with\n    formulas and variations without getting overwhelmed.\n  </li><li>\n    : Writing tiny code is both challenging and stimulating. It keeps my brain sharp, and I’m constantly developing new\n    skills. It’s basically become a game for me. I’ve accidentally learned a ton of math while trying to solve these\n    technical problems.\n  </li><li>\n    : I’ve connected with so many interesting people through this process—artists, designers, math folks, game devs,\n    engineers, tech enthusiasts, and more. Sharing my work has led to some exciting encounters. (More on some notable\n    people later!)\n  </li></ul><p>\n  So, in short, it’s fun, thought-provoking, and engaging, and it’s a great way to spark interest in graphics\n  programming. Now, what even is a shader?\n</p><p>\n  In case you haven’t heard of shaders before, they are programs that run on the GPU (Graphics Processing Unit) instead\n  of the CPU (Central Processing Unit). CPUs excel at complicated or branching operations, which are computed\n  sequentially, one at a time (I’m simplifying here). GPUs are designed to process billions or trillions of predictable\n  operations per second in parallel. This sounds like a lot, but a 4K screen at 60 frames per second outputs nearly 500M\n  pixels per second. Each pixel could have 100s or 1,000s of operations, not to mention anything else the GPU might be\n  used for.\n</p><p>\n  There are several different types of shaders: vertex shaders, fragment shaders, compute shaders, and more, but these\n  tweet shaders are specifically fragment shaders, also known as “pixel shaders,” because they run on every pixel. In\n  essence, fragment shaders take the input fragment coordinates and output a color and opacity (or alpha). Fragment\n  coordinates give you the position of the center of each pixel on screen, so (0.5, 0.5) is the bottom-left (or\n  top-left). One pixel to the right is (1.5, 0.5), and so on to (width – 0.5, height – 0.5). The coordinates variable is\n  called “FC” in Twigl. The output color, “o”, has 4 RGBA components: red, green, blue, and alpha, each ranging from 0.0\n  to 1.0.\n</p><p>\n  is pure white,\n  \n  is opaque black, and\n  \n  is pure red in the RGBA color format. From here, you can already make simple color gradients:\n</p><p><code>o = vec4(0.0, FC.y/100.0, 0.0, 1.0)</code>\n  ;\n</p><p>\n  Remember, this is run on every pixel, so each pixel will have a unique Fragment Coordinate. That formula makes a\n  simple gradient that starts black at the bottom of the screen (FC.y = 0.0), and the green output value reaches 1.0\n  when FC.y reaches 100.0.\n</p><p>\n  So you have an output color “o”, the input fragment coordinates “FC”, and four “uniform” inputs which are shared among\n  all pixels: “r” is the shader screen resolution in pixels, “t” is the time in seconds, and also the less commonly used\n  mouse position “m” and the backbuffer texture “b”. And that’s the core of it! From there, it’s a lot of math and logic\n  to control the output colors and generate cool images.\n</p><p>\n  I’m going to skip ahead a bit, but if you’re interested in learning more, try\n  <a href=\"https://thebookofshaders.com\">starting here</a>\n  !\n</p><p>\n  People often ask me whether I write my shaders in a compact form from the start or if I write them expanded and then\n  reduce the code afterward. The answer is the former. I’ve practiced code golfing so much that I find it easier to\n  prototype ideas in compact form, and I tend not to get lost in tiny shaders. Code golfing shaders requires finding the\n  right balance between code size, render performance, artistic appeal, design, and mathematical function. It’s a\n  delicate balance that definitely challenges both sides of my brain. I’ve learned a ton about math, art, and design\n  through writing these!\n</p><p>To start one, you need an idea. When writing the “Milky” stars shader, I knew I wanted to create some kind of galaxy, so that was my initial spark. </p><p>My shaders typically start with centering and scaling so that they look good at various resolutions and aspect ratios. For the stars, I looped through 100 point lights revolving around the center. I love glowing effects, and they are pretty easy to create. You just need to know the distance from the current pixel to the light source and use the inverse for the pixel brightness (close pixels are brighter, far pixels are darker). </p><p>I played around with the positions of the particles using some trigonometry and gave the disk a slight skew. For the coloring, I love to use some sine waves with a phase shift for the RGB channels. Sine waves are also useful for picking pseudo-random numbers, so that’s how I select the colors for each star. Using the sine formula, you can get palettes like these: </p><p>I ended up with a slight alteration of the one second from the left. It has a nice range of temperatures and brightness. I also added some variation to the star brightness, which made the image much more interesting to look at. </p><p>Next, I applied some <a href=\"https://mini.gmshaders.com/p/tonemaps\">tonemapping</a> with the <a href=\"https://mini.gmshaders.com/p/func-tanh\">hyperbolic tangent</a> function for size. Tonemapping prevents the harsh overexposure and hue shifts that happen when a color channel hits its maximum brightness value (left is original, right is with tonemapping):</p><p>\n  Any good shader that has High Dynamic Range lighting should apply some tonemapping, and tweet shaders are no\n  exception! Finally, I played with animation. It could have revolved or twisted, but in the end, I liked the\n  contraction effect most. I also created a loop so that new stars faded in when the old stars reached the center. You\n  can read about my design process in\n  <a href=\"https://mini.gmshaders.com/p/design-choices\">more detail here</a>\n  !\n</p><p>\n  As you can imagine, there are hundreds of little techniques that I have developed (and continue to discover) in the\n  process of shrinking the code down, but I can give you the abridged version! My generalized code-golfing process can\n  be listed like so:\n</p><ul><li>\n    It may be challenging initially, but you can get used to single-letter variables and function names. You may\n    sometimes forget what variables are for, but this is actually helpful for code golfing. It forces you to reread your\n    code, and you’ll often find better ways to write it when doing so. Like anything else, your memory will improve with\n    practice, and over time you will establish some standards (for me: p = position, c = color, O = frag output, I =\n    input, etc.).\n  </li><li>\n    This is pretty self-explanatory.\n    \n    ,\n    \n    . Don’t forget that with vector constructors, you can use any data type as an input, and it gets converted (“cast”)\n    to the new type:\n    <code>vec4(1.0, 1.0, 1.0, 1.0) == vec4(1)</code>\n    . If you’re multiplying by\n    \n    , you could instead divide by\n    \n    .\n  </li><li><strong>Minimize initializations:</strong>\n    If you have two floats, “x” and “y”, try to initialize them together like so:\n    \n    Look for opportunities to share data types. If you have a color vec3 and a vec4, make them both vec4s. Avoid\n    float/int conversions.\n  </li><li>\n    If statements in GLSL take up a bit of space, especially if you need an\n    \n    . Try using a ternary instead. For example:\n    <code>if (x&gt;y) O = vec4(1,0,0,1); else O = vec4(0,1,0,1);</code>\n    becomes\n    <code>O = x&gt;y ? vec4(1,0,0,1) : vec4(0,1,0,1);</code>\n    . Much shorter, and there’s a lot you can do with it. You can even set multiple variables between\n    \n    and\n    \n    .\n  </li><li>\n    and\n    \n    use the same number of characters, but\n    \n    has a spot for initializing (before the first semicolon) and a spot for the final step after each iteration (after\n    the last semicolon). These are free slots that can be used for lines that would otherwise have to end with a\n    semicolon. Also, avoid using\n    \n    , and use the condition spot instead! You can also remove the brackets if each line ends with a comma (so it doesn’t\n    work with nested\n    \n    -loops).\n  </li></ul><p>\n  Beyond that, I use some function substitutions to reduce the code further. More on that\n  <a href=\"https://mini.gmshaders.com/p/code-golfing\">over here</a>\n  !\n</p><p>\n  I’ve put together a\n  <a href=\"https://www.shadertoy.com/view/WcfXRs\">ShaderToy demo</a>\n  with some additional variables, formatting, and comments for clarity. Every shader is different and requires using\n  different techniques, approximations, and concepts, but that is precisely what makes it so fun for me! I’m still\n  learning new stuff nearly every day!\n</p><p>Here are some questions I was asked on X.</p><h5>Do you have a favorite “trick” or “technique”? If so, what is it?</h5><h4>How did you develop the intuition for related maths?</h4><p>\n  It takes lots of time and patience. I had to push through many times when I thought a topic was over my head. If you\n  take it in small pieces, take breaks, and sleep on it, you can learn a lot! I wrote about some of the\n  <a href=\"https://mini.gmshaders.com/p/imagination\">conceptualization techniques</a>\n  that I’ve picked up over the years. That might save you some time!\n</p><h4>Do you start writing the shader in code-golfing mode, or is it a process until you reach the most optimized code? Which is the best editor for normal shaders and for code-golfing shaders? </h4><p>\n  Yes, I write in code-golfing mode because I’ve developed an intuition for it, and it feels faster to prototype at this\n  point. I still have to refine the code when I find a look that I like, though. I’m a big fan of Twigl.app, but\n  ShaderToy is great too. ShaderToy is best for its community and wealth of knowledge. I try to use it when explaining\n  my tweet shaders.\n</p><h4>How did you start writing cool shaders, and what did you use to learn it?</h4><p>\n  Well, I’ll explain more about my background later, but it started with an interest in game development. Shaders have\n  tons of applications in video game graphics—that’s what sparked my curiosity to learn.\n</p><h4>Do you have regrets related to sacrificing readability?</h4><p>\n  Nope. I’m more concerned with size optimizations that lead to slower code, but I don’t mind the unreadable code. To\n  me, that’s part of the magic of it.\n</p><h4>What’s your background that got you to the point where you could effectively learn the material?</h4><p>Growing up, I was interested in video games, especially those with “fancy” 3D graphics. When I was around 10, my friend showed me a tool called GameMaker. I tinkered around with it and learned some of the basics of drag ‘n’ drop programming, variables, and conditionals. </p><p>Over time, I started experimenting with 3D graphics in GM, even though it was (and still is) primarily a 2D game engine. It was enough to learn the basics of how 3D rendering works and the render pipeline. Later, GameMaker introduced this thing called “shaders,” which allowed developers to create more advanced effects. At the time, there weren’t many resources available, so it took a while for me to pick it up. I started posting my shaders on the GameMaker forums and got some helpful feedback from the community (shoutout to “xygthop3” for his helpful examples)! </p><p>Game development was a great place to learn about shaders because you have performance constraints (you don’t want a game to stutter), and you learn a lot about the entire rendering process in that context. In 2014, I started posting my earliest shader tutorials, sharing techniques as I learned them. The early tutorials weren’t great, but I’m glad I wrote them. In 2015, I started exploring ShaderToy, and that’s where my skills really developed. </p><p>There were so many great examples to learn from, and it was a good place to get feedback on my ideas. In 2021, I launched a new <a href=\"https://gmshaders.com\">introductory tutorial series</a> for GameMaker with GLSL 1.00. Now I post more <a href=\"https://mini.gmshaders.com\">generalized tutorials</a> on all kinds of graphics topics, ranging from math to art to design to code and more. This is definitely my best series yet, and they continue to get better. If you are interested in video games and graphics, I highly recommend starting with GameMaker or Godot. They are relatively easy to learn while still powerful enough to teach you the ropes. If software or web dev is more your thing, you can’t go wrong with ShaderToy or compute.toys.</p><p>Here are some of the great people who have helped me, directly or indirectly, along the way:</p><p> – This guy’s free shader examples were probably the greatest help along the way. His examples were a pivotal point in my understanding of a variety of graphics techniques, so thanks, Michael!</p><p> – Inigo is the author of ShaderToy and the king of raymarching. His Signed Distance Field functions are still foundational to this day. An absolute legend!</p><p> – Fabrice is probably the best shader code golfer there is, and many shaders are inspired by his work. He has taught me so many techniques over the years.</p><p> – Another major inspiration for me. Yonatan’s work convinced me to try code golfing for real on Twitter, and his brain is amazing.</p><p>\n  I’m sure there are many others whose names are eluding me at the moment, but I want to thank the entire shader\n  community for their feedback and encouragement.\n</p><p>I’ll wrap this up with a few of my favorite tweet shaders so far:</p><p>Thank you for reading! Have a great day!</p>","contentLength":14837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44354032"},{"title":"Writing a basic Linux device driver when you know nothing about Linux drivers","url":"https://crescentro.se/posts/writing-drivers/","date":1750588182,"author":"sbt567","guid":216,"unread":true,"content":"<p>A couple of months ago I bought the <a href=\"https://nanoleaf.me/en-EU/products/pegboard-desk-dock/?size=1\">Nanoleaf Pegboard Desk Dock</a>, the latest and greatest in USB-hub-with-RGB-LEDs-and-hooks-for-gadgets technology. This invention unfortunately only supports the  operating systems of Windows and macOS, which necessitated the development of a Linux driver.</p><p>Over the past few posts I’ve set up a <a href=\"https://crescentro.se/posts/windows-vm-nixos/\">Windows VM with USB passthrough</a>, and attempted to <a href=\"https://crescentro.se/posts/wireshark-usb/\">reverse-engineer the official drivers</a>,  As I was doing that, I also thought I’d message the vendor and ask them if they could share any specifications or docs regarding their protocol. To my surprise, Nanoleaf tech support responded to me within 4 hours, with a full description of the protocol that’s used both by the Desk Dock as well as their RGB strips. The docs mostly confirmed what I had already discovered independently, but there were a couple of other minor features as well (like power and brightness management) that I did not know about, which was helpful.</p><p>Today, we’re going to take a crack at writing a driver based on the (reverse-engineered) protocol, while also keeping <a href=\"https://nanoleaf.atlassian.net/wiki/spaces/nlapid/pages/2615574530/Nanoleaf+USB+Lightstrip+Communication+Protocol\">the official documentation</a> at hand. One small problem, though: I’ve never written a Linux device driver before, nor interacted with any USB device as anything else but a user.</p><p>Most Linux distros ship with <a href=\"https://www.man7.org/linux/man-pages/man8/lsusb.8.html\"></a>, a simple utility that will enumerate all USB devices connected to the system. Since I had no clue where to start from, I figured I might as well run this to see if the device appears in the listing.</p><pre><code></code></pre><p>Well, good news, it’s definitely there. But, how can the kernel know that what I have plugged in is the “Nanoleaf Pegboard Desk Dock”? The kernel (presumably) has no knowledge of this device’s existence, yet the second I plug it in to my computer it receives power, turns on and gets identified by the kernel.</p><p>As it turns out, we actually already have a driver! It’s just a very stupid one. If we run  in verbose mode and request the information just for this specific device, we will get a lot more details about it:</p><p>This is a  of information, so we need to take a quick USB class.</p><p>The USB spec is long, complicated and mainly aimed at low-level implementations (think kernel developers, device vendors, and so on). You can, of course, still read it if you enjoy being bored. But, thankfully, a kind soul collected the good parts into <a href=\"https://www.beyondlogic.org/usbnutshell/usb1.shtml\">USB in a NutShell</a>.</p><p>To summarize the summary, a USB device can have multiple , which usually explain the power requirements for the device. Most devices will have just one.</p><p>Each of those configurations can have multiple . So for example, a camera might serve as a file storage device as well as a webcam.</p><p>Finally, each interface can have multiple , whcih describe how the data is transferred. Perhaps the camera has an “isochronous” (continuous) transfer for a webcam feed, and a “bulk” transfer for moving image files over.</p><p>Going back to our device, we can see that it exposes one interface, which is a . HIDs are a class of USB devices that covers things like keyboards, mice or gamepads, and each of those categories is a separate . The kernel contains a generic driver for USB HIDs - <a href=\"https://github.com/torvalds/linux/blob/master/drivers/hid/usbhid/hid-core.c\">here it is</a> in all of its C glory.</p><p>This is why the kernel developers do not need to write specific drivers for each individual keyboard and mouse on the market. Vendors will label their device with one of the well-known HID sub-classes, then use a common protocol to implement the functionality.</p><p>Unfortunately there’s no HID specification for an RGB LED… thing (well, there’s an “LED” specification, but it’s mainly for things like status LEDs, not color LEDs) so our device is just a plain old generic HID with an interface sub-class of . This means that the kernel recognizes it and powers it correctly, but it doesn’t really know what to do with it, so it just lets it sit there.</p><p>There are two options that we have at this point:</p><ol><li>We could write a kernel driver that follows the <a href=\"https://docs.kernel.org/leds/leds-class.html\">kernel standard</a> and exposes each individual LED as 3 devices (one per color) under . Interacting with the kernel devs sounds scary (yes I realize I’m a grown-ass adult man), but even if it wasn’t, I question the utility of trying to merge drivers for a very niche product into the kernel. Also,  feels like it’s intended for status LEDs and not  anyway.</li><li>We could write a userspace driver through <a href=\"https://github.com/libusb/libusb\">libusb</a>, thus defining our own way of controlling LEDs and reducing the quality bar from “Linus Torvalds might send you a strongly worded letter if you fuck up” to “fuck it, we ball”.</li></ol><p>Given that I have no idea what I am doing, I’m gonna go for option 2, but if one of you brave souls goes for option 1, please let me know and I will print out a photo of you and frame it on my wall.</p><p>To do anything fun on Linux, you need to be . This is also the case when talking to USB devices. You could always run your drivers as , thus sidestepping the problem. But we all know that’s bad form. And if I am to distribute this driver, most people would expect to run it without privilege escalation.</p><p>Linux generally relies on <a href=\"https://wiki.archlinux.org/title/Udev\"></a> to manage handlers for hardware events. I will spare you the long story this time and just give you the magic incantation: to make your device accessible to users, you need to create a file at  <code>/etc/udev/rules.d/70-pegboard.rules</code> with the following contents:</p><pre><code></code></pre><p>where  and  are the vendor and product IDs you got from , and  is the spell that grants the currently active user permissions to manage the device. Then, unplug your device and plug it back in.</p><p>Okay, enough yapping. Let’s start with a basic Rust binary and immediately add the <a href=\"https://crates.io/crates/rusb\"></a> crate, which will serve as a binding to .</p><pre data-lang=\"bash\"><code data-lang=\"bash\"></code></pre><p>To get going, we can try to get a handle on the device and get basic information about it, just like . This is explained pretty well in the crate readme, so I will not dwell on it too much. We’ll need a , which gives us a handy  method that we can use to get a handle to a device.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>Now that we have access to the device, we want to write a simple payload to it. For that, we first need to claim an interface. Recall that interfaces are essentially capabilities of the device, and through  we learned that we only have one interface with the ID () of . Thankfully, there’s an obvious  method on a .</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>So, what you just experienced is the joy of  error messages. This message, at 4 characters, is in fact pretty generous - you might be greeted with a message that only says , and good luck debugging that. In general,  means that something is already holding the device open, so you cannot do anything with it. However, you won’t actually be told what is holding it open.</p><p>The secret is that the device is, of course, being held open by the kernel. This is the generic driver I talked about earlier. And the secret solution is to release the kernel driver, if it is currently active on the device.</p><p>This requires you to have write access to the device, so if you did not do the  song and dance from earlier in this article, prepare to prefix all future invocations of your driver with .</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Note that the kernel driver won’t be reattached automatically, so you might want to call <code>device.attach_kernel_driver(INTERFACE)</code> if, for some reason, you need it back.</p><h2>Sending data to the device</h2><p>Surely,  we are ready to write out some bytes to a device?</p><p>Well, almost! If we try to naively start typing out something like , the IDE will helpfully suggest three options: <a href=\"https://docs.rs/rusb/latest/rusb/struct.DeviceHandle.html#method.write_bulk\"></a>, <a href=\"https://docs.rs/rusb/latest/rusb/struct.DeviceHandle.html#method.write_control\"></a> and <a href=\"https://docs.rs/rusb/latest/rusb/struct.DeviceHandle.html#method.write_interrupt\"></a>. This corresponds to three out of four possible types of endpoints that the USB standard supports. Once again, <a href=\"https://www.beyondlogic.org/usbnutshell/usb4.shtml\">USB in a NutShell</a> comes in clutch with an explanation of what each of the endpoint types mean. Thankfully, we can mostly skip over the implementation details, as we can once again refer to the  readout from earlier:</p><pre><code></code></pre><p>In USB parlance,  is always something that the device sends to the host, and  is always something that the host sends to the device. Basically, since this interface has two endpoints, and only one of them is an  endpoint, it’s safe to assume we’re looking to  on endpoint . The peculiarities of Interrupt endpoints will absolutely come back to bite us in a couple of minutes, but for now we can keep them out of sight and out of mind.</p><p>For testing purposes, I want to make the pegboard show a solid red color. According to my earlier investigation, this means that I need to send , followed by 64 repeats of , to an endpoint at . In addition,  only exposes the blocking API of , so we will also need to define a timeout after which  will give up and error out.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>And… just like that, the pegboard now shows a solid red color! We didn’t need to worry about manually splitting packets or any of the underlying implementation, just open up a pipe and write to it! It’s that easy.</p><p>Let’s run it again to make sure it was not a fluke!</p><h2>So, about those interrupts…</h2><p>Yeah, so if you happen to be following along, and you ran the same binary twice, you’ll notice that the firmware of the pegboard crashes unceremoniously, and shortly after reverts to its default animation. And if I go back to the original packet capture - or the official docs - it’s pretty obvious why: the device sends us back a response, but we never read it.</p><p>It turns out that “interrupts” are named as such for a reason, and we should probably handle them as they come in. However, the USB spec defines that the  must poll for interrupts. A device cannot interrupt the host by itself.</p><p>For our simple “driver”, this means we want to poll the device right after we write to it. Thankfully,  gives us a  method, and we have already sneakily defined the  constant. Let’s do just that:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Running this, we see that the contents of  are , which corresponds to  I got from the research. And since we clear the interrupt buffer every time now, we can run this binary many times to define a single solid color on the device. Neat!</p><p>Of course, this is… not really what you want. The device may issue more interrupts. For example, there’s a single button on the desk dock, which can be clicked, double-clicked or long-clicked, and each of those will issue a different interrupt. So what we  want is a background task of sort that will actively poll the device for interrupts and process them as they come in.</p><p>This is where you can get wild with async Rust, , channels, and other fun stuff. That would certainly be the  to do it in an actual, serious driver. But to avoid getting into complexities of async Rust, let’s keep it vanilla and use <a href=\"https://doc.rust-lang.org/std/thread/fn.scope.html\"></a>.</p><p>We’ll also adjust the timeout for reading interrupts to be 1 millisecond, as requested by the device (the  value in the  readout). This doesn’t mean we will get an interrupt every millisecond, just that the device  send one at that rate. If the device sends nothing (i.e., we get  ), we will just continue with the loop.</p><p>Put together, that might look something like this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><pre><code></code></pre><p>This… works! Of course, we send no more color frames to the device, so we won’t get any more interrupts, but we now have two threads, one which we can use to change the colors shown, and another which we can use to read the interrupts.</p><p>There are some quirks with this device: it seems to require a steady stream of color frames, otherwise it reverts to “offline mode” as it does not receive any new frames from the host, and the first frame’s brightness is significantly lower than the brightness of future frames. Not to mention that, despite what the official protocol documentation would have you believe, the colors seem to be in GRB instead of RGB format, and if you make the device , it will just hard-reset after a couple of seconds. That is, I suppose, a part of the joy of coding.</p><p>But this small proof of concept shows that writing simple device drivers is not all that hard, and that 50 lines of code can bring you quite far. Over the next few weeks I hope to polish up my proof of concept, make a small GUI for it, pack it up and share it with the two other Linux users who own this dumb thing. And I’m happy to have learned the basics of reverse-engineering a simple USB device driver, and using that as a foundation for writing my own. Even if I could have just asked for the spec earlier and not fussed with it.</p>","contentLength":12105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44345681"}],"tags":["dev","hn"]}