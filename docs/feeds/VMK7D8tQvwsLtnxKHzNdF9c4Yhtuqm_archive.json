{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"Define policy forbidding use of AI code generators","url":"https://github.com/qemu/qemu/commit/3d40db0efc22520fa6c399cf73960dced423b048","date":1750894015,"author":"todsacerdoti","guid":258,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44382752"},{"title":"A new pyramid-like shape always lands the same side up","url":"https://www.quantamagazine.org/a-new-pyramid-like-shape-always-lands-the-same-side-up-20250625/","date":1750881667,"author":"robinhouston","guid":224,"unread":true,"content":"<p>Achieving the right balance between the weight of the loading zone and the weight of the rest of the tetrahedron is easy in the abstract realm of mathematics —&nbsp;you can define the weight distribution without a care for whether it’s physically possible. You might, for instance, let parts of the shape weigh nothing at all, while concentrating a large amount of mass in other parts.</p><p>But that wasn’t entirely satisfying to the mathematicians. Almádi, Dawson and Domokos wanted to hold the shape in their hands. Was it possible to make a monostable tetrahedron in the real world, with real materials?</p><p>The team returned to their computer search. They considered the various ways in which monostable tetrahedra might tip onto their stable face. For instance, one kind of tetrahedron might follow a very simple path: Face A tips to Face B, which tips to Face C, which tips to Face D. But in a different tetrahedron, Face A might tip to Face B, and both Face B and Face D will tip to Face C.</p><p>The loading zones for these different tetrahedra look very different. The team calculated that to get one of these “falling patterns” to work, they would need to construct part of the shape out of a material about 1.5 times as dense as the sun’s core.</p><p>They focused on a more feasible falling pattern. Even so, part of their tetrahedron would have to be about 5,000 times as dense as the rest of it. And the materials had to be stiff — light, flimsy materials that could bend would ruin the project, since it’s easy to make a round or smooth shape (like the roly-poly) monostable.</p><p>In the end, they designed a tetrahedron that was mostly hollow. It consisted of a lightweight carbon fiber frame and one small portion constructed out of tungsten carbide, which is denser than lead. For the lighter portions to have as little weight as possible, even the carbon fiber frames had to be hollow.</p><p>With this blueprint in hand, Domokos got in touch with a <a href=\"https://cncnagykft.hu/\">precision engineering company</a> in Hungary to help build the tetrahedron. They had to be incredibly accurate in their measurements, even when it came to the weight of the tiny amounts of glue used to connect each of the shape’s faces. Several frustrating months and several thousand euros later, the team had a lovely model that didn’t work at all. Then Domokos and the chief engineer of the model spotted a glob of stray glue clinging to one of its vertices. They asked a technician to remove it. About 20 minutes later, the glue was gone and Almádi received a text from Domokos.</p><p>“It works,” the message read. Almádi, who was on a walk, started jumping up and down in the street. “Seeing the lines on the computer is very far from reality,” he said. “That we designed it, and it works, it’s kind of fantastic.”</p><p>“I wanted to be an architect,” he added. “So this is still very strange for me — how did I end up here?”</p><p>In the end, the work on monostable tetrahedra didn’t involve any particularly sophisticated math, according to <a href=\"https://www.math.brown.edu/reschwar/\">Richard Schwartz</a> of Brown University. But, he said, it’s important to ask this kind of question in the first place. It’s the kind of problem that’s often easiest to overlook. “It’s a surprising thing, a leap, to conjecture that these things would exist,” Schwartz said.</p><p>At the moment, it’s not clear what new theoretical insights the model of the monostable tetrahedron will provide — but experimenting with it might help mathematicians uncover other intriguing questions to ask about polyhedra. In the meantime, Domokos and Almádi are working to apply what they learned from their construction to help engineers design lunar landers that can turn themselves right side up after falling over.</p><p>In any case, sometimes you just need to see something to believe it, Schwartz said. “Even for theoretical math, geometry especially, people are kind of right to be skeptical because it’s quite hard to reason spatially. And you can make mistakes, people do.”</p><p>“Conway didn’t say anything about it, he just suggested it — never proved it, never proved it wrong, nothing. And now here we are, I don’t know, 60 years later,” Almádi said. “If he were still alive, we could put this on his desk and show him: You were right.”</p>","contentLength":4243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44381297"},{"title":"-2000 Lines of code","url":"https://www.folklore.org/Negative_2000_Lines_Of_Code.html","date":1750881193,"author":"xeonmc","guid":223,"unread":true,"content":"<p>In early 1982, the Lisa software team was trying to buckle down for the big push to ship the software within the next six months.  Some of the managers decided that it would be a good idea to track the progress of each individual engineer in terms of the amount of code that they wrote from week to week.  They devised a form that each engineer was required to submit every Friday, which included a field for the number of lines of code that were written that week.</p><p>\nBill Atkinson, the author of Quickdraw and the main user interface designer, who was by far the most important Lisa implementer, thought that lines of code was a silly measure of software productivity.  He thought his goal was to write as small and fast a program as possible, and that the lines of code metric only encouraged writing sloppy, bloated, broken code.</p><p>\nHe recently was working on optimizing Quickdraw's region calculation machinery, and had completely rewritten the region engine using a simpler, more general algorithm which, after some tweaking, made region operations almost six times faster.  As a by-product, the rewrite also saved around 2,000 lines of code.</p><p>\nHe was just putting the finishing touches on the optimization when it was time to fill out the management form for the first time.  When he got to the lines of code part, he thought about it for a second, and then wrote in the number: -2000.</p><p>\nI'm not sure how the managers reacted to that, but I do know that after a couple more weeks, they stopped asking Bill to fill out the form, and he gladly complied.\n  </p>","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44381252"},{"title":"OpenAI charges by the minute, so speed up your audio","url":"https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/","date":1750857445,"author":"georgemandis","guid":219,"unread":true,"content":"<p>Want to make OpenAI transcriptions faster and cheaper? Just speed up your audio.</p><p>I mean that very literally. Run your audio through <a href=\"https://gist.github.com/georgemandis/4fd62bf5027b7a058f913d5dc32c2040\">ffmpeg</a> at 2x or 3x before transcribing it. You’ll spend fewer tokens and less time waiting with almost no drop in transcription quality.</p><p>Here’s a script combining of all my favorite little toys and tricks to get the job. You’ll need <a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a>, <a href=\"https://ffmpeg.org\">ffmpeg</a> and <a href=\"https://github.com/simonw/llm\">llm</a> installed.</p><pre><code># Extract the audio from the video\nyt-dlp -f 'bestaudio[ext=m4a]' --extract-audio --audio-format m4a -o 'video-audio.m4a' \"https://www.youtube.com/watch?v=LCEmiRjPEtQ\" -k;\n\n# Create a low-bitrate MP3 version at 3x speed\nffmpeg -i \"video-audio.m4a\" -filter:a \"atempo=3.0\" -ac 1 -b:a 64k video-audio-3x.mp3;\n\n# Send it along to OpenAI for a transcription\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form <a href=\"https://george.mand.is/cdn-cgi/l/email-protection\" data-cfemail=\"83e5eaefe6bec3f5eae7e6ecaee2f6e7eaecaeb0fbadeef3b0\">[email&nbsp;protected]</a> \\\n  --form model=gpt-4o-transcribe &gt; video-transcript.txt;\n\n# Get a nice little summary\n\ncat video-transcript.txt | llm --system \"Summarize the main points of this talk.\"\n</code></pre><p>I just saved you time by jumping straight to the point, but read-on if you want more of a story about how I accidentally discovered this while trying to summarize a 40-minute talk from Andrej Karpathy.</p><p>Also read-on if you’re wondering why I didn’t just use the built-in auto-transcription that YouTube provides, though the short answer there is easy: I’m sort of a doofus and thought—incorrectly—it wasn’t available. So I did things the hard way.</p><h3>I Just Wanted the TL;DW(atch)</h3><p>A former colleague of mine sent me <a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">this talk</a> from Andrej Karpathy about how AI is changing software. I wasn’t familiar with Andrej, but saw he’d worked at Tesla. That coupled with the talk being part of a Y Combinator series and 40 minutes made me think “Ugh. Do I… really want to watch this? Another 'AI is changing everything' talk from the usual suspects, to the usual crowds?”</p><p>If ever there were a use-case for dumping something into an LLM to get the gist of it and walk away, this felt like it. I respected the person who sent it to me though and wanted to do the noble thing: use AI to summarize the thing for me, blindly trust it and engage with the person pretending I had watched it.</p><p>My first instinct was to pipe the transcript into an LLM and get the gist of it. <a href=\"https://gist.github.com/simonw/9932c6f10e241cfa6b19a4e08b283ca9\">This script</a> is the one I would previously reach for to pull the auto-generated transcripts from YouTube:</p><pre><code>yt-dlp --all-subs --skip-download \\\n  --sub-format ttml/vtt/best \\\n  [url]\n</code></pre><p>For some reason though, no subtitles were downloaded. I kept running into an error!</p><p>Later, after some head-scratching and rereading <a href=\"https://github.com/yt-dlp/yt-dlp?tab=readme-ov-file#subtitle-options\">the documentation</a>, I realized my version (2025.04.03) was outdated.</p><p>: Updating to the latest version (2025.06.09) fixed it, but for some reason I did not try this  going down a totally different rabbit hole. I guess I got this little write-up and exploration out of it though.</p><p>If you care more about summarizing transcripts and less about the vagaries of audio-transcriptions and tokens, this is the correct answer and your off-ramp.</p><h3>My Transcription Workflow</h3><p>I already had an old, home-brewed script that would extract the audio from any video URL, pipe it through <a href=\"https://github.com/openai/whisper\">whisper</a> locally and dump the transcription in a text file.</p><p>That worked, but I was on dwindling battery power in a coffee shop. Not ideal for longer, local inference, mighty as my M3 MacBook Air still feels to me. I figured I would try offloading it to <a href=\"https://platform.openai.com/docs/guides/speech-to-text\">OpenAI’s API</a> instead. Surely that would be faster?</p><h3>Testing OpenAI’s Transcription Tools</h3><p>Okay, using the  model it’s  pretty slow, but it gets the job done. Had I opted for the model I knew and moved on, the story might end here.</p><p>However, out of curiosity, I went straight for the newer  model first. It’s built to handle multimodal inputs and promises faster responses.</p><p>I quickly hit another roadblock: there’s a 25-minute audio limit and my audio was nearly 40 minutes long.</p><h3>Let's Try Something Obvious</h3><p>At first I thought about trimming the audio to fit somehow, but there wasn’t an obvious 14 minutes to cut. Trimming the beginning and end would give me a minute or so at most.</p><p>An interesting, weird idea I thought about for a second but never tried was cutting a chunk or two out of the middle. Maybe I would somehow still have enough info for a relevant summary?</p><p>Then it crossed my mind—<strong>what if I just sped up the audio before sending it over?</strong> People listen to podcasts at accelerated 1-2x speeds all the time.</p><pre><code>ffmpeg -i video-audio.m4a -filter:a \"atempo=2.0\" -ac 1 -b:a 64k video-audio-2x.mp3\n</code></pre><p>Ta-da! Now I had something closer to a 20 minute file to send to OpenAI.</p><p>I uploaded it and… it worked like a charm! <a href=\"https://gist.github.com/georgemandis/b2a68b345262b94782fa6b08e41fbcf2\">Behold the summary</a> bestowed upon me that gave me enough confidence to reply to my colleague as though I had watched it.</p><p>But there was something... interesting here. Did I just stumble across a sort of obvious, straightforward hack? Is everyone in the audio-transcription business already doing this and am I just haphazardly bumbling into their secrets?</p><h3>Why This Works: Our Brains Forgive, and So Does AI</h3><p>There’s an interesting parallel here in my mind with optimizing images. Traditionally you have lossy and lossless file formats. A lossy file-format kind of gives away the game in its description—the further you crunch and compact the bytes the more fidelity you’re going to lose. It works because the human brain just isn’t likely to pick-up on the artifacts and imperfection</p><p>But even with a “lossless” file format there are tricks you can lean into that rely on the limits of human perception. One of the primary ways you can do that with a PNG or GIF is reducing the number of unique colors in the palette. You’d be surprised by how often a palette of 64 colors or fewer might actually be enough and perceived as significantly more.</p><p>There’s also a parallel in my head between this and the brain’s ability to still comprehend text with spelling mistakes, dropped words and other errors, i.e. <a href=\"https://en.wikipedia.org/wiki/Transposed_letter_effect\">transposed letter effects</a>. Our brains have a knack for filling in the gaps, and when you go looking through the world with magnifying glass you'll start to notice lots of them.</p><p>Speeding up the audio starts to drop the more subtle sounds and occasionally shorter words from the audio, but it doesn’t seem to hurt my ability to  what I’m hearing—even if I do have to focus. These audio transcription models seem to be pretty good at this as well.</p><h3>Wait—how far can I push this? Does It Actually Save Money?</h3><p>Turns out yes. OpenAI <a href=\"https://platform.openai.com/docs/pricing\">charges for transcription</a> based on audio tokens, which scale with the duration of the input. Faster audio = fewer seconds = fewer tokens.</p><p>Here are some rounded numbers based on the 40-minute audio file breaking down the audio input and text output token costs:</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>That’s a solid 33% price reduction on input tokens at 3x! However the bulk of your costs for these transcription models are still going to be the output tokens. Those are priced at $10 per 1M tokens whereas audio input tokens are priced at $6 per 1M token as of the time of this writing.</p><p>Also interesting to note—my output tokens for the 2x and 3x versions were exactly the same: 2,048. This kind of makes sense, I think? To the extent the output tokens are a reflection of that model's ability to understand and summarize the input, my takeaway is a “summarized” (i.e. reduced-token) version of the same audio yields the same amount of comprehensibility.</p><p>This is also probably a reflection of the 4,096 token ceiling on transcriptions generally when using the  model. I suspect half the context window is reserved for the output tokens and this is basically reflecting our request using it up in its entirety. I suspect we might get diminishing results with longer transcriptions.</p><p>So the back-of-the-envelope calculator for a single transcription looks something like this:</p><pre><code>6 * (audio_input_tokens / 1_000_000) + 10 * (text_output_tokens / 1_000_000);\n</code></pre><p>That does  quite seem to jibe with the estimated cost of $0.006 per minute stated on the pricing page, at least for the 2x speed. That version (19-20 minutes) seemed to cost about $0.09 whereas the 3x version (13 minutes) cost about $0.07 (pretty accurate actually), if I’m adding up the tokens correctly.</p><pre><code># Pricing for 2x speed\n6 * (11_856 / 1_000_000) + 10 * (2_048 / 1_000_000) = 0.09\n\n# Pricing for 3x speed\n6 * (7_904 / 1_000_000) + 10 * (2_048 / 1_000_000) = 0.07\n</code></pre><p>It would seem that estimate isn’t just based on the length of the audio but also some assumptions around how many tokens per minute are going to be generated from a normal speaking cadence.</p><p>Comparing these costs to  is easy because the pricing table more confidently advertises the cost—not “estimated” cost—as a flat $0.006 per minute. I’m assuming that’s minute of audio processed, not minute of inference.</p><p>The  model actually compares pretty favorably.</p><table><tbody></tbody></table><p>In short, yes! It’s not particularly rigorous, but it seems like we reduced the cost of transcribing our 40-minute audio file by 23% from $0.09 to $0.07 simply by speeding up the audio.</p><p>If we could compare to a 1x version of the audio file trimmed to the 25-minute limit, I bet we could paint an even more impressive picture of cost reduction. We kind of can with the  chart. You could make the case this technique reduced costs by 67%!</p><p>I don’t know—I didn’t watch it, lol. That was the whole point. And if that answer makes you uncomfortable, buckle-up for this future we're hurtling toward. Boy, howdy.</p><p>More helpfully, I didn’t compare word-for-word, but spot checks on the 2x and 3x versions looked solid. 4x speed was too fast—the transcription started getting hilariously weird. So, 2x and 3x seem to be the sweet spot between efficiency and fidelity, though it will obviously depend on how fast the people are speaking in the first place.</p><p>That sure didn't stop my call to summarize from <a href=\"https://gist.github.com/georgemandis/1ec4ef084789f92ee06ac6283338a194#file-summarization-md\">trying</a> though.</p><p>Hey, not the worst talk I've been to!</p><p>Always, in short, to save time and money, consider doubling or tripling the speed of the audio you want to transcribe. The trade-off is, as always, fidelity, but it’s not an insignificant savings.</p><p>Simple, fast, and surprisingly effective.</p><ul><li>OpenAI charges for transcriptions based on audio duration () or tokens ().</li><li>You can  with  before uploading to save time and money.</li><li>This reduces audio tokens (or duration), lowering your bill.</li><li> works well.</li><li>? Probably too much—but fun to try.</li></ul><p>If you find problems with my math, have questions, found a more rigorous study qualitatively comparing different output speeds please <a href=\"https://george.mand.is/contact\">get in touch</a>! Or if you thought this was so cool you want to <a href=\"https://george.mand.is/hire\">hire me</a> for something fun...</p>--","contentLength":10697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44376989"},{"title":"Gemini CLI","url":"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/","date":1750857046,"author":"sync","guid":257,"unread":true,"content":"<div>\n      [{\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 7, \"fields\": {\"name\": \"Article Improvements - March 2025\", \"survey_id\": \"article-improvements-march-2025_250321\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"5a12fd89-d978-4a1b-80e5-2442a91422be\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How could we improve this article?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"30122b0d-1169-4376-af7c-20c9de52c91c\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Make it more concise\\\"}, {\\\"id\\\": \\\"18f3016a-7235-468b-b246-ffe974911ae9\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Add more detail\\\"}, {\\\"id\\\": \\\"5d19c11d-6a61-49d3-9f1d-dad5d661ba4f\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Make it easier to understand\\\"}, {\\\"id\\\": \\\"97064d1f-d9af-4a83-a44f-a84f8ed899d6\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Include more images or videos\\\"}, {\\\"id\\\": \\\"a9ec2a70-c7c5-4f00-a179-31a7b5641879\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"It's fine the way it is\\\"}]}}]\", \"target_article_pages\": true}}]\n    </div>","contentLength":6055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44376919"},{"title":"A new PNG spec","url":"https://www.programmax.net/articles/png-is-back/","date":1750823683,"author":"tbillington","guid":256,"unread":true,"content":"<ul><li><h3>Proper HDR support (future‐proof, too!)</h3><p>\n\t\t\t\t\t\t\t\tFigure 1 shows the colors our eyes can see.\n\t\t\t\t\t\t\t\tThe smaller, inner triangle represents the color space of most images.<p>\n\t\t\t\t\t\t\t\tThe larger, outer triangle represents the colors that are typical with a High Dynamic Range (HDR) image.\n\t\t\t\t\t\t\t</p></p><p>This new HDR support uses only 4 bytes (plus the usual PNG chunk overhead).</p></li><li><h3>Finally recognizes APNGs (animations!)</h3><p>Animated PNGs were proposed by Mozilla quite some time ago. Support was added to Firefox, but other programs hesitated to adopt them.</p><p>Today, animated PNGs are widely supported. It is time for the spec to reflect reality.</p></li><li><h3>Officially supports Exif data</h3><p>Exif stores additional information such as copyright information and even camera lens and GPS location of a photograph.</p></li><li><h3>General tidying up—fixing errata, clarifications, etc.</h3></li></ul><p>The last PNG spec was released over 20 years ago. Technology has advanced a lot since then. We're talking 3.5 years before the first iPhone.</p><p>In fact, technological advancement is what resurrected PNG. The W3C Timed Text Working Group (think:&nbsp;subtitles) needed HDR support in PNG. A proposal was made, but a few experts decided we could do better.</p><p>Momentum built, and additional parties became interested. Before we knew it, we had representation from (in alphabetical order) Adobe, Apple, BBC, Comcast / NBCUniversal, Google, MovieLabs, and of course W3C, who maintains the spec. It's quite the dream team.</p><p>With these titans behind it, the image format is back with full momentum. Work has already begun on the  PNG spec updates.</p><p>Many of the programs you use already support the new PNG spec: Chrome, Safari, Firefox, iOS/macOS, Photoshop, DaVinci Resolve, Avid Media Composer...</p><p>Plus, you saw some broadcast companies in that list above. Behind the scenes, hardware and tooling are being updated to support the new PNG spec. The next time you see a news ticker scrolling  or the score banner update as your team pulls out a clutch play, check if it is HDR.</p><p>I know you all immediately wondered, . We're already working on that. And parallel encoding/decoding, too! Just like this update, we want to make sure we do it right.</p><p>We expect the next PNG update (Fourth Edition) to be short. It will improve HDR &amp; Standard Dynamic Range (SDR) interoperability. While we work on that, we'll be researching compression updates for PNG Fifth Edition.</p>","contentLength":2363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44373504"},{"title":"Thnickels","url":"https://thick-coins.net/?_bhlid=8a5736885893b7837e681aa73f890b9805a4673e","date":1750810407,"author":"jxmorris12","guid":255,"unread":true,"content":"<p>Hello this is Theodore Nichols and welcome to my world wide web page.</p><p> I have partnered with someone to help me take <a href=\"https://www.legboot.com/thnickels-pre-order/\">pre-orders</a> for thnickels. Slots limited.</p><p> I have received hundreds of e-Mails from my flyer. I told my grandson that flyers still work in this day and age. He said societal media is the only way, but I was right. And he is still very thin. <b>Thank you all for the interest, I will be in touch.</b>-Theo</p><p> Thank you for the huge interest in my coins. Yes there are some pre order slots left but going fast.</p><p>Nickels are too thin we need a much heftier coin. That is why I am making thicker coins in my new <a href=\"https://thick-coins.net/?_bhlid=8a5736885893b7837e681aa73f890b9805a4673e#mint\">minting facility</a>.</p><p>Some burglars recently entered my garage to pilfer my things. The only weapon nearby was a , which I brandished at the ruffians.</p><img src=\"https://thick-coins.net/ten%20cents.jpg\" alt=\"two nickels was not an effective weapon\"><p>The burglars  about the slight bag of coins. They did not  me or .</p>They stole the nickels and my best push broom. <b>I have never felt so humiliated.</b><p>That's when I resolved to NEVER be disrespected about my pocketchange again.</p><h2>Introducing: Thicker Coins</h2><p>My new coins have a  when compared to currency from the U.S. Mint.</p><p>The enhanced weight is both  to good guys and  to bad guys.</p><img src=\"https://thick-coins.net/size%20comparison.jpg\" alt=\"wimpy nickel vs beefy thnickel\"><p>You can see in the photograph above how  a regular \"nickel\" is between my fingers.</p><p>The \"thnickel\" is , there is much more coin per coin.</p><p>Feel free to review my designs below. <i>I welcome feedback on the designs as long as it is positive and respectful of me.</i></p><p>I have converted my garage into a mint. I am dedicated to producing enough thnickels for everyone in need of respect.</p><img src=\"https://thick-coins.net/in%20garage.jpg\" alt=\"theodore in garage\"><p>My new state of some art facility is equipped with several tools and a powerful workhorse (me)!</p><p>Please help spread the word about my coins.</p><p>Post it in your neighborhood or city to let the people know about thnickels.</p><h3>NUMMOS CRASSIORES OMNIBUS</h3><p>If you want a thnickel coin or have any questions about my new mint please e-Mail me or call my phone.</p><img src=\"https://thick-coins.net/thnickel%20rotate.gif\" alt=\"coin2\">","contentLength":1839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44372424"},{"title":"Microsoft Edit","url":"https://github.com/microsoft/edit","date":1750810024,"author":"ethanpil","guid":254,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44372380"},{"title":"Fun with uv and PEP 723","url":"https://www.cottongeeks.com/articles/2025-06-24-fun-with-uv-and-pep-723","date":1750790486,"author":"deepakjois","guid":253,"unread":true,"content":"<p>For the longest time, I have been frustrated with Python because I couldn’t use it for one-off scripts. I had to first ensure it was running in an environment where it could find the right Python version and the dependencies installed. That is now a thing of the past.</p><p>If you are not a Pythonista (or one possibly living under a rock), <a href=\"https://docs.astral.sh/uv/\">uv</a> is <em>an extremely fast Python package and project manager, written in Rust.</em></p><p>uv also provides this nifty tool called  (kinda like  from the Node/NPM ecosystem for Javascript/Typescript packages) which can be used to invoke a Python tool inside a package.  takes care of creating a (cached) disposable virtual environment, setting up the right Python version and installing all the dependencies before running.</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p><a href=\"https://peps.python.org/pep-0723/\">PEP 723</a> is a Python Enhancement Proposal that <em>specifies a metadata format that can be embedded in single-file Python scripts to assist launchers, IDEs and other external tools which may need to interact with such scripts.</em></p><p>Here is the example directly lifted from the proposal:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Combining uv and the PEP-723 metadata inside a Python script, we can run the script in the previous section as follows:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>We can combine things we covered in the previous sections to create a simple executable script that can extract YouTube transcripts.</p><p>First we create a Python script with a shebang and inline metadata.</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Note the shebang line: <code>#!/usr/bin/env -S uv run --script</code>. It is important to specify  with the  flag when used on the shebang line.</p><p>We save this script as  and then make it executable with .</p><p>We can now run the script like:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>This opens up a lot of possibilities for running Python code more seamlessly. Before this I used to prefer <a href=\"https://go.dev/\">Go</a> for one-off scripts because it was easy to create a self-contained binary executable. But now that I could use uv, I coded up a quick MCP server in Python for extracting YouTube transcripts. Check it out on Github at <a href=\"https://github.com/cottongeeks/ytt-mcp\">cottongeeks/ytt-mcp</a>.</p>","contentLength":1905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44369388"},{"title":"Man 'refused entry into US' as border control catch him with bald JD Vance meme","url":"https://www.dublinlive.ie/news/world-news/man-refused-entry-us-border-31925059","date":1750789273,"author":"miles","guid":252,"unread":true,"content":"<p>A 21-year-old Norwegian tourist claims he was denied entry to the United States and harassed by ICE agents after they discovered a JD Vance meme on his phone.</p><p>Mads Mikkelsen arrived at New Jersey's Newark Airport on June 11 when he was pulled aside by border control and placed in a cell, he told Norwegian outlet Nordlys. Mads was travelling to the States to visit friends, first in New York and then in Austin, Texas, but suffered \"harassment and abuse of power\" at the hands of US immigration authorities.</p><p>\"I felt prejudiced, suspected and simply humiliated even then, in front of many other people at the airport,\" The Tromsø native recounted. \"They took me to a room with several armed guards, where I had to hand over my shoes, mobile phone and backpack.\"</p><p>Officers quizzed Mads about his visit, and his plans, before adopting a personal line of questioning. \"They asked direct questions about drug smuggling, terrorist plans and right-wing extremism, completely without reason,\" he claimed.</p><p>\"They demanded full information about everyone I was going to meet in the US, including name, address, phone number and what they did for work.\" Mads' mother was due to meet up with him a few weeks into his stay and the pair had planned to travel to several national parks.</p><p>\"I had travelled for twelve hours, slept poorly, and was physically and mentally completely exhausted even before they started the questioning,\" he continued. A strenuous crackdown by U.S. Customs and Border Protection has followed President Donald Trump's return to office, with the service being allowed to search phones.</p><p>A French scientist was denied entry at the border earlier this year, in March, after officers unearthed messages criticising Trump on his phone. Mikkelsen explained: \"They threatened me with a minimum fine of $5,000 or five years in prison if I refused to provide the password to my phone.\"</p><p>After handing over his password, Mads was told he would not be allowed to go through with his planned vacation after two images were not to the officers' liking. One image was of a meme showcasing JD Vance with a bald, egg-shaped head. Variations of the image were shared endlessly in March on social media, with the Vice President himself posting his own version.</p><p>The other picture showed Mads with a wooden pipe which he had made years prior. \"Both pictures had been automatically saved to my camera roll from a chat app, but I really didn't think that these innocent pictures would put a stop to my entry into the country,\" the 21-year-old admitted.</p><p>Mads told Nordlys he tried to explain the images as being harmless and meant as jokes but the immigration authorities ignored his pleas. He claims he was then strip-searched, forced to give blood samples, a facial scan and fingerprints.</p><p>\"Later I was taken back in, and the situation got even worse. I was pushed up against a wall and was strip-searched with a lot of force. They were incredibly harsh and used physical force the whole time,\" he claimed.</p><p>\"I felt completely devastated and broke down, and was close to crying several times. I was on the verge of panic. It felt like I was a terrorist suspect where I was sitting. I tried to pull myself together several times, but in the end, I just wanted to get home again.\"</p><p>The Norwegian adds he was placed in a cell for a further five hours, refused food or water and placed on a plane back to Oslo the same day he arrived for the holiday of a lifetime. \"I don't feel there is any point in contacting the State Department, nor do I think they have any power against such a powerful and strict country as the United States,\" Mads conceded.</p><p>Mathias Rongved, a spokesperson at the Ministry of Foreign Affairs has warned fellow Norwegians that it is their duty to be clued up on US regulations before entering the country. \"Most trips to the US go without any particular problems,\" he said.</p><p>\"Entry regulations can change at short notice, and it is the traveller's responsibility to have valid documents and be familiar with the current entry regulations. It is the immigration authorities upon arrival who decide whether you are rejected at the border. Norwegian authorities cannot intervene in this decision.</p><p>\"It is also not necessarily the case that we receive a message either from other countries' border authorities or the Norwegian traveller if the person in question is not allowed to enter a country.\"</p><p><b>Join our Dublin Live breaking news service on WhatsApp. Click</b><a href=\"https://chat.whatsapp.com/DJTiyHweTeJHu5aLKRIiPc\" target=\"_blank\" rel=\"nofollow\"></a><b>to receive your daily dose of Dublin Live content. We also treat our community members to special offers, promotions, and adverts from us and our partners. If you don’t like our community, you can check out any time you like. If you’re curious, you can read our</b><a href=\"https://www.reachplc.com/site-services/privacy-policy\" target=\"_blank\" rel=\"nofollow\"></a></p>","contentLength":4708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44369140"},{"title":"Writing toy software is a joy","url":"https://blog.jsbarretto.com/post/software-is-joy","date":1750777752,"author":"bundie","guid":251,"unread":true,"content":"<p>I am a huge fan of Richard Feyman’s famous quote:</p><blockquote><p>“What I cannot create, I do not understand”</p></blockquote><p>I think it’s brilliant, and it remains true across many fields (if you’re willing to be a little creative with the\ndefinition of ‘create’). It is to this principle that I believe I owe everything I’m truly good at. Some will tell you\nto avoid reinventing the wheel, but they’re wrong: you  build your own wheel, because it’ll teach you more about\nhow they work than reading a thousand books on them ever will.</p><p>In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the\nmost joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and\nindustrialised. Software development needs more simple joy, and I’ve found that creating toy programs is a great way to\nremember why I started working with computers again.</p><p>Toy programs follow the 80:20 rule: 20% of the work, 80% of the functionality. The point is  to build\nproduction-worthy software (although it is true that some of the best production software began life as a toy).\nAggressively avoid over-engineering, restrict yourself to only whatever code is necessary to achieve your goal. Have\nevery code path panic/crash until you’re forced to implement it to make progress. You might be surprised by just how\neasy it is to build toy versions of software you might previously have considered to be insummountably difficult to\ncreate.</p><p>I’ve been consistently surprised by just how often some arcane nugget of knowledge I’ve acquired when working on a toy\nproject has turned out to be immensely valuable in my day job, either by giving me a head-start on tracking down a\nproblem in a tool or library, or by recognising mistakes before they’re made.</p><p>Understanding the constraints that define the shape of software is vital for working with it, and there’s no better way\nto gain insight into those constraints than by running into them head-first. You might even come up with some novel\nsolutions!</p><p>Here is a list of toy programs I’ve attempted over the past 15 years, rated by difficulty and time required. These\nratings are estimates and assume that you’re already comfortable with at least one general-purpose programming language\nand that, like me, you tend to only have an hour or two per day free to write code. Also included are some suggested\nresources that I found useful.</p><h3>Regex engine (difficulty = 4/10, time = 5 days)</h3><p>A regex engine that can read a POSIX-style regex program and recognise strings that match it. Regex is simple yet\nshockingly expressive, and writing a competent regex engine will teach you everything you need to know about using the\nlanguage too.</p><h3>x86 OS kernel (difficulty = 7/10, time = 2 months)</h3><p>A multiboot-compatible OS kernel with a simple CLI, keyboard/mouse driver, ANSI escape sequence support, memory manager,\nscheduler, etc. Additional challenges include writing an in-memory filesystem, user mode and process isolation, loading\nELF executables, and supporting enough video hardware to render a GUI.</p><h3>GameBoy/NES emulator (difficulty = 6/10, time = 3 weeks)</h3><p>A crude emulator for the simplest GameBoy or NES games. The GB and the NES are classics, and both have relatively simple\ninstruction sets and peripheral hardware. Additional challenges include writing competent PPU (video) and PSG (audio)\nimplementations, along with dealing with some of the more exotic cartridge formats.</p><h3>GameBoy Advance game (difficulty = 3/10, time = 2 weeks)</h3><p>A sprite-based game (top-down or side-on platform). The GBA is a beautiful little console to write code for and there’s\nan active and dedicated development community for the console. I truly believe that the GBA is one of the last game\nconsoles that can be fully and completely understood by a single developer, right down to instruction timings.</p><h3>Physics engine (difficulty = 5/10, time = 1 week)</h3><p>A 2D rigid body physics engine that implements Newtonian physics with support for rectangles, circles, etc. On the\nsimplest end, just spheres that push away from one-another is quite simple to implement. Things start to get complex\nwhen you introduce more complex shapes, angular momentum, and the like. Additional challenges include making collision\nresolution fast and scaleable, having complex interactions move toward a steady state over time, soft-body interactions,\netc.</p><h3>Dynamic interpreter (difficulty = 4/10, time = 1-2 weeks)</h3><p>A tree-walking interpreter for a JavaScript-like language with basic flow control. There’s an unbounded list of extra\nthings to add to this one, but being able to write programs in my own language still gives me child-like elation. It\nfeels like a sort of techno-genesis: once you’ve got your own language, you can start building the universe within it.</p><h3>Compiler for a C-like (difficulty = 8/10, time = 3 months)</h3><p>A compiler for a simply-typed C-like programming language with support for at least one target archtecture. Extra\nchallenges include implementing some of the most common optimisations (inlining, const folding, loop-invariant code\nmotion, etc.) and designing an intermediate representation (IR) that’s general enough to support multiple backends.</p><h3>Text editor (difficulty = 5/10, time = 2-4 weeks)</h3><p>This one has a lot of variability. At the blunt end, simply reading and writing a file can be done in a few lines of\nPython. But building something that’s closer to a daily driver gets more complex. You could choose to implement the UI\nusing a toolkit like QT or GTK, but I personally favour an editor that works in the console. Properly handling unicode,\nsyntax highlighting, cursor movement, multi-buffer support, panes/windows, tabs, search/find functionality, LSP support,\netc. can all add between a week or a month to the project. But if you persist, you might join the elite company of those\ndevelopers who use an editor of their own creation.</p><h3>Async runtime (difficulty = 6/10, time = 1 week)</h3><p>There’s a lot of language-specific variability as to what ‘async’ actually means. In Rust, at least, this means a\nlibrary that can ingest  tasks and poll them concurrently until completion. Adding support for I/O waking\nmakes for a fun challenge.</p><h3>Hash map (difficulty = 4/10, time = 3-5 days)</h3><p>Hash maps (or sets/dictionaries, as a higher-level language might call them) are a programmer’s bread &amp; butter. And yet,\nsurprisingly few of us understand how they really work under the bonnet. There are a plethora of techniques to throw\ninto the mix too: closed or open addressing, tombstones, the robin hood rule, etc. You’ll gain an appreciation for when\nand why they’re fast, and also when you should just use a vector + linear search.</p><h3>Rasteriser / texture-mapper (difficulty = 6/10, time = 2 weeks)</h3><p>Most of us have played with simple 3D graphics at some point, but how many of us truly understand how the graphics\npipeline works and, more to the point, how to fix it when it doesn’t work? Writing your own software rasteriser will\ngive you that knowledge, along with a new-found appreciation for the beauty of vector maths and half-spaces that have\napplications across many other fields. Additional complexity involves properly implementing clipping, a Z-buffer, N-gon\nrasterisation, perspective-correct texture-mapping, Phong or Gouraud shading, shadow-mapping, etc.</p><h3>SDF Rendering (difficulty = 5/10, time = 3 days)</h3><p>Signed Distance Fields are a beautifully simple way to render 3D spaces defined through mathematics, and are perfectly\nsuited to demoscene shaders. With relatively little work you can build yourself a cute little visualisation or some\nmoving shapes like the graphics demos of the 80s. You’ll also gain an appreciation for shader languages and vector\nmaths.</p><h3>Voxel engine (difficulty = 5/10, time = 2 weeks)</h3><p>I doubt there are many reading this that haven’t played Minecraft. It’s surprisingly easy to build your own toy voxel\nengine cut from a similar cloth, especially if you’ve got some knowledge of 3D graphics or game development already. The\nsimplicity of a voxel engine, combined with the near-limitless creativity that can be expressed with them, never ceases\nto fill me with joy. Additional complexity can be added by tackling textures, more complex procedural generation,\nfloodfill lighting, collisions, dynamic fluids, sending voxel data over the network, etc.</p><h3>Threaded Virtual Machine (difficulty = 6/10, time = 1 week)</h3><p>Writing interpreters is great fun. What’s more fun? . If you keep pushing interpreters as far as\nthey can go without doing architecture-specific codegen (like AOT or JIT), you’ll eventually wind up (re)discovering\n (not to be confused with multi-threading, which is a very different beast). It’s a beautiful way of\nweaving programs together out highly-optimised miniature programs, and a decent implementation can even give an AOT\ncompiler a run for its money in the performance department.</p><h3>GUI Toolkit (difficulty = 6/10, time = 2-3 weeks)</h3><p>Most of us have probably cobbled together a GUI program using tkinter, GTK, QT, or WinForms. But why not try writing\nyour GUI toolkit? Additional complexity involves implementing a competent layout engine, good text shaping (inc.\nunicode support), accessibility support, and more. Fair warning: do not encourage people to use your tool unless it’s\n - the world has enough GUIs with little-to-no accessibility or localisation support.</p><h3>Orbital Mechanics Sim (difficulty = 6/10, time = 1 week)</h3><p>A simple simulation of Newtonian gravity can be cobbled together in a fairly short time. Infamously, gravitational\nsystems with more than two bodies cannot be solved analytically, so you’ll have to get familiar with iterative\n methods. Additional complexity comes with implementing more precise and faster integration methods,\naccounting for relativistic effects, and writing a visualiser. If you’ve got the maths right, you can even try plugging\nin real numbers from NASA to predict the next high tide or full moon.</p><h3>Bitwise Challenge (difficulty = 3/10, time = 2-3 days)</h3><p>Here’s one I came up with for myself, but I think it would make for a great game jam: write a game that only persists 64\nbits of state between subsequent frames. That’s 64 bits for everything: the entire frame-for-frame game state should be\nreproducible using only 64 bits of data. It sounds simple, but it forces you to get incredibly creative with your game\nstate management. Details about the rules can be found on the GitHub page below.</p><h3>An ECS Framework (difficulty = 4/10, time = 1-2 weeks)</h3><p>For all those game devs out there: try building your own <a href=\"https://en.wikipedia.org/wiki/Entity_component_system\">ECS</a>\nframework. It’s not as hard as you might think (you might have accidentally done it already!). Extra points if you can\nbuild in safety and correctness features, as well as good integration with your programming language of choice’s type\nsystem features.</p><p>I built a custom ECS for my <a href=\"https://www.youtube.com/watch?v=nS5rj80L-pk\">Super Mario 64 on the GBA</a> project due to the\nunique performance and memory constraints of the platform, and enjoyed it a lot.</p><h3>CHIP-8 Emulator (difficulty = 3/10, time = 3-6 days)</h3><p>The <a href=\"https://en.wikipedia.org/wiki/CHIP-8\">CHIP-8</a> is a beautifully simple virtual machine from the 70s. You can write\na fully compliant emulator in a day or two, and there are an enormous plethora of fan-made games that run on it.\n<a href=\"https://github.com/zesterer/emul8/raw/refs/heads/master/test/test.ch8\">Here’s</a> a game I made for it.</p><h3>Chess engine (difficulty = 5/10, time = 2-5 days)</h3><p>Writing a chess engine is great fun. You’ll start off with every move it makes being illegal, but over time it’ll get\nsmart and smarter. Experiencing a loss to your own chess engine really is a rite of passage, and it feels magical.</p><h3>POSIX shell (difficulty = 4/10, time = 3-5 days)</h3><p>We interact with shells every day, and building one will teach you can incredible amount about POSIX - how it works, and\nhow it doesn’t. A simple one can be built in a day, but compliance with an existing shell language will take time and\nteach you more than you ever wanted to know about its quirks.</p><h2>A note on learning and LLMs</h2><p>Perhaps you’re a user of LLMs. I get it, they’re neat tools. They’re useful for certain kinds of learning. But I might\nsuggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a\nplate. If you want that sort of learning, read a book - the joy in building toy projects like this comes from an\nexploration of the unknown, without polluting one’s mind with an existing solution. If you’ve been using LLMs for a\nwhile, this cold-turkey approach might even be painful at first, but persist. There is no joy without pain.</p><p>The runner’s high doesn’t come to those that take the bus.</p>","contentLength":12598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44367084"},{"title":"PlasticList – Plastic Levels in Foods","url":"https://www.plasticlist.org/","date":1750774691,"author":"homebrewer","guid":250,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44366548"},{"title":"A new PNG spec","url":"https://www.programmax.net/articles/png-is-back/","date":1750770077,"author":"bluedel","guid":249,"unread":true,"content":"<ul><li><h3>Proper HDR support (future‐proof, too!)</h3><p>\n\t\t\t\t\t\t\t\tFigure 1 shows the colors our eyes can see.\n\t\t\t\t\t\t\t\tThe smaller, inner triangle represents the color space of most images.<p>\n\t\t\t\t\t\t\t\tThe larger, outer triangle represents the colors that are typical with a High Dynamic Range (HDR) image.\n\t\t\t\t\t\t\t</p></p><p>This new HDR support uses only 4 bytes (plus the usual PNG chunk overhead).</p></li><li><h3>Finally recognizes APNGs (animations!)</h3><p>Animated PNGs were proposed by Mozilla quite some time ago. Support was added to Firefox, but other programs hesitated to adopt them.</p><p>Today, animated PNGs are widely supported. It is time for the spec to reflect reality.</p></li><li><h3>Officially supports Exif data</h3><p>Exif stores additional information such as copyright information and even camera lens and GPS location of a photograph.</p></li><li><h3>General tidying up—fixing errata, clarifications, etc.</h3></li></ul><p>The last PNG spec was released over 20 years ago. Technology has advanced a lot since then. We're talking 3.5 years before the first iPhone.</p><p>In fact, technological advancement is what resurrected PNG. The W3C Timed Text Working Group (think:&nbsp;subtitles) needed HDR support in PNG. A proposal was made, but a few experts decided we could do better.</p><p>Momentum built, and additional parties became interested. Before we knew it, we had representation from (in alphabetical order) Adobe, Apple, BBC, Comcast / NBCUniversal, Google, MovieLabs, and of course W3C, who maintains the spec. It's quite the dream team.</p><p>With these titans behind it, the image format is back with full momentum. Work has already begun on the  PNG spec updates.</p><p>Many of the programs you use already support the new PNG spec: Chrome, Safari, Firefox, iOS/macOS, Photoshop, DaVinci Resolve, Avid Media Composer...</p><p>Plus, you saw some broadcast companies in that list above. Behind the scenes, hardware and tooling are being updated to support the new PNG spec. The next time you see a news ticker scrolling  or the score banner update as your team pulls out a clutch play, check if it is HDR.</p><p>I know you all immediately wondered, . We're already working on that. And parallel encoding/decoding, too! Just like this update, we want to make sure we do it right.</p><p>We expect the next PNG update (Fourth Edition) to be short. It will improve HDR &amp; Standard Dynamic Range (SDR) interoperability. While we work on that, we'll be researching compression updates for PNG Fifth Edition.</p>","contentLength":2363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44365754"},{"title":"Starship: A minimal, fast, and customizable prompt for any shell","url":"https://starship.rs/","date":1750763512,"author":"benoitg","guid":248,"unread":true,"content":"<p>Add the init script to your shell's config file:</p><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of <code>~/.config/fish/config.fish</code>:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of <code>Microsoft.PowerShell_profile.ps1</code>. You can check the location of this file by querying the  variable in PowerShell. Typically the path is <code>~\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1</code> or <code>~/.config/powershell/Microsoft.PowerShell_profile.ps1</code> on -Nix.</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><div><p>Only elvish v0.18 or higher is supported.</p></div><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><div><p>This will change in the future. Only Nushell v0.96+ is supported.</p></div><p>Add the following to the end of your Nushell configuration (find it by running  in Nushell):</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Add the following to the end of :</p><div><pre tabindex=\"0\"><code></code></pre></div><p>You need to use <a href=\"https://chrisant996.github.io/clink/clink.html\" target=\"_blank\" rel=\"noreferrer\">Clink</a> (v1.2.30+) with Cmd. Add the following to a file  and place this file in Clink scripts directory:</p><div><pre tabindex=\"0\"><code></code></pre></div>","contentLength":912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44364874"},{"title":"uv: An extremely fast Python package and project manager, written in Rust","url":"https://github.com/astral-sh/uv","date":1750696045,"author":"chirau","guid":247,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44357411"},{"title":"Vera C. Rubin Observatory first images","url":"https://rubinobservatory.org/news/rubin-first-look/cosmic-treasure-chest","date":1750693260,"author":"phsilva","guid":246,"unread":true,"content":"<p dir=\"ltr\">Introducing the first riches from NSF–DOE Vera C. Rubin Observatory’s cosmic treasure chest, a wealth of data that will help scientists make countless new discoveries about our Universe. This image, one of the first released by Rubin Observatory, exposes a Universe teeming with stars and galaxies — transforming seemingly empty, inky-black pockets of space into glittering tapestries for the first time. Only Rubin can quickly produce such large images with this much color and richness.</p><p dir=\"ltr\">Here, Rubin’s view is focused on the southern region of the , about 55 million light-years away from Earth and the nearest large collection of galaxies to our own Milky Way. The image offers a stunning variety of objects — from bright stars ranging from blue to red in color, to nearby blue spiral galaxies, to distant red galaxy groups — demonstrating the broad range of science made possible by Rubin data.</p><p dir=\"ltr\">During the 10-year Legacy Survey of Space and Time, scientists around the world will access Rubin’s treasure trove of data to address questions like: How did the Milky Way form? What makes up the 95% of the Universe we can’t see? What will a detailed inventory of Solar System objects reveal? What will we learn from watching hundreds of millions of changes in the night sky over 10 years?</p>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44356890"},{"title":"How I use my terminal","url":"https://jyn.dev/how-i-use-my-terminal/","date":1750691669,"author":"todsacerdoti","guid":245,"unread":true,"content":"<p>this is a whole blog post because it is \"outside the overton window\"; it usually takes at least a video before people even understand the thing i am trying to describe. so, here's the video:</p><p>the steps here that tend to surprise people are \n, \n, and \n. when i say \"surprise\" i don't just mean that people are surprised that i've set this up, but they are surprised this is possible at all.</p><p>here's what happens in that video:</p><ol><li>\n I start with Windows Terminal open on my laptop.</li><li>\n I hit , which opens a new terminal tab which 's to my home desktop and immediately launches tmux.</li><li>\n tmux launches my default shell, . zsh shows a prompt, while loading the full config asynchronously</li><li>\n i use  to fuzzy find a recent directory</li><li>\n i start typing a ripgrep command. zsh autofills the command since i've typed it before and i accept it with .</li><li>\n i hit , which tells tmux to search all output in the scrollback for filenames. the filenames are highlighted in blue.</li><li>\n i hold  to navigate through the files. there are a lot of them, so it takes me a bit to find the one i'm looking for.</li><li>\n i press  to open the selected file in my default application (). tmux launches it in a new pane. note that this is still running ; it is opening a remote file in a remote tmux pane. i do not need to have this codebase cloned locally on my laptop.</li><li>\n i try to navigate to several references using rust-analyzer, which fails because RA doesn't understand the macros in this file. at \n i finally find one which works and navigate to it.</li><li>\n i hit , which tells tmux to switch focus back to the left pane.</li><li>\n i hit  again. the pane is still in \"copy-mode\", so all the files from before are still the focus of the search. they are highlighted again and tmux selects the next file in search order.</li><li>\n i hit , which opens a different file than before, but in the  instance of .</li><li>\n i hit , which shows my open file buffers. in particular, this shows that the earlier file is still open. i switch back and forth between the two files a couple times before ending the stream.</li></ol><p>i got annoyed at VSCode a while back for being laggy, especially when the vim plugin was running, and at having lots of keybind conflicts between the editor, vim plugin, terminal, and window management. i tried zed but at the time it was quite immature (and still had the problem of lots of keybind conflicts).</p><p>i switched to using nvim in the terminal, but quickly got annoyed at how much time i spent copy-pasting filenames into the editor; in particular i would often copy-paste files with columns from ripgrep, get a syntax error, and then have to edit them before actually opening the file. this was quite annoying. what i wanted was an equivalent of ctrl-click in vscode, where i could take an arbitrary file path and have it open as smoothly as i could navigate to it. so, i started using tmux and built it myself.</p><p>people sometimes ask me why i use tmux. this is why! this is the whole reason! (well, this and session persistence.) terminals are stupidly powerful and most of them expose almost none of it to you as the user. i like tmux, despite its age, bugs, and antiquated syntax, because it's very extensible in this way.</p><p>this is done purely with tmux config:</p><pre data-lang=\"tmux\"><code data-lang=\"tmux\"></code></pre><p>and this is the contents of :</p><pre data-lang=\"sh\"><code data-lang=\"sh\"></code></pre><p>i will not go through the whole regex, but uh. there you go. i spent more time on this than i probably should have.</p><p>this is actually a trick; there are many steps here.</p><p>this part is not so bad. tmux again.</p><pre data-lang=\"tmux\"><code data-lang=\"tmux\"></code></pre><p>i also have a version that always opens an editor in the current pane, instead of launching in the default application. for example i use <a href=\"https://fx.wtf/\"></a> by default to view json files, but  to edit them.</p><pre data-lang=\"tmux\"><code data-lang=\"tmux\"></code></pre><p>here is the trick. i have created <a href=\"https://github.com/jyn514/dotfiles/blob/master/bin/hx-hax\">a shell script</a> (actually a perl script) that is the default application for all text files.</p><div><div><p>setting up that many file associations by hand is a pain. i will write a separate blog post about the scripts that install my dotfiles onto a system. i don't use Nix partly because all my friends who use Nix have  bugs than they already had, and partly because i don't like the philosophy of not being able to install things at runtime. i want to install things at runtime and  that i did so. that's a separate post too.</p></div></div><p>the relevant part is this:</p><pre data-lang=\"perl\"><code data-lang=\"perl\"></code></pre><p>this bounces  to tmux. in particular, this is being very dumb and assuming that tmux is running on the machine where the file is, which happens to be the case here. this is not too bad to ensure - i just use a separate terminal  tab for each instance of tmux i care about; for example i will often have open one Windows Terminal tab for WSL on my local laptop, one for my desktop, and one for a remote work machine via a VPN.</p><div><div><p>there's actually even more going on here—for example i am translating the  syntax to something vim understands, and overriding  so that it doesn't error out on the —but for the most part it's straightforward and not that interesting.</p></div></div><p>this is a perl script that scripts tmux to send keys to a running instance of nvim (actually the same perl script as before, so that both of these can be bound to the same keybind regardless of whether nvim is already open or not):</p><pre data-lang=\"perl\"><code data-lang=\"perl\"></code></pre><ul><li>i don't need a fancy terminal locally; something with nice fonts is enough. all the fancy things are done through tmux, which is good because it means they work on Windows too without needing to install a separate terminal.</li><li>the editor thing works even if the editor doesn't support remote scripting. nvim  support RPC, but this setup also worked back when i used  and .</li><li>i  have written this such that the fancy terminal emulator scripts were in my editor, not in tmux (e.g.  in nvim). but again this locks me into the editor; and the built-in terminals in editors are usually not very good.</li></ul><p>well. well. now that you mention it. the last thing keeping me on tmux was session persistence and <a href=\"https://ansuz.sooke.bc.ca/entry/389\">Ansuz has just released a standalone tool that does persistence and nothing else</a>. so. i plan to switch to <a href=\"https://sw.kovidgoyal.net/kitty/\">kitty</a> in the near future, which lets me keep all these scripts and does not require shoving a whole second terminal emulator inside my terminal emulator, which hopefully will reduce the number of weird mysterious bugs i encounter on a regular basis.</p><p>the reason i picked kitty over <a href=\"https://wezterm.org/\">wezterm</a> is that ssh integration works by integrating with the shell, not by launching a server process, so it doesn't need to be installed on the remote. this mattered less for tmux because tmux is everywhere, but hardly anywhere has wezterm installed by default.</p><p>honestly, yeah. i spend quite a lot less time fighting my editor these days.</p><ul><li>it's  easier to debug when something goes wrong (vscode's debugging tools are mostly for plugin extension authors and running them is non-trivial). with vim plugins i can just add  statements to the lua source and see what's happening.</li><li>all my keybinds make sense to me!</li><li>my terminal is much easier to script through tmux than through writing a VSCode plugin, which usually involves setting up a whole typescript toolchain and context-switching into a new project</li></ul><p>that said, i cannot in good conscience recommend this to anyone else. all my scripts are fragile and will probably break if you look at them wrong, which is not ideal if you haven't written them yourself and don't know where to start debugging them.</p><p>if you do want something similar without writing your own tools, i can recommend:</p><ul><li><a href=\"https://fishshell.com/\">fish</a> + <a href=\"https://github.com/ajeetdsouza/zoxide/\">zoxide</a> + <a href=\"https://github.com/junegunn/fzf\">fzf</a>. that gets you steps 4, 5, and kinda sorta-ish 6.</li><li>\"builtin functionality in your editor\" - fuzzy find, full text search, tabs and windows, and \"open recent file\" are all commonly supported.</li><li><a href=\"https://git.causal.agency/src/tree/bin/qf.c\">qf</a>, which gets you the \"select files in terminal output\" part of 6, kinda. you have to remember to pipe your output to it though, so it doesn't work after the fact and it doesn't work if your tool is interactive. note that it hard-codes a vi-like CLI (), so you may need to fork it or still add a script that takes the place of $EDITOR. see <a href=\"https://jvns.ca/blog/2025/06/10/how-to-compile-a-c-program/\">julia evans' most recent post</a> for more info.</li><li><a href=\"https://github.com/kilobyte/e\">e</a>, which gets you the \"translate  into something your editor recognizes\" part of 8, kinda. i had never heard of this tool until i wrote my own with literally the exactly the same name that did literally exactly the same thing, forgot to put it in PATH, and got a suggestion from  asking if i wanted to install it, lol.</li><li> or  or , all of which get you 12, kinda. the problem with this is that they don't all support , and it means you have to modify this whenever you switch editors. admittedly most people don't switch editors that often, lol.</li></ul><ul><li>terminals are a lot more powerful than people think! by using terminals that let you script them, you can do quite a lot of things.</li><li>you can kinda sorta replicate most of these features without scripting your terminal, as long as you don't mind tying yourself to an editor.</li><li>doing this requires quite a lot of work, because no one who builds these tools thought of these features ahead of time.</li></ul><p>hopefully this was interesting! i am always curious what tools people use and how - feel free to <a href=\"https://jyn.dev/cdn-cgi/l/email-protection#b3d1dfdcd4f3d9cadd9dd7d6c5\">email me</a> about your own setup :)</p>","contentLength":8921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44356646"},{"title":"Backyard Coffee and Jazz in Kyoto","url":"https://thedeletedscenes.substack.com/p/backyard-coffee-and-jazz-in-kyoto","date":1750689425,"author":"wyclif","guid":244,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44356248"},{"title":"I wrote my PhD Thesis in Typst","url":"https://fransskarman.com/phd_thesis_in_typst.html","date":1750626721,"author":"todsacerdoti","guid":243,"unread":true,"content":"<h2>I wrote my PhD Thesis in Typst</h2><p>I recently submitted my <a href=\"https://doi.org/10.3384/9789181181777\">PhD thesis</a>, and while waiting for the physical copies to get printed I thought I'd write about something you (hopefully) wouldn't notice when reading it. I wrote it in Typst, not LaTeX. In this post I will talk a bit about what went well and what didn't.</p><p>Typst (<a href=\"https://typst.app/\">https://typst.app/</a>) is a modern take on a typesetting language that I think has a real shot at dethroning LaTeX. I would describe the language as a mix of markdown and dynamically typed Rust, which may sound weird but is a really nice fit. Day-to-day document writing being markdown-like is very comfortable (certainly much nicer than littering your writing with ). The scripting language is powerful, well thought out, and makes it very easy to jump between code and typesetting. For example</p><p>This is . The sum of [1, 2, 5, 8] is </p><p>In the rest of this post, I will talk about some of the things I liked and disliked about Typst while writing the thesis.</p><p>The thing that pushed me over the edge to try Typst for my thesis was a friend telling me his LaTeX thesis took 90 seconds to compile towards the end. I am far too easily distracted to tolerate 90 second compile times when I'm making small changes. The Typst compiler is , on small-medium sized documents it compiles fast enough to do live writing in the PDF preview. As my thesis grew beyond 150 pages, compile times dropped a bit. Clean builds take about 15 seconds, but it also does incremental compilation so for  changes, it is still nearly instant, for local layout changes it takes a second or two. Even for large global template changes, &nbsp;10 seconds is a whole lot better than 90 seconds, and I am confident that being able to iterate on layout and style 9 times faster produced a nicer looking result in the end.</p><p>The Typst language is amazingly well thought out. Markdown syntax is nicer than Tex syntax for general writing, but what really shines is the scripting language. My biggest complaint with LaTeX as a language is that  is consistent. Every package defines their own little utilities for even basic things like -statements. It feels like you don't learn LaTeX, you learn each package individually. Typst on the other hand is a well-designed dynamically typed language. Being Rust inspired and me being a Rust user meant that it barely feels like I had to learn the language at all, I can just assume that I'm writing Rust. That's not true as you get into more advanced things of course, but it feels like I  Typst in a way that I don't think is possible with LaTeX.</p><p>This isn't just philosophical, being able to script Typst is extremely useful. For example, a central part of my thesis is that there are a lot of hardware description languages out there, and I wanted to come up with a way to categorize them. Over a few years, I have collected them all with some metadata in a toml file (<a href=\"https://gitlab.com/TheZoq2/list_of_hdls/\">https://gitlab.com/TheZoq2/list_of_hdls/</a>). Since Typst can parse toml, I could simply use that data to generate a figure to put the languages into my taxonomy:</p><p>Sure, it would be possible to do this in many other ways, but having it self-contained in the document is nice, and at this point Typst is one of my favourite scripting languages, so using it here is a no-brainer and something I did for pretty much all my data processing.</p><p>A modern language of course also comes with modern tools, a compiler with dependency management, a working language server protocol integration for editors etc. All that makes it even nicer to work with.</p><p>A few times I have tried to understand latex templates, and every time it has ended in frustration. I believe it took me longer to remove a header from our university presentation template than it did for me to write a new presentation template in Typst. Again, this comes down to the language being well-thought-out and consistent. This was also hugely advantageous when tweaking my thesis template the way I want it. Naturally, this is something of a double-edged sword, had I used LaTeX I would not have had to make a template in the first place, but at the same time, modifying anything in that template would have been much more difficult. This way I ended up with everything right where I wanted it without the frustration of figuring out how to get the results I wanted.</p><p>My thesis has a lot of code in it, and I  reading code without good syntax highlighting. LaTeX's syntax highlighting systems leave a lot to be desired, but Typst's certainly does not. It does have built in support for textmate grammars, the same system used by sublime text for syntax highlighting, but for various reasons I decided not to use them. Instead, I just used Typst's  rules with regex. For example,</p><p>will syntax highlight Spade conditionals. My syntax highlighting definitions¹ are… more more complicated but produce results that I am very happy with. At some point, I even ended up parsing a subset of Spade in Typst with a recursive descent parser because I really wanted to highlight named arguments. Was that a good way to spend my time? Debatable, but it certainly was not something I would have dreamt of doing in LaTeX.</p><p>LaTeX error messages are awful. Typst is  much better with errors that  to the right place, gives accurate information about what is wrong, and doesn't fill the terminal with garbage. No more \"Missing $ inserted!\".</p><p>Typst's bibliography management leaves some things to be desired when writing a whole thesis. The first roadblock I encountered was that you can only have one bibliography section and file per document. One per document is a deal breaker because I need one for the introduction, and one per included paper. My master  file makes heavy use of Bibtex variables which are included from other sources, this does not work in Typst. I ended up solving it with a Makefile that merges my files into one before going to Typst, but that's not ideal ²</p><p>The multi-bibliography problem was worse, but luckily <a href=\"https://typst.app/universe/package/alexandria\">https://typst.app/universe/package/alexandria</a> came along as I was working to save the day. It requires a little bit of work per bibliography so I would absolutely have preferred an automated system where I just do  and would insert all the citations I've used since last time, but Alexandria did the job.</p><p>Then there is the  of the bibliography… At first glance, it looks fine but when you have an advisor that is very picky about bibliography styles, things start to go wrong. For example, paper titles should be written in sentence case with quotes around them, for example:</p><p>[1] F. Skarman and O. Gustafsson, “Spade: An expression-based HDL with pipelines,” in <em>Proc. Workshop Open-Source Des. Automat.</em>, Apr. 2023.</p><p>But in the Bibtex file it is very common for the titles to appear in their original title case form (Spade: An Expression-Based HDL With Pipelines). By default, Typst does not seem to do this conversion, but it can be turned on with a custom  file. However, then it will not do it quite right. \"Expression-Based\" turns into \"expression-Based\" for example. In the end, I did the conversion to sentence case manually.</p><p>The system is also kind of strange about what information it includes in references. For example, the  class in Bibtex has an  field. This does not get included, but changing it to  (which AFAIK is not a standard field in Bibtex) makes it work.</p><p>I also found out that book titles should not be written with quotes, but with italics. I set a rule to match on  in my CSL field but it did not have an effect on , only on . In general, the bibliography management is death by a thousand papercuts, and a lot of them are caused by a very opaque translation from Bibtex fields to CSL…</p><p>Wait, didn't I say that the error messages are good before? I did,but they are only good in simple cases. For example, with the Alexandria library, a bibliography that fails to compile only says \"failed to parse bibliography\" and does not give further errors. Errors in stateful show rules also do not give any breadcrumbs back to where the error was triggered which makes them hard to track down. In general, I would like to see a longer traceback for errors to avoid having feel like I'm writing latex again by binary searching recent changes.</p><p>My thesis also has the dreaded <code>Layout did not converge in 5 attempts</code> warning which is pretty much impossible to debug currently. In my case, things turn out fine, but it is a bit sketchy. I believe this is being worked on.</p><p>LaTeX is everywhere. My early papers were of course written in latex and I had to convert them to Typst. This was surprisingly easy with Pandoc which did 95% of the work with me just having to do a bit of final cleanup. Unfortunately, I also wrote a few new papers for the thesis that will be submitted later, and that's a more complicated situation since journals demand LaTeX code. My workflow so far has been to write the initial version of papers in Typst, submit the PDF as generated by Typst, then for the final version submit a converted version.</p><p>For conversion, I don't believe pandoc works (or doesn't work the way I want it) so I had wrote my own tool ³. It lets me do 90% of the content and layout in Typst, use Typst for generating figures and code snippets, and then I can insert style tweaks with inline LaTeX. The Typst compiler being open source was of huge help here as I just hook into that. I will talk more about that tool in a future post once one of my papers compiled using it is made public</p><p>A big downside of this is that it generates latex with  for things like inline code. I am hoping publishers will not count those as figures and be weirded out by them…</p><p>Everyone also knows LaTeX, so if I want to use Typst in collaboration, I force everyone I want to work with to learn it. My advisor was not super pleased with that 😇</p><p>The Typst ecosystem is young. Being able to whip up a thesis template and doing all the tweaks to make it  is great. On the other hand, since I am probably the first person at my university to do it meant I  to whip up a template. For submitting to conferences and journals, there are Typst templates, but they are not flawless. The IEEE template only has conference variant, not a journal variant. The LLNCS template I found on git looked OK but had the wrong margins, so our paper ended up being slightly longer when converted to latex.</p><p>With all this in mind, would I recommend using Typst for a PhD Thesis? If you are like me, absolutely probably. I like playing with programming languages, I'm easily frustrated by tools that bother me. I also much prefer a tool that I can tweak to get exactly the way I want over a tool that \"just works\" out of the box but where it is hard to tweak. In the end, I don't think I compromised on the quality of the document, the quick iteration probably made things look better than they would have otherwise. It certainly took some effort to work around a few issues, and some extra work setting up a template, but that was  work compared to having to suffer through LaTeX's annoyances.</p><p>If you're not like me and want a system that works out of the box, Typst today probably isn't the right choice for a PhD thesis. But I would still recommend playing with Typst for smaller things but leave the big documents to LaTeX, for now.</p><p><em>Updated June 23 2025: Changed absolutely to probably in the conclusion and added the 'advisors perspective section below'</em></p><p>My advisor had a somewhat different perspective on this and I think it is worth posting that here. This is essentially taken verbatim from a LinkedIn comment where we discussed it:</p><p>\"The thing is that you HAD to tweak it to make things look the way they should. Not sure that is a benefit.</p><p>Would I, as a supervisor, recommend someone to use Typst if they are in a field where all manuscripts are written in LaTeX? No. The fact that Frans is clever and a good programmer helped for sure. But think twice. Thrice if you are not a good programmer. And ask Frans for the scripts he developed to move things between Typst and LaTeX.</p><p>It is simply not mature enough for most people to be used at this stage when you have to produce documents looking a particular way. Unless you are willing to spend much time on tweaking things etc. It may be mature for sure and someone will have to contribute to make it more mature for sure.</p><p>\"Good\" thing from a supervisor perspective is that I barely edited any text in the source at all, but had to let Frans edit the text (and format). Quite inefficient though.</p><p>Despite the disagreement on the procedure, I cannot stress enough that the end result was great though! Both technically and layoutwise.\"</p><p>He certainly has a point here, especially needing to be interested in working around issues and needing some programming experience to get things working perfectly. The problem of having to spend time adjusting things is in my opinion hard to quantify since it trades easy fun upfront work for frustration over other aspects of the language.</p>","contentLength":12912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44350322"},{"title":"Klein Bottle Amazon Brand Hijacking (2021)","url":"https://www.kleinbottle.com/Amazon_Brand_Hijacking.html","date":1750619921,"author":"sebg","guid":242,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44349525"},{"title":"Mechanical Watch: Exploded View","url":"https://fellerts.no/projects/epoch.html","date":1750604204,"author":"fellerts","guid":241,"unread":true,"content":"<div><p>\n        In May 2022, someone posted to Hacker News <a href=\"https://ciechanow.ski/mechanical-watch/\">Bartosz\n          Ciechanowski's\n          blog post</a>\n        explaining how mechanical watch movements work. Since then, his blog has\n        been my absolute favorite corner of the Internet. His posts are not just\n        well written and easy to follow, the accompanying interactive\n        illustrations are magnificent.\n        <p>\n        The first illustration in his blog post about mechanical watch movements\n        allows you to \"explode\" a ticking mechanical movement and rotate it to\n        inspect its every component from any angle. I owe my foray into the\n        hobby of watchmaking to Bartosz, but that's not what this blog post is\n        about. Instead, I want to scratch an itch I've had for years now: How\n        cool would it be to hold an exploded view of a </p> mechanical\n        watch in your hand?\n        </p></div><div><p>\n        I figured that surely, someone has had this idea and built it before. On\n        eBay you'll find cubes of resin embedding various random components from\n        mechanical watches, but they are typically sold as \"steampunk art\" and\n        bear little resemblance to the proper assembly of a mechanical watch\n        movement. Sometimes, you'll find resin castings showing every component\n        of a movement spread out in a plane like a buffet---very cool, but not\n        what I'm looking for. Despite my best efforts, I haven't found anyone\n        who makes what I'm after, and I have a sneaking suspicion as to why that\n        is.\n      </p><div><a href=\"https://fellerts.no/img/epoch/watch-buffet.jpg\"><img src=\"https://fellerts.no/img/epoch/watch-buffet.jpg\"></a><p>\n          Every component making up the movement of a beautiful 1960's Longines\n          Flagship wristwatch I worked on recently, laid out \"buffet style\".\n        </p></div><p>\n        Building an exploded view of a mechanical watch movement is undoubtedly\n        very fiddly work and requires working knowledge about how a mechanical\n        watch is assembled. People with that skillset are called watchmakers.\n        , not \"destroyer for the sake of art\". I guess it falls to\n        me, then, to give this project an honest shot.\n      </p></div><div><p>\n        Here comes my favorite part: jumping head-first into a project requiring\n        a set of skills I don't even know exist, let alone possess, following a\n        process that develops as the project evolves. So how would one go\n        about building a real-life exploded view of a mechanical watch movement?\n        <p>\n        The first thing to consider is what type of mechanical watch movement to\n        \"explode\". Although he doesn't explicitly say, I think Bartosz\n        Ciechanowski's blog post is based on the </p><a href=\"https://calibercorner.com/eta-caliber-2824-2/\">ETA caliber\n          2824-2</a> mechanical watch movement (or one of its many clones, such as\n        the Chinese PT5000). This is a very robust and popular automatic\n        (self-winding) mechanical wristwatch movement found in many watches.\n        It's considered one of the all-time \"workhorse\" movements of the\n        industry. I highly encourage the curious reader to check out <a href=\"https://www.youtube.com/watch?v=Utzdcc-XJ5g\">this YouTube video</a>\n        of a very skilled and equally witty watchmaker servicing a Hamilton\n        watch featuring this exact movement. The video will also reveal how many\n        absolutely miniscule components make up the ETA-2824, making it less\n        than ideal for my initial prototyping. A larger and simpler movement\n        would be much better.\n        <p>\n        Luckily, in the late 1800's to early 1900's, many people carried a\n        pocketwatch about their person. However, as smaller wristwatches gained\n        popularity among men, beginning around the first world war with trench\n        watches, pocket watches lost their value as timepieces. Many were melted\n        down to reclaim the gold used to case the movement. Today, you can find\n        gorgeous, hand-engraved movements from the turn of the 20th century on\n        </p><a href=\"https://www.ebay.com/sch/i.html?_nkw=vintage+pocket+watch+movement\">eBay</a>\n        for next to nothing.\n        <p>\n        Pocketwatches are the simplest mechanical watch movements out there:\n        they typically don't have any date complications, aren't self-winding,\n        and usually don't even have a central seconds hand---that's a\n        relatively modern feature. There's a reason budding watchmakers start\n        out practicing on pocketwatch movements.\n        </p><p>\n        Now for the hard part: How do you suspend 50-100 tiny components to form\n        an exploded view of the assembly?\n      </p></p></div><div><a href=\"https://fellerts.no/img/epoch/pocketwatch.jpg\"><img src=\"https://fellerts.no/img/epoch/pocketwatch.jpg\"></a><p>\n        A beautiful early 1900's pocketwatch movement from eBay. $20 for a piece\n        of horological art.\n      </p></div><div><p>\n        The model I'm planning to build must stand up to being handled, which\n        means it must be solid. In turn, that probably means that I need to cast\n        my components in clear epoxy resin. My first idea was to build the\n        exploded model up, layer by layer, letting the resin cure between each\n        layer. To save time, I experimented with clear resin that cures when\n        exposed to UV light. There are several reasons why this did not work:\n      </p><ol><li><p>The resin I purchased ended up with a heavy yellow tint after\n              curing.\n            </p></li><li><p>My puny UV flashlight took forever to cure a pour even 1 mm\n              deep.</p></li><li><p>The seams between layers are very visible.</p></li></ol><p>\n        The first two problems can probably be solved by throwing money at the\n        problem, but the last issue seems to be a problem with varying indices\n        of refraction throughout my casting. The resin art community is\n        well-aware of this problem, and the suggested solution is to pour the\n        next layer of resin before the previous layer has fully cured. This\n        supposedly helps the two layers fuse, and should make the transition\n        between layers much less pronounced. Unfortunately, UV resin seems to\n        cure from the outside in, meaning I needed to use \"regular\" two-part\n        epoxy instead.\n        <p>\n        To achieve the effect I want, I need around 20 layers for a regular\n        pocket watch movement. If I were to cast each layer in a transparent\n        container, adding components and epoxy as the previous layer was\n        half-cured, I would be doing nothing else for a solid week. Instead, I\n        figured my best bet was to cast all 20-or-so layers at once, and stack\n        them together once the epoxy was semi-cured. So, my next project was to\n        order some casting silicone and cast myself a mold suited to casting\n        thin disks of clear epoxy. A baking tray studded with poker chips formed\n        the ideal mold for casting the silicone.\n        </p><p>\n        My resin disks solved problems 1 and 2 above, but problem 3 is still\n        unsolved. In addition, this was a very messy and challenging way to\n        cast resin: the half-cured disks are floppy, resin runs everywhere and\n        bubbles get stuck between the layers. I bought a small vacuum chamber to\n        combat the bubble issue, but escaping gas weaked havoc on the disks of\n        resin. I learned enough about resin castings to finally realize that\n        layered casting is not the way to go.\n      </p></p></div><div><a href=\"https://fellerts.no/img/epoch/uv-resin.jpg\"><img src=\"https://fellerts.no/img/epoch/uv-resin.jpg\"></a><p>\n        1st attempt: embedding some nails in layers of UV resin.\n      </p><a href=\"https://fellerts.no/img/epoch/silicone-mold.jpg\"><img src=\"https://fellerts.no/img/epoch/silicone-mold.jpg\"></a><p>\n        2nd attempt: Embedding more nails in disks of clear resin in a silicone\n        mold...\n      </p><a href=\"https://fellerts.no/img/epoch/silicone-mold-result.jpg\"><img src=\"https://fellerts.no/img/epoch/silicone-mold-result.jpg\"></a></div><div><p>\n        It took a while to accept that the only hope I had was to cast the\n        entire model at once. This is difficult because I need to somehow\n        suspend each tiny component in a way that is robust enough for me to\n        pour resin all over it and then pull out any air bubbles.\n        <p>\n        Scarred and burnt by resin's index of refraction, I went looking for\n        very thin rods of plastic or acrylic that I could cut to length and glue\n        my components to. Fishing line fits the bill. Specifically, monofilament\n        nylon leader used in fly fishing. It comes in many thicknesses, has an\n        index of refraction very similar to epoxy resin, and is cheap. The only\n        major challenge with using fishing line is that it \"remembers\" its\n        spooled shape, so every segment I cut off has a slight bend to it. I was\n        able to remedy this somewhat by stringing it repeatedly across the grill\n        pan in my oven and baking it at 150°C (300°F) for an hour or so. When done,\n        the middle segments are reasonably straight and significantly stiffer\n        than what I started with.\n        </p><p>\n        The assembly process actually resembles \"proper\" watchmaking a lot. Of\n        course, tweezer control and steady hands are important. But applying\n        tiny drops of CA glue to components with a pinhead is a lot like\n        applying oil to bearing surfaces and jewels. I just find it amusing to\n        do it with glue instead of oil---CA glue being the </p><i>complete\n          opposite</i> of a lubricant.\n        <p>\n        Armed with a pair of helping hands and a set of self-closing tweezers, I\n        found the process of building up the exploded model of a scrap watch\n        movement to be very satisfying.\n      </p></p></div><div><a href=\"https://fellerts.no/img/epoch/fishing-line-screws.jpg\"><img src=\"https://fellerts.no/img/epoch/fishing-line-screws.jpg\"></a><p>\n        0.7 mm monofilament fishing line attached to bridge screws.\n      </p><a href=\"https://fellerts.no/img/epoch/crown-wheel-assy.jpg\"><img src=\"https://fellerts.no/img/epoch/crown-wheel-assy.jpg\"></a><p>\n        Monofilament fishing line maintains the distance betwen components. A\n        jig holds everything still while the CA glue hardens.\n      </p></div><div><p>\n        At this point I've tried a number of different epoxy resins claiming to\n        cure crystal clear. As far as I can tell, they all turn out appreciably\n        clear. Their main differences are viscosity, cure time and how much air\n        they trap when mixing. Some resins claim to expel bubbles when curing.\n        They manage this to some extent, but to get completely clear castings\n        you either need a vacuum chamber to pull out all the air from your cast,\n        or a pressure chamber to completely squash any pockets of air. I went\n        for a vacuum pump because it's a fun thing to have around (boiling water\n        at room temperature doesn't get old). Another benefit of the vacuum pump\n        is that you don't need to leave your cast in the chamber for the entire\n        cure time. My castings have a lot of voids that love to trap air, and\n        the vacuum chamber does a good job of pulling out the trapped air.\n        <p>\n        My casting method goes like this: Mix enough of part A and B of your\n        resin to fill the mold plus 10–15 %. Mix thoroughly for 3 minutes. Pour\n        the resin into another mixing container to ensure that no unmixed resin\n        is stuck to the walls of your container. Use a fresh stirring stick, and\n        mix for another 3 minutes. Pull the whole container under vacuum (I\n        manage around -0.96 bar) and leave it there for 30 minutes. Depending on\n        your resin, the froth may very well overflow your mixing container:\n        cycle between pulling a vacuum and letting air back in a couple times to\n        pop most of the bubbles.\n        </p><p>\n        Now pour the resin into the mold and go through the vacuum process\n        again. This time, most of the bubbles you're pulling out come from\n        around the embedment, not from air trapped in the resin, which helps\n        keep the frothing down.\n      </p></p></div><div><a href=\"https://fellerts.no/img/epoch/froth.jpg\"><img src=\"https://fellerts.no/img/epoch/froth.jpg\"></a><p>\n        Some resins froth up to several times the liquid volume under vacuum!\n        Make sure there's enough room to accomodate this.\n      </p></div><div><p>\n        My first somewhat promising cast was in a borosilicate class cylinder. I\n        won't dwell on the looks of this, because I mostly wanted to experiment\n        and answer a few burning questions with this prototype:\n      </p><dl><dd> Not very! It's visible in the right light, but I don't\n          think I'm able to do any better.\n        </dd><dd> Most resin datasheets say that CA glue can interfere with\n          the resin's curing process, but I can't tell from this test.\n        </dd><dd> Definitely not. Light refracts on the curved cylinder\n          surface making it difficult to understand the geometry inside,\n          defeating the purpose of the model. Cast in a cube.\n        </dd></dl>\n      A week or so later, the cylinder shattered. Resin shrinks as it cures. On\n      to the next prototype!\n      </div><div><a href=\"https://fellerts.no/img/epoch/first-cast.jpg\"><img src=\"https://fellerts.no/img/epoch/first-cast.jpg\"></a><p>\n        First prototype cast. Fishing line all but disappears in the resin.\n      </p></div><div><p>\n        At this point I have settled on a somewhat structured process.\n\n      </p>\n      Start with a clean, disassembled movement. For instructions on how to\n      properly disassemble and clean a watch movement, I highly recomment <a href=\"https://www.watchfix.com/\">Mark\n        Lovick's Watch Repair Course</a>.\n      Assemble the train of wheels with their bridges and seize them by\n      depositing small amounts of CA glue on the pinions. Just like with the\n      watchmaker's oil, a sewing needle with a flat spot is perfect for\n      picking up a small droplet of CA glue and depositing it where it's\n      needed. Capillary action wicks it between the components. Less is more\n      when it comes to CA glue: the bond between surfaces is stronger when no\n      excess is used, and the cure time is around 30 seconds even without the\n      use of an activator.\n      <p>\n      I made a small jig for cutting pieces of fishing line accurately and\n      squarely out of a janky hand press tool.\n      </p>\n      Work starts on the train wheel side of the movement (often called the\n      watchmaker's side) because that's where most of the complexity lies.\n      While working here, the movement can rest in a movement holder. I attach\n      long strands of fishing line to the end of each of the bridge screws.\n      Self-closing tweezers hold the bridges in place above where they seat on\n      the mainplate, and the screw/fishing line combo is threaded through the\n      screw hole in the bridge into the corresponding holes in the mainplate.\n      More glue binds the bridges to the fishing line.\n      <p>\n      Subassemblies such as of the keyless works, motion works, balance\n      assembly, etc. can be constructed separately and fastened as a unit.\n      Small, flat components such as cover plates, intermediate wheels and cap\n      jewels each get their own short studs of fishing line. I dip the studs\n      in CA glue and place it near the center of mass of the component I'm\n      working on: the fishing line's flat ends allows it to stand upright.\n      </p><h3>Flip and assemble the dial side</h3>\n      Before starting work on the dial side of the movement, it must be flipped\n      over. I transfer the assembly into the jaws of some tweezers and apply\n      closing force with a rubber band. The tweezers are clamped by \"helping\n      hands\" glued to a piece of cardboard which allows me to spin my work\n      around. Work continues similarly on the dial side. Finally, some thin\n      transparent nylon sewing thread attaches the mainplate to a pegwood stick\n      that allows me to suspend the whole assembly over a mold, ready for\n      casting resin.\n      <h3>Make the mold, prepare and pour resin</h3>\n      Speaking of the mold: I bought some 2 mm thick 20x30 cm acrylic sheets.\n      Foamcore or wood could work as well, but I don't want the vacuum pump to\n      pull air from the mold into the casting. I cut them into 7x10 cm\n      rectangles and lined them with \"epoxy mold tape\" (fancy packing tape\n      that epoxy resin does not bond to). Then I used some Tec7 construction\n      adhesive to form a cube.\n      <p>\n      Prototype #2 shows a lot of promise! It's far from perfect, though. Here\n      are some of the things I'll improve for prototype #3 in no particular\n      order:\n      </p><ul><li><p>I struggled with attaching subassemblies to the mainplate because\n              I need to precisely control the distance between the subassembly and\n              the target surface: too snug and the assembly ends up crooked, too\n              far away and the glue doesn't adhere properly. A proper lab jack\n              (tiny scissor lift) might solve this, so one is on the way.\n            </p></li><li><p>The resin shrunk a lot while curing, most likely because it\n              overheated. I'll focus on proper airflow for my next cast.\n            </p></li><li><p>\n              People noted that it's difficult to see between the components, so\n              I'll \"explode\" the next model even more.\n            </p></li><li><p>\n              The hands are set to an invalid time. 10:10 is the way to go.\n            </p></li></ul></div><div><p>\n        The third iteration incorporated the scissor lift lab jack into the\n        process which, together with helping hands, allowed for much greater\n        precision in bringing components together true and square. I'm also\n        committing another watchmaking sin by placing a small magnet in the jaws\n        of my self-closing tweezers to gently hold tiny screws. This allows me\n        to lower a screw into its target drop of CA glue and simply lift the\n        tweezers once the glue has set. This is a much more reliable method than\n        trying to hold onto a sub-millimeter screw with self-closing tweezers\n        whose gripping force I have little control over. Magnets are sinful in\n        this case because magnetism can cause all sorts of timing issues in\n        mechanical watch movements, but that's obviously not an issue in my\n        case.\n        <p>\n        I also started using a CA accelerator to speed up the assembly, because\n        waiting for glue to cure is painful. Spraying the accelerator onto my\n        work would create a huge mess, so instead I spray some into a lidded\n        container and use tweezers to pick up droplets of the stuff to deposit\n        accurately onto the glue joints. This stuff is too volatile and runny to\n        be picked up by a needle. For me, the most effective use of the CA\n        accelerant is to first place a drop of CA glue on one of the two mating\n        surfaces, then dip the other surface in accelerant and quickly bringing\n        them into contact. I believe this works well because the joint cures\n        along the interface instead of curing from the outside-in as is the case\n        when spraying accelerant after the joint has glue in it.\n        </p><p>\n        You may see that the balance wheel is hanging from the hairspring in\n        this casting. The balance wheel marks the end of the train of wheels and\n        releases tiny amounts of energy from the mainspring 18,000 times per\n        hour. The balance assembly is the heart of the movement and is also the\n        most delicate component, and I want to highlight that by stretching the\n        hairspring to show its form. These components are not glued in place,\n        and the balance wheel hangs freely from its spring, meaning I have to\n        cast the whole assembly upside-down to achieve this effect.\n      </p></p></div><div><p>\n        At this point I figured I was ready to tackle the final boss of this\n        project: the ETA 2824 wristwatch movement that we've all seen in <a href=\"https://ciechanow.ski/mechanical-watch/\">Bartosz Ciechanowski's\n        blog post</a>. Well, I'll tackle the Chinese PT5000 clone movement\n        instead, because I can't justify spending €300 on a genuine movement\n        only to ensure it will never run again.\n      </p><div><a href=\"https://fellerts.no/img/epoch/pt5000-vs-render.jpg\"><img src=\"https://fellerts.no/img/epoch/pt5000-vs-render.jpg\"></a><p>\n          Comparing my PT5000 to Bartosz's render.\n        </p></div><p>\n        This movement arrived from China in good working order---I was actually\n        suprised at its performance out of the box. It ran with good amplitude\n        and little positional variance between horizontal and vertical\n        positions. I became less and less impressed as I disassembled the\n        movement prior to cleaning, though, because it was absolutely drenched\n        in oil. Many of the bridges had sharp burrs that broke off during\n        cleaning. Nothing was broken though. All in all, these movements really\n        need a proper service before putting them to service. That's not an\n        issue here though.\n        <p>\n        I was worried that the much smaller components would be challenging to\n        work with, but I found the process to be essentially the same as for\n        larger pocket watch movements. My 0.7 mm nylon fishing wire still fits\n        through most of the screw holes and the lab jack makes alignment a\n        breeze.\n        </p><p>\n        Some components needed special care, in particular the balance shock\n        springs that protect the delicate pivots of the balance staff from\n        shocks. These aren't normally found on older pocket watch movements\n        which is why a broken balance staff is one of the most common failure\n        modes of pocketwatches. Shock springs are fragile and are, in my\n        opinion, among the most difficult components to handle when servicing a\n        watch. Of course, I want to also explode the balance assembly, so I\n        needed a way to suspend the shock springs above the capstone jewel. By\n        laying the spring down flat on silicone and placing a drop of CA glue on\n        it, the surface tension of the glue fills in the inner disk. Once cured,\n        the spring with its hard and transparent interior can be lifted off the\n        silicone. This is how the luminous material (lume) is applied to the\n        hands of a watch---another watchmaker's trick.\n        </p><p>\n        The assembly process was luckily uneventful, and I finished the build\n        off with a black dial and a random set of hands from eBay.\n        Unfortunately, the casting process went completely south. Only after I\n        had created a mold, mixed, degassed, and poured resin, did I realize\n        that my mold was just barely too tall to fit comfortably in the vacuum\n        chamber. With some hasty modifications to the scaffolding that supported\n        the exploded model, I was able to pull a reasonable vacuum on it, but it\n        left the exploded model crooked. To add insult to injury, the resin\n        seems to have dissolved the paint on the date indicator ring, which left\n        milky streaks throughout the casting.\n        </p><p>\n        All in all, I spent roughly 18 hours stripping, cleaning and assembling\n        the exploded view of the PT5000. With improved technique I might get\n        this down below 15 hours, but it's </p> tedious work, and rushing\n        means I'll knock something off and have to redo work. Good to know,\n        because I will be doing this again until I get it right.\n        </p></div><div><a href=\"https://fellerts.no/img/epoch/pt5000-whale.jpg\"><img src=\"https://fellerts.no/img/epoch/pt5000-whale.jpg\"></a><p>The PT5000 disassembles nicely into a steampunk whale.</p><a href=\"https://fellerts.no/img/epoch/pt5000-assy-1.jpg\"><img src=\"https://fellerts.no/img/epoch/pt5000-assy-1.jpg\"></a><p>Installing the barrel bridge assembly.</p><a href=\"https://fellerts.no/img/epoch/pt5000-assy-4.jpg\"><img src=\"https://fellerts.no/img/epoch/pt5000-assy-4.jpg\"></a><a href=\"https://fellerts.no/img/epoch/pt5000-final.jpg\"><img src=\"https://fellerts.no/img/epoch/pt5000-final.jpg\"></a><p>Final casting. The paint on the date ring, dial and hands dissolved. Also, the model is crooked.</p></div><div><p>\n        Prototype #4 has a couple flaws which I'd like to fix before calling\n        this project done. The process of assembling the model is more or less\n        nailed down: I just need to find a way to seal any painted surface\n        before casting, as well as try to remember that my vacuum chamber has\n        limited volume. I bought another PT5000, this time in a complete watch\n        with case and a metal band, and started experimenting with sealing the\n        painted surfaces. Here's what didn't work:\n      </p><ul><li>CA glue dissolves paint just as readily as epoxy resin. It might\n          work on simple paint-jobs like the chapter indices and the hands, but\n          the date ring has crisp lines (the numbers) which I need to\n          preserve.</li><li>UV curable CA glue didn't cure on top of the paint. No idea why.</li><li>Same with UV curable epoxy resin.</li></ul>\n      Clear spray lacquer from the hardware store worked well and didn't\n      dissolve the paint. It did turn yellow as you'll see in the final casting,\n      but I'm okay with that. I also built a better jig to cut the fishing line\n      into equal lengths with square ends. It's a bit janky but worked very\n      well.\n      <p>\n      There's not much more to say. I'll stop blabbering and make way for an\n      image series.\n      </p></div><div><div><a href=\"https://fellerts.no/img/epoch/fishing-line-jig.jpg\"><img src=\"https://fellerts.no/img/epoch/fishing-line-jig.jpg\"></a><p>Janky but effective cutting jig for fishing line.</p></div><div><a href=\"https://fellerts.no/img/epoch/painting.jpg\"><img src=\"https://fellerts.no/img/epoch/painting.jpg\"></a><p>Painting the dial, hands and calendar ring...</p></div><div><a href=\"https://fellerts.no/img/epoch/ww-assy-1.jpg\"><img src=\"https://fellerts.no/img/epoch/ww-assy-1.jpg\"></a><p>\n          The watchmaker's side of the movement is assembled and fastened to the case.\n        </p></div><div><a href=\"https://fellerts.no/img/epoch/ww-assy-2.jpg\"><img src=\"https://fellerts.no/img/epoch/ww-assy-2.jpg\"></a><p>\n          Our watch is held by a stiff cardboard tube (okay, a toilet roll core)\n          to keep the finished half suspended mid-air while work continues on\n          the dial-side.\n        </p></div><div><a href=\"https://fellerts.no/img/epoch/ww-assy-3.jpg\"><img src=\"https://fellerts.no/img/epoch/ww-assy-3.jpg\"></a><p>\n          Aligning the date ring...\n        </p></div><div><a href=\"https://fellerts.no/img/epoch/ww-assy-6.jpg\"><img src=\"https://fellerts.no/img/epoch/ww-assy-6.jpg\"></a><p>\n          Done! I wish I could keep it in this form, but it's too\n          fragile.\n        </p></div></div>","contentLength":24052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44347425"},{"title":"Git Notes: Git's coolest, most unloved­ feature (2022)","url":"https://tylercipriani.com/blog/2022/11/19/git-notes-gits-coolest-most-unloved-feature/","date":1750583675,"author":"Delgan","guid":240,"unread":true,"content":"<blockquote><p>the short of it is: they’re cool for appending notes from automated\nsystems (like ticket or build systems) but not really for having\ninteractive conversations with other developers (at least not yet)</p></blockquote><p>Git notes are almost a secret.</p><p>They’re buried by their own distressing usability.</p><p>But git notes are continually rediscovered by engineers trying to\nstash metadata inside git.</p><p><strong>Git notes are powerful tools.</strong> And they could solve\nso many problems—if only they were better known and easier to use.</p><section><p>A common use of git notes is tacking metadata onto commits.</p><p>Once a commit cements itself in git’s history—that’s it. It’s\nimpossible to amend a commit message buried deep in a repo’s log.</p><p>But git notes enable you to amend new information about old commits\nin a special namespace. And they’re capable of so much more.</p><p><strong>Notes stow metadata about anything tracked by\ngit</strong>—any object: commits, blobs, and trees. All without futzing\nwith the object itself.</p><p>You add notes to the latest commit in a repo like this:</p><pre><code>git notes add -m 'Acked-by: &lt;tyler@tylercipriani.com&gt;'</code></pre><p>And then it shows up in :</p><pre><code>commit 1ef8b30ab7fc218ccc85c9a6411b1d2dd2925a16\nAuthor: Tyler Cipriani &lt;thcipriani@gmail.com&gt;\nDate:   Thu Nov 17 16:51:43 2022 -0700\n\n    Initial commit\n\n    Notes:\n        Acked-by: &lt;tyler@tylercipriani.com&gt;</code></pre></section><section><p>The git project itself offers an example of git notes in the wild.\nThey link each commit to its discussion on their mailing list.</p><pre><code>commit 00f09d0e4b1826ee0519ea64e919515032966450\nAuthor: &lt;redacted&gt;\nDate:   Thu Jan 28 02:05:55 2010 +0100\n\n    bash: support 'git notes' and its subcommands\n    ...\n\nNotes (amlog):\n    Message-Id: &lt;1264640755-22447-1-git-send-email-szeder@ira.uka.de&gt;</code></pre><p>Other folks are using notes for things like:</p><ul><li>Tracking time spent per commit or branch</li><li>Adding review and testing information to git log</li><li>And even fully distributed code review</li></ul></section><section><h2> Storing code reviews and test results in git\nnotes</h2><p>Here is a plea for all forges: make code review metadata available\noffline, inside git.</p><p>The <a href=\"https://gerrit.googlesource.com/plugins/reviewnotes/+/refs/heads/master/src/main/resources/Documentation/refs-notes-review.md\">reviewnotes</a>\nplugin for Gerrit is an example of how to do this\nwell. It makes it easy to see who reviewed code in git log:</p><pre><code>git fetch origin refs/notes/review:refs/notes/review\ngit log --show-notes=review</code></pre><p>The command above shows me all the standard git log info alongside\ninformation about what tests ran and who reviewed the code. All without\nforcing me into my browser.</p><pre><code>commit d1d17908d2a97f057887a4afbd99f6c40be56849\nAuthor: User &lt;user@example.com&gt;\nDate:   Sun Mar 27 18:10:51 2022 +0200\n\n    Change the thing\n\nNotes (review):\n    Verified+1: SonarQube Bot\n    Verified+2: jenkins-bot\n    Code-Review+2: Reviewer Human &lt;reviewerhuman@wikimedia.org&gt;\n    Submitted-by: jenkins-bot\n    Submitted-at: Tue, 14 Jun 2022 21:59:58 +0000\n    Reviewed-on: https://gerrit.wikimedia.org/r/c/mediawiki/core/+/774005\n    Project: mediawiki/core\n    Branch: refs/heads/master</code></pre></section><section><h2> Distributed code review  git notes</h2><p>Motivated hackers can knead and extend git notes. Using them as\ndistributed storage for any madcap idea.</p><p>Someone at Google cobbled together a full-on code review system\nteetering atop git notes called <a href=\"https://github.com/google/git-appraise\">git-appraise</a>.</p><p>Its authors have declared it a “fully distributed code\nreview”—independent of GitHub, GitLab, or any other code forge.</p><ul><li>Request review of a change</li><li>Review and merge a change</li></ul><p>And you can do all this from your local computer, even if GitHub is\ndown.</p><p>Plus, it’s equipped with an affectedly unaesthetic web interface, if\nthat’s your thing.</p></section><section><p>Git notes are a pain to use.</p><p>For commits, you can make viewing and adding notes easier using fancy\noptions in your gitconfig. But for storing notes about blobs\nor trees? Forget it. You’d need to be comfortable rooting around in\ngit’s <a href=\"https://git-scm.com/book/en/v2/Git-Internals-Plumbing-and-Porcelain\">plumbing</a>\nfirst.</p><p>So, for now: <strong>git notes are relegated to obscurity</strong>.\nForever hamstrung by an obscure and clunky interface and limited\nadoption—I often forget they’re there.</p></section><section><p>Git is a distributed code review system. But much of the value of git\nrepos ends up locked into forges, like GitHub.</p><p>Git notes are a path toward an alternative.</p><p>Git distributes the history of a piece of code. <strong>Git notes\ncould make it possible to distribute the history of an entire\nproject.</strong></p></section>","contentLength":4129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44345334"},{"title":"TPU Deep Dive","url":"https://henryhmko.github.io/posts/tpu/tpu.html","date":1750560663,"author":"transpute","guid":239,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44342977"},{"title":"U.S. bombs Iranian nuclear sites","url":"https://www.bbc.co.uk/news/live/ckg3rzj8emjt","date":1750550416,"author":"mattcollins","guid":238,"unread":true,"content":"<p>Since the recent US strikes, there are whispers from inside Iran that the country will close the strategic Hormuz Strait - although nothing is concrete yet.</p><p>The strait is located in the Gulf between Oman and Iran, and is considered the world's most vital oil transit choke point.</p><p>Foreign Minister Abbas Araghchi, when asked today if the country will close the strait, said there are \"various options\" on the table.</p><p>The Islamic Revolutionary Guard Corps (IRGC) has speedboats that could be used to blockade the strait. Its closure could lead to significant delays in the supply of oil needed by global markets, with a rise in oil prices.</p><p>Around a fifth of the planet's crude oil goes through the strait, which is only 40km wide at its narrowest point.</p><p>Several countries could be impacted, including the Gulf countries such as Saudi Arabia, the UAE, and Kuwait, but the impact is not limited to them.</p><p>China, India, Japan, and South Korea are among the top importers of crude oil that passes along it. Beijing is highly unlikely to welcome any rise in oil prices or disruptions to shipping routes.</p><p>And of course, by closing one of its major export routes, Iran itself would lose out. US Secretary of State Marco Rubio today said it would be “economic suicide”.</p>","contentLength":1254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44341639"},{"title":"Microsoft suspended the email account of an ICC prosecutor at The Hague","url":"https://www.nytimes.com/2025/06/20/technology/us-tech-europe-microsoft-trump-icc.html","date":1750507575,"author":"blinding-streak","guid":237,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44336915"},{"title":"Samsung embeds IronSource spyware app on phones across WANA","url":"https://smex.org/open-letter-to-samsung-end-forced-israeli-app-installations-in-the-wana-region/","date":1750475202,"author":"the-anarchist","guid":236,"unread":true,"content":"<p>In recent months, we have received numerous reports from users across West Asia and North Africa (WANA) expressing alarm over a little-known but deeply intrusive bloatware application—AppCloud—pre-installed on Samsung’s A and M series smartphones. Without users’ knowledge or consent, this bloatware collects sensitive personal data, cannot be removed without compromising device security, and offers no clear information about its privacy practices.</p><p>AppCloud, developed by the controversial Israeli-founded company ironSource (now owned by the American company <a href=\"https://investors.unity.com/news/news-details/2022/Unity-Announces-Merger-Agreement-with-ironSource/default.aspx\">Unity</a>), is embedded into devices sold in countries where such affiliations carry legal implications. Despite the serious privacy and security risks, Samsung has offered no transparency on how AppCloud functions, what data it collects, or why users cannot opt out.</p><p>This open letter, addressed to Samsung, calls for immediate transparency, accountability, and dialogue. Users deserve to know what is installed on their devices and how their data is being used, especially amid Israel’s espionage campaigns in the region.&nbsp;</p><p>We are writing to urgently request that Samsung be transparent regarding the pre-installation of AppCloud on its A and M series smartphones, particularly in West Asia and North Africa (WANA). We ask that Samsung provide information about AppCloud’s privacy practices, opt-out and removal options, and that Samsung reconsider future pre-installations in light of privacy rights. We also request a meeting with Samsung teams to discuss these concerns further.&nbsp;</p><p>According to our <a href=\"https://smex.org/invasive-israeli-software-is-harvesting-data-from-samsung-users-in-wana/\">analysis</a>, this intrusive software is , deeply integrated into the devices’ operating system, making it nearly impossible for regular users to uninstall it without root access, which voids warranties and poses security risks. Even disabling the bloatware is not effective as it can reappear after system updates.&nbsp;</p><p>The privacy policy is there is no accessible and transparent privacy policy for this bloatware and users are in the dark about what data is collected and how it is used. There is also no straightforward opt-out mechanism. The bloatware collects sensitive user data, including biometric information, IP addresses, device fingerprints.&nbsp;</p><p>The installation of AppCloud is done from the user, which violates GDPR provisions in the EU and relevant data protection laws in the WANA region states.&nbsp;</p><p>AppCloud is developed by ironSource, <strong>an Israel-founded company (now acquired by American company Unity),</strong> raising additional legal and ethical concerns in countries where Israeli companies are barred from operating, such as <a href=\"https://www.lexismiddleeast.com/law/Lebanon/Law_1_1955\">Lebanon</a>. ironSource is notorious for its questionable practices regarding user consent and data privacy.&nbsp;</p><p>Samsung’s terms of service mention third party applications but do not specifically address AppCloud or ironSource, despite the significant data access and control granted to this bloatware app.&nbsp;</p><p>The forced installation of AppCloud <strong>undermines the privacy and security rights of users in the MENA region and beyond.</strong> The lack of transparency and control over personal data is particularly alarming given<a href=\"https://telecomlead.com/smart-phone/smartphone-strategies-in-middle-east-for-market-share-119061\"> Samsung’s <strong>significant market share in the region</strong></a>.</p><p>In light of these concerns, we respectfully request that Samsung:</p><ul><li>Disclose the full privacy policy and data handling practices of AppCloud, making this information easily accessible to all users.</li><li>Offer a straightforward and effective method for users to opt out of AppCloud and remove it from their devices without compromising device functionality or warranty.</li><li>Provide a clear explanation for the decision to pre-install AppCloud on all A and M series devices in the WANA region.</li><li>Reconsider the continued pre-installation of AppCloud on future devices, in line with the right to privacy as established by Article 12 of the Universal Declaration of Human Rights.</li><li>We also request a <strong>meeting with the relevant Samsung teams</strong> to discuss these issues in detail and to better understand the company’s approach to user privacy and data protection in the WANA region.</li></ul><p>We look forward to your prompt response and to working together to ensure the privacy and security of all Samsung users.</p>","contentLength":4132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44334167"},{"title":"Harper – an open-source alternative to Grammarly","url":"https://writewithharper.com/","date":1750449105,"author":"ReadCarlBarks","guid":235,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44331362"},{"title":"YouTube's new anti-adblock measures","url":"https://iter.ca/post/yt-adblock/","date":1750438895,"author":"smitop","guid":234,"unread":true,"content":"<p>Over the past few months, YouTube has been trying another round of anti-adblock measures. Currently the anti-adblock stuff is being A/B tested, and one of my accounts is in the experimental group. I wrote a filter that partially avoids one of the anti-adblock measures, fake buffering, on uBlock Origin (and Brave browser, since it uses the same filter rules). (It’s already in the default filter lists, you don’t need to manually add the filter.)</p><p>One thing that people have ran into is “fake buffering”, where videos will take a while to load due to a lot of buffering, but only at the very start of the video (there’s no mid-video fake buffering). As I’ll explain, the fake buffering is 80% of the length of the ads you would’ve seen, so even with fake buffering you’re still saving time using an adblocker.</p><p>InnerTube is YouTube’s first party internal API that the web client and mobile apps use to interact with videos and get details about them. InnerTube endpoints look like <code>https://www.youtube.com/youtubei/</code>. One of those endpoints is , which the web client calls when you click on a video to get data about the URL and GVS stream URLs.</p><p>GVS (Google Video Services) is a service that serves video streams for YouTube, Google Drive, and Google Photos. To stream a video from GVS, you need to get a GVS  URL from InnerTube (or the Drive/Photos internal API). GVS URLs are signed and have an expiry time (usually 6 hours), so you can’t construct them on your own, you need to get one from InnerTube. One weird thing about GVS is that it isn’t only hosted from Google’s data center: ISPs can put <a href=\"https://support.google.com/interconnect/answer/9058809\">Google Global Cache</a> servers in their infrastructure so that they can serve YouTube videos without needing to send traffic outside the ISP’s network (only public/unlisted YouTube videos are served by GGC; private YT videos and Drive/Photos videos are always served from Google data centers). GVS URLs look like <code>https://rr1---sn-gvbxgn-tt1e6.googlevideo.com/videoplayback?expire=1750321185&amp;...</code> (but with a lot more query parameters).</p><p>Originally the web client streamed video by just using some query parameters to the GVS URL to specify what range of video it wanted, and GVS responded with the video contents for that range. But for complicated reasons, YouTube decided to improve on this with SABR (Server ABR (Adaptive Bit Rate)), which is YouTube’s proprietary binary protocol for streaming video data, which is better at avoiding buffering than today’s open formats (e.g. MPEG-DASH). One thing that SABR supports is the server sending a backoff to the client, instructing the client to wait some amount of time before trying again instead of sending video/audio data.</p><h2>The source of fake buffering</h2><p>What’s happening is that InnerTube is providing GVS streams that will give a backoff of 80% of the ad duration for ads for the first  request (for the content video, not the ads), so for example if the ad is 15 seconds you’ll get 12 seconds of backoff when blocking ads. If you have an unskippable 6 second ad AND a 15 second unskippable ad together the backoff will be 16.8 seconds. To be clear this isn’t server-side ad insertion; the ad and content streams are still separate (YouTube  doing a server-side ad insertion experiment, but that’s separate from fake buffering). The “Experiencing interruptions” dialogs are likely triggered by long backoffs from GVS.</p><p>This backoff  happen if you’re in the A/B test, regardless of if YouTube thinks you’re using an adblocker. You just don’t notice it if you’re not blocking ads, because the web client starts loading the content video while the ad plays, so the only difference for non-ad-blocking users is that the content video doesn’t start buffering until the ad is 80% over.</p><p>I’ve seen claims online that YouTube is “DAMAGING Computers By SPIKING CPU Usage If You Use Ad Block”. This is completely false; YouTube doesn’t use CPU usage waiting for the backoff to expire (and even if they used a spinloop to implement the wait, maxing a single core for &lt;30 seconds won’t damage any CPU).</p><p>How can you avoid getting backoffed until the unskippable ad is over? Don’t get served an ad in the first place. If you set the <code>playbackContext.contentPlaybackContext.isInlinePlaybackNoAd</code> property in player requests to true, InnerTube won’t serve you any ads and thus won’t include any backoff in the GVS streams.</p><p>We can write a filter rule that makes it so whenever the web client stringifies JSON bound for a server request, we add <code>\"isInlinePlaybackNoAd\":true</code> to the stringified JSON.</p><div><pre tabindex=\"0\"><code data-lang=\"adb\"></code></pre></div><p>How did I know to set that property? It’s referenced in the frontend JavaScript, so I could have spent a bunch of time reading all that. But there’s an easier way - while the web client interacts with InnerTube using JSON, that JSON API is actually generated from a Protocol Buffers definition, and there’s a way you can extract most of the underlying protobuf definition. I used <a href=\"https://github.com/ddd/req2proto\">req2proto</a>, which is a tool to extract protobuf definitions from Google’s internal APIs to get the full definitions used in the  call, and used that to find the  property.</p><p>This method only works for warm navigation, where you’ve already loaded the YouTube single page app and are clicking around within it. When you navigate directly to a watch page, the YouTube backend embeds a player response directly into the page as . Since the player request is made on the backend, we can’t set  on it.  One way to fix this for cold loads is to just remove that initial data to force YouTube to make a player request we can control (but see below before using these):</p><div><pre tabindex=\"0\"><code data-lang=\"adb\"></code></pre></div><p>This approach has some problems though, so you might not want to use it:</p><ul><li>It completely breaks livestreams, and probably some other things I haven’t tested</li><li>It causes the video player to briefly flash</li><li>It slows the page loading time</li></ul><h2>Bypassing the locker script</h2><p>So that filter kinda worked, but sometimes uBlock Origin wasn’t hooking . I investigated further and it turns out YouTube is running an A/B test where sometimes they add this to the frontend HTML as the very first thing in the  tag:</p><div><pre tabindex=\"0\"><code data-lang=\"html\"></code></pre></div><p>This locks a few global objects by using  to set them as non-writable, which prevents later code from overwriting them with a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy\">Proxy</a> that alters their behaviour. So uBlock Origin can only proxy JSON.stringify if it can run before this locker script does. On Firefox this is easily resolvable - you can use a <a href=\"https://github.com/gorhill/ublock/wiki/static-filter-syntax#html-filters\">HTML filter</a> to filter out the script tag from the source HTML before the page even starts being parsed. But that relies on extension APIs that Chromium doesn’t support.</p><p>The “fix” for the locker script so far is to hook  instead of .  is another function that handles the request body before it gets fetched. It would be nice if there was a way to actually defuse the locker script, instead of working around it. The version of the filter that hooks  instead is more complicated because uBO’s scriptlets don’t let you replace text on a key of an object, so the filter in uBO’s filter list injects this JS:</p><div><pre tabindex=\"0\"><code data-lang=\"js\"></code></pre></div><p>Thanks to the <a href=\"https://github.com/uBlockOrigin/uAssets/\">uAssets</a> maintainers for helping with that.</p><p>If you have any questions for me you can DM me on Discord as .</p>","contentLength":7154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44329712"},{"title":"Phoenix.new – Remote AI Runtime for Phoenix","url":"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/","date":1750431424,"author":"wut42","guid":233,"unread":true,"content":"<div><p>I’m Chris McCord, the creator of Elixir’s Phoenix framework. For the past several months, I’ve been working on a skunkworks project at Fly.io, and it’s time to show it off.</p></div><p>I wanted LLM agents to work just as well with Elixir as they do with Python and JavaScript. Last December, in order to figure out what that was going to take, I started a little weekend project to find out how difficult it would be to build a coding agent in Elixir.</p><p>A few weeks later, I had it spitting out working Phoenix applications and driving a full in-browser IDE. I knew this wasn’t going to stay a weekend project.</p><p>If you follow me on Twitter, you’ve probably seen me teasing this work as it picked up steam. We’re at a point where we’re pretty serious about this thing, and so it’s time to make a formal introduction.</p><p>World, meet <a href=\"https://phoenix.new\" title=\"\">Phoenix.new</a>, a batteries-included fully-online coding agent tailored to Elixir and Phoenix. I think it’s going to be the fastest way to build collaborative, real-time applications.</p><h2><a href=\"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/#whats-interesting-about-phoenix-new\" aria-label=\"Anchor\"></a></h2><p>First, even though it runs entirely in your browser, Phoenix.new gives both you and your agent a root shell, in an ephemeral virtual machine (a <a href=\"https://fly.io/docs/machines/overview/\" title=\"\">Fly Machine</a>) that gives our agent loop free rein to install things and run programs  — without any risk of messing up your local machine. You don’t think about any of this; you just open up the VSCode interface, push the shell button, and there you are, on the isolated machine you share with the Phoenix.new agent.</p><p>Second, it’s an agent system I built specifically for Phoenix. Phoenix is about real-time collaborative applications, and Phoenix.new knows what that means. To that end, Phoenix.new includes, in both its UI and its agent tools, a full browser. The Phoenix.new agent uses that browser “headlessly” to check its own front-end changes and interact with the app. Because it’s a full browser, instead of trying to iterate on screenshots, the agent sees real page content and JavaScript state – with or without a human present.</p><p>Agents build software the way you did when you first got started, the way you still do today when you prototype things. They don’t carefully design Docker container layers and they don’t really do release cycles. An agent wants to pop a shell and get its fingernails dirty.</p><p>A fully isolated virtual machine means Phoenix.new’s fingernails can get  If it wants to add a package to , it can do that and then run  or  and check the output. Sure. Every agent can do that. But if it wants to add an APT package to the base operating system, it can do that too, and make sure it worked. It owns the whole environment.</p><p>This offloads a huge amount of tedious, repetitive work.</p><p>At his <a href=\"https://youtu.be/LCEmiRjPEtQ?si=sR_bdu6-AqPXSNmY&amp;t=1902\" title=\"\">AI Startup School talk last week</a>, Andrej Karpathy related his experience of building a restaurant menu visualizer, which takes camera pictures of text menus and transforms all the menu items into pictures. The code, which he vibe-coded with an LLM agent, was the easy part; he had it working in an afternoon. But getting the app online took him a whole week.</p><p>With Phoenix.new, I’m taking dead aim at this problem. The apps we produce live in the cloud from the minute they launch. They have private, shareable URLs (we detect anything the agent generates with a bound port and give it a preview URL underneath , with integrated port-forwarding), they integrate with Github, and they inherit all the infrastructure guardrails of Fly.io: hardware virtualization, WireGuard, and isolated networks.</p><div><p>Github’s  CLI is installed by default. So the agent knows how to clone any repo, or browse issues, and you can even authorize it for internal repositories to get it working with your team’s existing projects and dependencies.</p></div><p>Full control of the environment also closes the loop between the agent and deployment. When Phoenix.new boots an app, it watches the logs, and tests the application. When an action triggers an error, Phoenix.new notices and gets to work.</p><h2><a href=\"https://fly.io/blog/phoenix-new-the-remote-ai-runtime/#watch-it-build-in-real-time\" aria-label=\"Anchor\"></a></h2><p><a href=\"https://phoenix.new\" title=\"\">Phoenix.new</a> can interact with web applications the way users do: with a real browser.</p><p>The Phoenix.new environment includes a headless Chrome browser that our agent knows how to drive. Prompt it to add a front-end feature to your application, and it won’t just sketch the code out and make sure it compiles and lints. It’ll pull the app up itself and poke at the UI, simultaneously looking at the page content, JavaScript state, and server-side logs.</p><p>Phoenix is all about <a href=\"https://fly.io/blog/how-we-got-to-liveview/\" title=\"\">“live” real-time</a> interactivity, and gives us seamless live reload. The user interface for Phoenix.new itself includes a live preview of the app being worked on, so you can kick back and watch it build front-end features incrementally. Any other  tabs you have open also update as it goes. It’s wild.</p><p>Phoenix.new can already build real, full-stack applications with WebSockets, Phoenix’s Presence features, and real databases. I’m seeing it succeed at business and collaborative applications right now.</p><p>But there’s no fixed bound on the tasks you can reasonably ask it to accomplish. If you can do it with a shell and a browser, I want Phoenix.new to do it too. And it can do these tasks with or without you present.</p><p>For example: set a  and tell the agent about it. The agent knows enough to go explore it with , and it’ll propose apps based on the schemas it finds. It can model Ecto schemas off the database. And if MySQL is your thing, the agent will just  a MySQL client and go to town.</p><p>Frontier model LLMs have vast world knowledge. They generalize extremely well. At ElixirConfEU, I did a <a href=\"https://www.youtube.com/watch?v=ojL_VHc4gLk&amp;t=3923s\" title=\"\">demo vibe-coding Tetris</a> on stage. Phoenix.new nailed it, first try, first prompt. It’s not like there’s gobs of Phoenix LiveView Tetris examples floating around the Internet! But lots of people have published Tetris code, and lots of people have written LiveView stuff, and 2025 LLMs can connect those dots.</p><p>At this point you might be wondering – can I just ask it to build a Rails app? Or an Expo React Native app? Or Svelte? Or Go?</p><p>Our system prompt is tuned for Phoenix today, but all languages you care about are already installed. We’re still figuring out where to take this, but adding new languages and frameworks definitely ranks highly in my plans.</p><p>Agents can do real work, today, with or without a human present. Buckle up: the future of development, at least in the common case, probably looks less like cracking open a shell and finding a file to edit, and more like popping into a CI environment with agents working away around the clock.</p><p>Local development isn’t going away. But there’s going to be a shift in where the majority of our iterations take place. I’m already using Phoenix.new to triage  Github issues and pick problems to solve. I close my laptop, grab a cup of coffee, and wait for a PR to arrive — Phoenix.new knows how PRs work, too. We’re already here, and this space is just getting started.</p><p>This isn’t where I thought I’d end up when I started poking around. The Phoenix and LiveView journey was much the same. Something special was there and the projects took on a life of their own. I’m excited to share this work now, and see where it might take us. I can’t wait to see what folks build.</p>","contentLength":7163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44328326"},{"title":"Hurl: Run and test HTTP requests with plain text","url":"https://github.com/Orange-OpenSource/hurl","date":1750391703,"author":"flykespice","guid":232,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44324592"}],"tags":["dev","hn"]}