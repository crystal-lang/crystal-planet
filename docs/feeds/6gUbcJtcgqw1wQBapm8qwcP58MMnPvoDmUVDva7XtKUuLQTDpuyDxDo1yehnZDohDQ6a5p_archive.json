{"id":"6gUbcJtcgqw1wQBapm8qwcP58MMnPvoDmUVDva7XtKUuLQTDpuyDxDo1yehnZDohDQ6a5p","title":"Slashdot: Developers","displayTitle":"Dev - Slashdot - Dev","url":"http://rss.slashdot.org/Slashdot/slashdotDevelopers","feedLink":"https://developers.slashdot.org/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":15,"items":[{"title":"'The Computer-Science Bubble Is Bursting'","url":"https://developers.slashdot.org/story/25/06/25/1730250/the-computer-science-bubble-is-bursting?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750872600,"author":"msmash","guid":311,"unread":true,"content":"theodp writes: The job of the future might already be past its prime,\" writes The Atlantic's Rose Horowitch in The Computer-Science Bubble Is Bursting. \"For years, young people seeking a lucrative career were urged to go all in on computer science. From 2005 to 2023, the number of comp-sci majors in the United States quadrupled. All of which makes the latest batch of numbers so startling. This year, enrollment grew by only 0.2 percent nationally, and at many programs, it appears to already be in decline, according to interviews with professors and department chairs. At Stanford, widely considered one of the country's top programs, the number of comp-sci majors has stalled after years of blistering growth. Szymon Rusinkiewicz, the chair of Princeton's computer-science department, told me that, if current trends hold, the cohort of graduating comp-sci majors at Princeton is set to be 25 percent smaller in two years than it is today. The number of Duke students enrolled in introductory computer-science courses has dropped about 20 percent over the past year.\" \n\n\"But if the decline is surprising, the reason for it is fairly straightforward: Young people are responding to a grim job outlook for entry-level coders. In recent years, the tech industry has been roiled by layoffs and hiring freezes. The leading culprit for the slowdown is technology itself. Artificial intelligence has proved to be even more valuable as a writer of computer code than as a writer of words. This means it is ideally suited to replacing the very type of person who built it. A recent Pew study found that Americans think software engineers will be most affected by generative AI. Many young people aren't waiting to find out whether that's true.\" \n\nMeanwhile, writing in the Communications of the ACM, Orit Hazzan and Avi Salmon ask: Should Universities Raise or Lower Admission Requirements for CS Programs in the Age of GenAI? \"This debate raises a key dilemma: should universities raise admission standards for computer science programs to ensure that only highly skilled problem-solvers enter the field, lower them to fill the gaps left by those who now see computer science as obsolete due to GenAI, or restructure them to attract excellent candidates with diverse skill sets who may not have considered computer science prior to the rise of GenAI, but who now, with the intensive GenAI and vibe coding tools supporting programming tasks, may consider entering the field?","contentLength":2470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Behind the Scenes at the Python Software Foundation","url":"https://developers.slashdot.org/story/25/06/23/0542243/behind-the-scenes-at-the-python-software-foundation?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750657680,"author":"EditorDavid","guid":310,"unread":true,"content":"The Python Software Foundation (\"made up of, governed, and led by the community\") does more than just host Python and its documnation, the Python Package Repository, and the development workflows of core CPython developers. This week the PSF released its 28-page Annual Impact Report this week, noting that 2024 was their first year with three CPython developers-in-residence â€” and \"Between Lukasz, Petr, and Serhiy, over 750 pull requests were authored, and another 1,500 pull requests by other authors were reviewed and merged.\"\n\n\nLukasz Langa co-implemented the new colorful shell included in Python 3.13, along with Pablo Galindo Salgado, Emily Morehouse-Valcarcel, and Lysandros Nikolaou.... Code-wise, some of the most interesting contributions by Petr Viktorin were around the ctypes module that allows interaction between Python and C.... These are just a few of Serhiy Storchaka's many contributions in 2024: improving error messages for strings, bytes, and bytearrays; reworking support for var-arguments in the C argument handling generator called \"Argument Clinic\"; fixing memory leaks in regular expressions; raising the limits for Python integers on 64-bit platforms; adding support for arbitrary code page encodings on Windows; improving complex and fraction number support... \n\n\nThanks to the investment of [the OpenSSF's security project] Alpha-Omega in 2024, our Security Developer-in-Residence, Seth Larson, continued his work improving the security posture of CPython and the ecosystem of Python packages. Python continues to be an open source security leader, evident by the Linux kernel becoming a CVE Numbering Authority using our guide as well as our publication of a new implementers guide for Trusted Publishers used by Ruby, Crates.io, and Nuget. Python was also recommended as a memory-safe programming language in early 2024 by the White House and CISA following our response to the Office of the National Cyber Directory Request for Information on open source security in 2023... Due to the increasing demand for SBOMs, Seth has taken the initiative to generate SBOM documents for the CPython runtime and all its dependencies, which are now available on python.org/downloads. Seth has also started work on standardizing SBOM documents for Python packages with PEP 770, aiming to solve the \"Phantom Dependency\" problem and accurately represent non-Python software included in Python packages. \n\n\nWith the continued investment in 2024 by Amazon Web Services Open Source and Georgetown CSET for this critical role, our PyPI Safety &amp; Security Engineer, Mike Fiedler, completed his first full calendar year at the PSF... In March 2024, Mike added a \"Report project as malware\" button on the website, creating more structure to inbound reports and decreasing remediation time. This new button has been used over 2,000 times! The large spike in June led to prohibiting Outlook email domains, and the spike in November was driven by a persistent attack. Mike developed the ability to place projects in quarantine pending further investigation. Thanks to a grant from Alpha-Omega, Mike will continue his work for a second year. We plan to do more work on minimizing time-on-PyPI for malware in 2025... \n\nIn 2024, PyPI saw an 84% growth in download counts and 48% growth in bandwidth, serving 526,072,569,160 downloads for the 610,131 projects hosted there, requiring 1.11 Exabytes of data transfer, or 281.6 Gbps of bandwidth 24x7x365. In 2024, 97k new projects, 1.2 million new releases, and 3.1 million new files were uploaded to the index.","contentLength":3565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RedMonk Ranks Top Programming Languages Over Time - and Considers Ditching Its 'Stack Overflow' Metric","url":"https://developers.slashdot.org/story/25/06/23/009244/redmonk-ranks-top-programming-languages-over-time---and-considers-ditching-its-stack-overflow-metric?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750637520,"author":"EditorDavid","guid":309,"unread":true,"content":"The developer-focused analyst firm RedMonk releases twice-a-year rankings of programming language popularity. This week they also released a handy graph showing the movement of top 20 languages since 2012. Their current rankings for programming language popularity...\n\n\n\n 1. JavaScript\n2. Python\n3. Java\n4. PHP\n5. C#\n6. TypeScript\n7. CSS\n8. C++\n9. Ruby\n10. C \n\n\nThe chart shows that over the years the rankings really haven't changed much (other than a surge for TypeScript and Python, plus a drop for Ruby). JavaScript has consistently been #1 (except in two early rankings, where it came in behind Java). And in 2020 Java finally slipped from #2 down to #3, falling behind... Python. Python had already overtaken PHP for the #3 spot in 2017, pushing PHP to a steady #4. C# has maintained the #5 spot since 2014 (though with close competition from both C++ and CSS). And since 2021 the next four spots have been held by Ruby, C, Swift, and R. \n\nThe only change in the current top 20 since the last ranking \"is Dart dropping from a tie with Rust at 19 into sole possession of 20,\" writes RedMonk co-founder Stephen O'Grady. \"In the decade and a half that we have been ranking these languages, this is by far the least movement within the top 20 that we have seen. While this is to some degree attributable to a general stasis that has settled over the rankings in recent years, the extraordinary lack of movement is likely also in part a manifestation of Stack Overflow's decline in query volume...\"\n\nThe arrival of AI has had a significant and accelerating impact on Stack Overflow, which comprises one half of the data used to both plot and rank languages twice a year... Stack Overflow's value from an observational standpoint is not what it once was, and that has a tangible impact, as we'll see.... \nAs that long time developer site sees fewer questions, it becomes less impactful in terms of driving volatility on its half of the rankings axis, and potentially less suggestive of trends moving forward... [W]e're not yet at a point where Stack Overflow's role in our rankings has been deprecated, but the conversations at least are happening behind the scenes. \n\n\"The veracity of the Stack Overflow data is increasingly questionable,\" writes RedMonk's research director:\nWhen we use Stack Overflow for programming language rankings we measure how many questions are asked using specific programming language tags... While other pieces, like Matt Asay's AI didn't kill Stack Overflow are right to point out that the decline existed before the advent of AI coding assistants, it is clear that the usage dramatically decreased post 2023 when ChatGPT became widely available. The number of questions asked are now about 10% what they were at Stack Overflow's peak. \n\n\"RedMonk is continuing to evaluate the quality of this analysis,\" the research director concludes, arguing \"there is value in long-lived data, and seeing trends move over a decade is interesting and worthwhile. On the other hand, at this point half of the data feeding the programming language rankings is increasingly stale and of questionable value on a going-forward basis, and there is as of now no replacement public data set available. \n\n\"We'll continue to watch and advise you all on what we see with Stack Overflow's data.\"","contentLength":3300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is 'Minecraft' a Better Way to Teach Programming in the Age of AI?","url":"https://developers.slashdot.org/story/25/06/22/0535221/is-minecraft-a-better-way-to-teach-programming-in-the-age-of-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750577640,"author":"EditorDavid","guid":308,"unread":true,"content":"The education-news site EdSurge published \"sponsored content\" from Minecraft Education this month. \"Students light up when they create something meaningful,\" the article begins. \"Self-expression fuels learning, and creativity lies at the heart of the human experience.\" \n\nBut they also argue that \"As AI rapidly reshapes software development, computer science education must move beyond syntax drills and algorithmic repetition.\" Students \"must also learn to think systemically...\"\nAs AI automates many of the mechanical aspects of programming, the value of CS education is shifting, from writing perfect code to shaping systems, telling stories through logic and designing ethical, human-centered solutions... [I]t's critical to offer computer science experiences that foster invention, expression and design. This isn't just an education issue â€” it's a workforce one. Creativity now ranks among the top skills employers seek, alongside analytical thinking and AI literacy. As automation reshapes the job market, McKinsey estimates up to 375 million workers may need to change occupations by 2030. The takeaway? We need more adaptable, creative thinkers. \nCreative coding, where programming becomes a medium for self-expression and innovation, offers a promising solution to this disconnect. By positioning code as a creative tool, educators can tap into students' intrinsic motivation while simultaneously building computational thinking skills. This approach helps students see themselves as creators, not just consumers, of technology. It aligns with digital literacy frameworks that emphasize critical evaluation, meaningful contribution and not just technical skills. \nOne example of creative coding comes from a curriculum that introduces computer science through game design and storytelling in Minecraft... Developed by Urban Arts in collaboration with Minecraft Education, the program offers middle school teachers professional development, ongoing coaching and a 72-session curriculum built around game-based instruction. Designed for grades 6-8, the project-based program is beginner-friendly; no prior programming experience is required for teachers or students. It blends storytelling, collaborative design and foundational programming skills with a focus on creativity and equity.... Students use Minecraft to build interactive narratives and simulations, developing computational thinking and creative design... Early results are promising: 93 percent of surveyed teachers found the Creative Coders program engaging and effective, noting gains in problem-solving, storytelling and coding, as well as growth in critical thinking, creativity and resilience. \n\nAs AI tools like GitHub Copilot become standard in development workflows, the definition of programming proficiency is evolving. Skills like prompt engineering, systems thinking and ethical oversight are rising in importance, precisely what creative coding develops... As AI continues to automate routine tasks, students must be able to guide systems, understand logic and collaborate with intelligent tools. Creative coding introduces these capabilities in ways that are accessible, culturally relevant and engaging for today's learners. \n\nSome background from long-time Slashdot reader theodp:\n\n\n\nThe Urban Arts and Microsoft Creative Coders program touted by EdSurge in its advertorial was funded by a $4 million Education Innovation and Research grant that was awarded to Urban Arts in 2023 by the U.S. Education Department \"to create an engaging, game-based, middle school CS course using Minecraft tools\" for 3,450 middle schoolers (6th-8th grades)\" in New York and California (Urban Arts credited Minecraft for helping craft the winning proposal)... New York City is a Minecraft Education believer â€” the Mayor's Office of Media and Entertainment recently kicked off summer with the inaugural NYC Video Game Festival, which included the annual citywide Minecraft Education Battle of the Boroughs Esports Competition in partnership with NYC Public Schools.","contentLength":4039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anthropic Deploys Multiple Claude Agents for 'Research' Tool - Says Coding is Less Parallelizable","url":"https://developers.slashdot.org/story/25/06/21/0442227/anthropic-deploys-multiple-claude-agents-for-research-tool---says-coding-is-less-parallelizable?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750520040,"author":"EditorDavid","guid":307,"unread":true,"content":"In April Anthorpic introduced a new AI trick: multiple Claude agents combine for a \"Research\" feature that can \"search across both your internal work context and the web\" (as well as Google Workspace \"and any integrations...\") \n\nBut a recent Anthropic blog post notes this feature \"involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously,\" which brings challenges \"in agent coordination, evaluation, and reliability.... The model must operate autonomously for many turns, making decisions about which directions to pursue based on intermediate findings.\"\nMulti-agent systems work mainly because they help spend enough tokens to solve the problem.... This finding validates our architecture that distributes work across agents with separate context windows to add more capacity for parallel reasoning. The latest Claude models act as large efficiency multipliers on token use, as upgrading to Claude Sonnet 4 is a larger performance gain than doubling the token budget on Claude Sonnet 3.7. Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents. \n\nThere is a downside: in practice, these architectures burn through tokens fast. In our data, agents typically use about 4Ãƒ-- more tokens than chat interactions, and multi-agent systems use about 15Ãƒ-- more tokens than chats. For economic viability, multi-agent systems require tasks where the value of the task is high enough to pay for the increased performance. Further, some domains that require all agents to share the same context or involve many dependencies between agents are not a good fit for multi-agent systems today. \n\nFor instance, most coding tasks involve fewer truly parallelizable tasks than research, and LLM agents are not yet great at coordinating and delegating to other agents in real time. We've found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools. \nThanks to Slashdot reader ZipNada for sharing the news.","contentLength":2166,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Migrates Its Password Monitoring Service to Swift from Java, Gains 40% Performance Uplift","url":"https://apple.slashdot.org/story/25/06/15/2126220/apple-migrates-its-password-monitoring-service-to-swift-from-java-gains-40-performance-uplift?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750022940,"author":"EditorDavid","guid":306,"unread":true,"content":"Meta and AWS have used Rust, and Netflix uses Go,reports the programming news site InfoQ. But using another language, Apple recently \"migrated its global Password Monitoring service from Java to Swift, achieving a 40% increase in throughput, and significantly reducing memory usage.\" \n\n\nThis freed up nearly 50% of their previously allocated Kubernetes capacity, according to the article, and even \"improved startup time, and simplified concurrency.\"\n\n\nIn a recent post, Apple engineers detailed how the rewrite helped the service scale to billions of requests per day while improving responsiveness and maintainability... \"Swift allowed us to write smaller, less verbose, and more expressive codebases (close to 85% reduction in lines of code) that are highly readable while prioritizing safety and efficiency.\" \nApple's Password Monitoring service, part of the broader Password app's ecosystem, is responsible for securely checking whether a user's saved credentials have appeared in known data breaches, without revealing any private information to Apple. It handles billions of requests daily, performing cryptographic comparisons using privacy-preserving protocols. This workload demands high computational throughput, tight latency bounds, and elastic scaling across regions... Apple's previous Java implementation struggled to meet the service's growing performance and scalability needs. Garbage collection caused unpredictable pause times under load, degrading latency consistency. Startup overhead â€” from JVM initialization, class loading, and just-in-time compilation, slowed the system's ability to scale in real time. Additionally, the service's memory footprint, often reaching tens of gigabytes per instance, reduced infrastructure efficiency and raised operational costs. \nOriginally developed as a client-side language for Apple platforms, Swift has since expanded into server-side use cases.... Swift's deterministic memory management, based on reference counting rather than garbage collection (GC), eliminated latency spikes caused by GC pauses. This consistency proved critical for a low-latency system at scale. After tuning, Apple reported sub-millisecond 99.9th percentile latencies and a dramatic drop in memory usage: Swift instances consumed hundreds of megabytes, compared to tens of gigabytes with Java.\n \n\n\"While this isn't a sign that Java and similar languages are in decline,\" concludes InfoQ's article, \"there is growing evidence that at the uppermost end of performance requirements, some are finding that general-purpose runtimes no longer suffice.\"","contentLength":2587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python Creator Guido van Rossum Asks: Is 'Worse is Better' Still True for Programming Languages?","url":"https://developers.slashdot.org/story/25/06/14/2359251/python-creator-guido-van-rossum-asks-is-worse-is-better-still-true-for-programming-languages?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749952140,"author":"EditorDavid","guid":305,"unread":true,"content":"In 1989 a computer scientist argued that more functionality in software actually lowers usability and practicality â€” leading to the counterintuitive proposition that \"worse is better\". But is that still true? \n\nPython's original creator Guido van Rossum addressed the question last month in a lightning talk at the annual Python Language Summit 2025.\n\nGuido started by recounting earlier periods of Python development from 35 years ago, where he used UNIX \"almost exclusively\" and thus \"Python was greatly influenced by UNIX's 'worse is better' philosophy\"... \"The fact that [Python] wasn't perfect encouraged many people to start contributing. All of the code was straightforward, there were no thoughts of optimization... These early contributors also now had a stake in the language; [Python] was also their baby\"... \n\nGuido contrasted early development to how Python is developed now: \"features that take years to produce from teams of software developers paid by big tech companies. The static type system requires an academic-level understanding of esoteric type system features.\" And this isn't just Python the language, \"third-party projects like numpy are maintained by folks who are paid full-time to do so.... Now we have a huge community, but very few people, relatively speaking, are contributing meaningfully.\" \nGuido asked whether the expectation for Python contributors going forward would be that \"you had to write a perfect PEP or create a perfect prototype that can be turned into production-ready code?\" Guido pined for the \"old days\" where feature development could skip performance or feature-completion to get something into the hands of the community to \"start kicking the tires\". \"Do we have to abandon 'worse is better' as a philosophy and try to make everything as perfect as possible?\" Guido thought doing so \"would be a shame\", but that he \"wasn't sure how to change it\", acknowledging that core developers wouldn't want to create features and then break users with future releases. \nGuido referenced David Hewitt's PyO3 talk about Rust and Python, and that development \"was using worse is better,\" where there is a core feature set that works, and plenty of work to be done and open questions. \"That sounds a lot more fun than working on core CPython\", Guido paused, \"...not that I'd ever personally learn Rust. Maybe I should give it a try after,\" which garnered laughter from core developers. \n\n\"Maybe we should do more of that: allowing contributors in the community to have a stake and care\".","contentLength":2528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UK Universities Sign $13.3 Million Deal To Avoid Oracle Java Back Fees","url":"https://developers.slashdot.org/story/25/06/13/2034219/uk-universities-sign-133-million-deal-to-avoid-oracle-java-back-fees?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749857400,"author":"BeauHD","guid":304,"unread":true,"content":"An anonymous reader quotes a report from The Register: UK universities and colleges have signed a framework worth up to 9.86 million pounds ($13.33 million) with Oracle to use its controversial Java SE Universal Subscription model, in exchange for a \"waiver of historic fees due for any institutions who have used Oracle Java since 2023.\" Jisc, a membership organization that runs procurement for higher and further education establishments in the UK, said it had signed an agreement to purchase the new subscription licenses after consultation with members. In a procurement notice, it said institutions that use Oracle Java SE are required to purchase subscriptions. \"The agreement includes the waiver of historic fees due for any institutions who have used Oracle Java since 2023,\" the notice said.\n \nThe Java SE Universal Subscription was introduced in January 2023 to an outcry from licensing experts and analysts. It moved licensing of Java from a per-user basis to a per-employee basis. At the time, Oracle said it was \"a simple, low-cost monthly subscription that includes Java SE Licensing and Support for use on Desktops, Servers or Cloud deployments.\" However, licensing advisors said early calculations to help some clients showed that the revamp might increase costs by up to ten times. Later, analysis from Gartner found the per-employee subscription model to be two to five times more expensive than the legacy model.\n \n\"For large organizations, we expect the increase to be two to five times, depending on the number of employees an organization has,\" Nitish Tyagi, principal Gartner analyst, said in July 2024. \"Please remember, Oracle defines employees as part-time, full-time, temporary, agents, contractors, as in whosoever supports internal business operations has to be licensed as per the new Java Universal SE Subscription model.\" Since the introduction of the new Oracle Java licensing model, user organizations have been strongly advised to move off Oracle Java and find open source alternatives for their software development and runtime environments. A survey of Oracle users found that only one in ten was likely to continue to stay with Oracle Java, in part as a result of the licensing changes.","contentLength":2225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Code.org Curriculum Aims To Make Schoolkids Python-Literate and AI-Ready","url":"https://developers.slashdot.org/story/25/06/09/2254257/new-codeorg-curriculum-aims-to-make-schoolkids-python-literate-and-ai-ready?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749560400,"author":"BeauHD","guid":303,"unread":true,"content":"Longtime Slashdot reader theodp writes: The old Code.org curriculum page for middle and high school students has been changed to include a new Python Lab in the tech-backed nonprofit's K-12 offerings. Elsewhere on the site, a Computer Science and AI Foundations curriculum is described that includes units on 'Foundations of AI Programming [in Python]' and 'Insights from Data and AI [aka Data Science].' A more-detailed AI Foundations Syllabus 25-26 document promises a second semester of material is coming soon: \"This semester offers an innovative approach to teaching programming by integrating learning with and about artificial intelligence (AI). Using Python as the primary language, students build foundational programming skills while leveraging AI tools to enhance computational thinking and problem-solving. The curriculum also introduces students to the basics of creating AI-powered programs, exploring machine learning, and applying data science principles.\"\n \nNewly-posted videos on Code.org's YouTube channel appear to be intended to support the new Python-based CS &amp; AI course. \"Python is extremely versatile,\" explains a Walmart data scientist to open the video for Data Science: Using Python. \"So, first of all, Python is one of the very few languages that can handle numbers very, very well.\" A researcher at the Univ. of Washington's Institute for Health Metrics and Evaluation (IHME) adds, \"Python is the gold standard and what people expect data scientists to know [...] Key to us being able to handle really big data sets is our use of Python and cluster computing.\" Adding to the Python love, an IHME data analyst explains, \"Python is a great choice for large databases because there's a lot of support for Python libraries.\"\n \nCode.org is currently recruiting teachers to attend its CS and AI Foundations Professional Learning program this summer, which is being taught by Code.org's national network of university and nonprofit regional partners (teachers who signup have a chance to win $250 in DonorsChoose credits for their classrooms). A flyer for a five-day Michigan Professional Development program to prepare teachers for a pilot of the Code.org CS &amp; A course touts the new curriculum as \"an alternative to the AP [Computer Science] pathway\" (teachers are offered scholarships covering registration, lodging, meals, and workshop materials).\n \nInterestingly, Code.org's embrace of Python and Data Science comes as the nonprofit changes its mission to 'make CS and AI a core part of K-12 education' and launches a new national campaign with tech leaders to make CS and AI a graduation requirement. Prior to AI changing the education conversation, Code.org in 2021 boasted that it had lined up a consortium of tech giants, politicians, and educators to push its new $15 million Amazon-bankrolled Java AP CS A curriculum into K-12 classrooms. Just three years later, however, Amazon CEO Andy Jassy was boasting to investors that Amazon had turned to AI to automatically do Java coding that he claimed would have otherwise taken human coders 4,500 developer-years to complete.","contentLength":3105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bill Atkinson, Hypercard Creator and Original Mac Team Member, Dies at Age 74","url":"https://apple.slashdot.org/story/25/06/08/016210/bill-atkinson-hypercard-creator-and-original-mac-team-member-dies-at-age-74?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749346440,"author":"EditorDavid","guid":302,"unread":true,"content":" AppleInsider reports:\n\nThe engineer behind much of the Mac's early graphical user interfaces, QuickDraw, MacPaint, Hypercard and much more, William D. \"Bill\" Atkinson, died on June 5 of complications from pancreatic cancer... \nAtkinson, who built a post-Apple career as a noted nature photographer, worked at Apple from 1978 to 1990. Among his lasting contributions to Apple's computers were the invention of the menubar, the selection lasso, the \"marching ants\" item selection animation, and the discovery of a midpoint circle algorithm that enabled the rapid drawing of circles on-screen. \nHe was Apple Employee No. 51, recruited by Steve Jobs. Atkinson was one of the 30 team members to develop the first Macintosh, but also was principle designer of the Lisa's graphical user interface (GUI), a novelty in computers at the time. He was fascinated by the concept of dithering, by which computers using dots could create nearly photographic images similar to the way newspapers printed photos. He is also credited (alongside Jobs) for the invention of RoundRects, the rounded rectangles still used in Apple's system messages, application windows, and other graphical elements on Apple products. \nHypercard was Atkinson's main claim to fame. He built the a hypermedia approach to building applications that he once described as a \"software erector set.\" The Hypercard technology debuted in 1987, and greatly opened up Macintosh software development.\n \nIn 2012 some video clips of Atkinson appeared in some rediscovered archival footage. (Original Macintosh team developer Andy Hertzfeld uploaded \"snippets from interviews with members of the original Macintosh design team, recorded in October 1983 for projected TV commercials that were never used.\") \n\nBlogger John Gruber calls Atkinson \"One of the great heroes in not just Apple history, but computer history.\"\n\n If you want to cheer yourself up, go to Andy Hertzfeld's Folklore.org site and (re-)read all the entries about Atkinson. Here's just one, with Steve Jobs inspiring Atkinson to invent the roundrect. Here's another (surely near and dear to my friend Brent Simmons's heart) with this kicker of a closing line: \"I'm not sure how the managers reacted to that, but I do know that after a couple more weeks, they stopped asking Bill to fill out the form, and he gladly complied.\" \n\nSome of his code and algorithms are among the most efficient and elegant ever devised. The original Macintosh team was chock full of geniuses, but Atkinson might have been the most essential to making the impossible possible under the extraordinary technical limitations of that hardware... In addition to his low-level contributions like QuickDraw, Atkinson was also the creator of MacPaint (which to this day stands as the model for bitmap image editorsÃ¢ â€” Ã¢Photoshop, I would argue, was conceptually derived directly from MacPaint) and HyperCard (\"inspired by a mind-expanding LSD journey in 1985\"), the influence of which cannot be overstated.\n\n I say this with no hyperbole: Bill Atkinson may well have been the best computer programmer who ever lived. Without question, he's on the short list. What a man, what a mind, what gifts to the world he left us.","contentLength":3207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask Slashdot: How Important Is It For Programmers to Learn Touch Typing?","url":"https://ask.slashdot.org/story/25/06/07/0811223/ask-slashdot-how-important-is-it-for-programmers-to-learn-touch-typing?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749317640,"author":"EditorDavid","guid":301,"unread":true,"content":"Once upon a time, long-time Slashdot reader tgibson learned how to type on a manual typewriter, back in an 8th grade classroom. \nAnd to this day, they write, \"my bias is to nod approvingly at touch typists and roll my eyes at those who need to stare at the keyboard while typing...\" But how true is that for computer professionals today?\n\nAfter 15 years I left industry and became a post-secondary computer science educator. Occasionally I rant to my students about the importance of touch-typing as a skill to have as a software engineer. \n\nBut I've been out of the game for some time now. Those of you hiring or working with freshly-minted software engineers, what's your take? \n\nOne anonymous Slashdot reader responded:\n\nOh, you mean the kid in the next cubicle that has said \"Hey Siri\" 297 times this morning? I'll let you know when he starts typing. A minor suggestion to office managers... please purchase a very quiet keyboard. Fellow cube-mates who are accomplished typists would consider that struggling audibly to be akin to nails on a blackboard... \nShare your own thoughts in the comments. \n\nHow important is it for programmers to learn touch typing?","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'For Algorithms, a Little Memory Outweighs a Lot of Time'","url":"https://developers.slashdot.org/story/25/06/07/0714256/for-algorithms-a-little-memory-outweighs-a-lot-of-time?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749314040,"author":"EditorDavid","guid":300,"unread":true,"content":"MIT comp-sci professor Ryan Williams suspected that a small amount of memory \"would be as helpful as a lot of time in all conceivable computations...\" writes Quanta magazine. \n\n\"In February, he finally posted his proof online, to widespread acclaim...\"\n\n\nEvery algorithm takes some time to run, and requires some space to store data while it's running. Until now, the only known algorithms for accomplishing certain tasks required an amount of space roughly proportional to their runtime, and researchers had long assumed there's no way to do better. Williams' proof established a mathematical procedure for transforming any algorithm â€” no matter what it does â€” into a form that uses much less space. \nWhat's more, this result â€” a statement about what you can compute given a certain amount of space â€” also implies a second result, about what you cannot compute in a certain amount of time. This second result isn't surprising in itself: Researchers expected it to be true, but they had no idea how to prove it. Williams' solution, based on his sweeping first result, feels almost cartoonishly excessive, akin to proving a suspected murderer guilty by establishing an ironclad alibi for everyone else on the planet. It could also offer a new way to attack one of the oldest open problems in computer science. \n\n\"It's a pretty stunning result, and a massive advance,\" said Paul Beame, a computer scientist at the University of Washington.\n \n\n\nThanks to long-time Slashdot reader mspohr for sharing the article.\n","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What To Expect From Apple's WWDC","url":"https://apple.slashdot.org/story/25/06/06/2147236/what-to-expect-from-apples-wwdc?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749251400,"author":"BeauHD","guid":299,"unread":true,"content":"Apple's Worldwide Developers Conference 25 (WWDC) kicks off next week, June 9th, showcasing the company's latest software and new technologies. That includes the next version of iOS, which is rumored to have the most significant design overhaul since the introduction of iOS 7. Here's an overview of what to expect: \nMajor Software Redesigns \nApple plans to shift its operating system naming to reflect the release year, moving from sequential numbers to year-based identifiers. Consequently, the upcoming releases will be labeled as iOS 26, macOS 26, watchOS 26, etc., streamlining the versioning across platforms.\n \niOS 26 is anticipated to feature a glossy, glass-like interface inspired by visionOS, incorporating translucent elements and rounded buttons. This design language is expected to extend across iPadOS, macOS, watchOS, and tvOS, promoting a cohesive user experience across devices. Core applications like Phone, Safari, and Camera are slated for significant redesigns, too. For instance, Safari may introduce a translucent, \"glassy\" address bar, aligning with the new visual aesthetics.\n \nWhile AI is not expected to be the main focus due to Siri's current readiness, some AI-related updates are rumored. The Shortcuts app may gain \"Apple Intelligence,\" enabling users to create shortcuts using natural language. It's also possible that Gemini will be offered as an option for AI functionalities on the iPhone, similar to ChatGPT.\n \nOther App and Feature Updates \nThe lock screen might display charging estimates, indicating how long it will take for the phone to fully charge. There's a rumor about bringing live translation features to AirPods. The Messages app could receive automatic translations and call support; the Music app might introduce full-screen animated lock screen art; and Apple Notes may get markdown support. Users may also only need to log into a captive Wi-Fi portal once, and all their devices will automatically be logged in.\n \nSignificant updates are expected for Apple Home. There's speculation about the potential announcement of a \"HomePad\" with a screen, Apple's competitor to devices like the Nest Hub Mini. A new dedicated Apple gaming app is also anticipated to replace Game Center.\n\nIf you're expecting new hardware, don't hold your breath. The event is expected to focus primarily on software developments. It may even see discontinued support for several older Intel-based Macs in macOS 26, including models like the 2018 MacBook Pro and the 2019 iMac, as Apple continues its transition towards exclusive support for Apple Silicon devices.\n \nSources:\nApple WWDC 2025 Rumors and Predictions! (Waveform)\nWWDC 2025 Overview (MacRumors)\nWWDC 2025: What to expect from this year's conference (TechCrunch)\nWhat to expect from Apple's Worldwide Developers Conference next week (Ars Technica)\nApple's WWDC 2025: How to Watch and What to Expect (Wired)","contentLength":2893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Andrew Ng Says Vibe Coding is a Bad Name For a Very Real and Exhausting Job","url":"https://developers.slashdot.org/story/25/06/05/165258/andrew-ng-says-vibe-coding-is-a-bad-name-for-a-very-real-and-exhausting-job?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749139500,"author":"msmash","guid":298,"unread":true,"content":"An anonymous reader shares a report: Vibe coding might sound chill, but Andrew Ng thinks the name is unfortunate. The Stanford professor and former Google Brain scientist said the term misleads people into imagining engineers just \"go with the vibes\" when using AI tools to write code. \"It's unfortunate that that's called vibe coding,\" Ng said at a firechat chat in May at conference LangChain Interrupt. \"It's misleading a lot of people into thinking, just go with the vibes, you know -- accept this, reject that.\" \n\nIn reality, coding with AI is \"a deeply intellectual exercise,\" he said. \"When I'm coding for a day with AI coding assistance, I'm frankly exhausted by the end of the day.\" Despite his gripe with the name, Ng is bullish on AI-assisted coding. He said it's \"fantastic\" that developers can now write software faster with these tools, sometimes while \"barely looking at the code.\"","contentLength":896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code.org Changes Mission To 'Make CS and AI a Core Part of K-12 Education'","url":"https://news.slashdot.org/story/25/06/04/1323220/codeorg-changes-mission-to-make-cs-and-ai-a-core-part-of-k-12-education?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1749045600,"author":"msmash","guid":297,"unread":true,"content":"theodp writes: Way back in 2010, Microsoft and Google teamed with nonprofit partners to launch Computing in the Core, an advocacy coalition whose mission was \"to strengthen computing education and ensure that it is a core subject for students in the 21st century.\" In 2013, Computing in the Core was merged into Code.org, a new tech-backed-and-directed nonprofit. And in 2015, Code.org declared 'Mission Accomplished' with the passage of the Every Student Succeeds Act, which elevated computer science to a core academic subject for grades K-12. \n\nFast forward to June 2025 and Code.org has changed its About page to reflect a new AI mission that's near-and-dear to the hearts of Code.org's tech giant donors and tech leader Board members: \"Code.org is a nonprofit working to make computer science (CS) and artificial intelligence (AI) a core part of K-12 education for every student.\" The mission change comes as tech companies are looking to chop headcount amid the AI boom and just weeks after tech CEOs and leaders launched a new Code.org-orchestrated national campaign to make CS and AI a graduation requirement.","contentLength":1117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","slashdot"]}