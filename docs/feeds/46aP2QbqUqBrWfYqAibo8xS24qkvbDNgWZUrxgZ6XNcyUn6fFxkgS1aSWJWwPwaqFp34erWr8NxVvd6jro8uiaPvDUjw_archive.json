{"id":"46aP2QbqUqBrWfYqAibo8xS24qkvbDNgWZUrxgZ6XNcyUn6fFxkgS1aSWJWwPwaqFp34erWr8NxVvd6jro8uiaPvDUjw","title":"top scoring links : kubernetes","displayTitle":"Reddit - Kubernetes","url":"https://www.reddit.com/r/kubernetes/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/kubernetes/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Gateway Api without real ip in the logs","url":"https://www.reddit.com/r/kubernetes/comments/1ll6v5n/gateway_api_without_real_ip_in_the_logs/","date":1750960522,"author":"/u/Jeremymr2","guid":618,"unread":true,"content":"<p>Hello, kubernetes community! I am starting this adventure in the world of kubernetes, and I am currently building a cluster where it will be the future testing environment if everything goes well. Currently, I have backend and frontend set up as service clusterip. I have the metallb that exposes a Trafik Gatewayapi. I managed to connect everything satisfactorily, but the problem that arose was that the Trafik logs showed the IP from '10.244.1.1' and not the real IP of the user who was entering the service. Does anyone know how I could fix this? Isn't there a way to do it? </p>","contentLength":579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is it just me or is eBPF configuration becoming a total shitshow?","url":"https://www.reddit.com/r/kubernetes/comments/1ll3bwq/is_it_just_me_or_is_ebpf_configuration_becoming_a/","date":1750952343,"author":"/u/Tiny_Habit5745","guid":623,"unread":true,"content":"<p>Seriously, what's happening with eBPF configs lately? </p><p>Getting PRs with random eBPF programs copy-pasted from Medium articles, zero comments, and when I ask \"what does this actually do?\" I get \"it's for observability\" like that explains anything. </p><p>Had someone deploy a Falco rule monitoring every syscall on the cluster. Performance tanked, took 3 hours to debug, and their response was \"but the tutorial said it was best practice.\" </p><p>Another team just deployed some Cilium eBPF config into prod because \"it worked in kind.\" Now we have packet drops and nobody knows why because nobody actually understands what they deployed. </p><p>When did everyone become an eBPF expert? Last month half these people didn't know what a syscall was. </p><p>Starting to think we need to treat eBPF like Helm charts - proper review, testing, docs. But apparently I'm an asshole for suggesting we shouldn't just YOLO kernel-level code into production. </p><p>Anyone else dealing with this? How do you stop people from cargo-culting eBPF configs? </p><p>Feels like early Kubernetes when people deployed random YAML from Stack Overflow.</p>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Helm chart testing","url":"https://www.reddit.com/r/kubernetes/comments/1ll0ame/helm_chart_testing/","date":1750944874,"author":"/u/calm-machine-beater","guid":620,"unread":true,"content":"<p>For all the Helm users here: are you using some kind of testing framework to perform unit testing on your helm charts? If so, do you deem it reliable?</p>","contentLength":150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1lkw82j/weekly_this_week_i_learned_twil_thread/","date":1750932058,"author":"/u/gctaylor","guid":619,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking for an Open Source Kubernetes Replication Tool for Periodic Cluster Sync (Disaster Recovery Use Case)","url":"https://www.reddit.com/r/kubernetes/comments/1lktgyy/looking_for_an_open_source_kubernetes_replication/","date":1750921143,"author":"/u/Tulpar007","guid":621,"unread":true,"content":"<p>I have 2 Kubernetes clusters: one is production, the other is a standby. I want to periodically replicate all data (pods, PVCs, configs, etc.) from the prod cluster to the standby cluster.</p><p>Goal: if prod goes down, the standby can quickly take over with minimal data loss.</p><p>Looking for an open source tool that supports:</p><ul><li>PVC + resource replication</li></ul><p>So far I’ve seen: Velero, VolSync, TrilioVault CE, Stash — any recommendations or real-world experiences?</p>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ArgoCD deploying sensitive non-Secrets","url":"https://www.reddit.com/r/kubernetes/comments/1lkmgnf/argocd_deploying_sensitive_nonsecrets/","date":1750898341,"author":"/u/nullvar2000","guid":622,"unread":true,"content":"<p>Happy Wednesday fellow Kubernetes enthusiasts! I have a homelab cluster that I've spent quite a bit of time learning and implementing Gitops using ArgoCD. I'm still planning out my secrets management, but I've run into a question that's somewhat related. How do I manage sensitive parameters in non-secrets? I'm talking about things like hostnames, domains, IP addresses, etc. </p><p>For example, ingresses have my purchased domain included and even though I'm only using internal DNS records for them, I'd rather not have that kind of information public on Github.</p><p>After some research, it would seem FluxCD has a post build variable substitution capability that could take care of this, but I'd like to find a solution using Kustomize or ArgoCD. Does anybody have another solution to this kind of data? Am I just being too paranoid about this?</p>","contentLength":836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","k8s"]}