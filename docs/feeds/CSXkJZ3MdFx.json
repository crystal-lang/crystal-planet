{"id":"CSXkJZ3MdFx","title":"Dev News","displayTitle":"Dev News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":65,"items":[{"title":"Gateway Api without real ip in the logs","url":"https://www.reddit.com/r/kubernetes/comments/1ll6v5n/gateway_api_without_real_ip_in_the_logs/","date":1750960522,"author":"/u/Jeremymr2","guid":618,"unread":true,"content":"<p>Hello, kubernetes community! I am starting this adventure in the world of kubernetes, and I am currently building a cluster where it will be the future testing environment if everything goes well. Currently, I have backend and frontend set up as service clusterip. I have the metallb that exposes a Trafik Gatewayapi. I managed to connect everything satisfactorily, but the problem that arose was that the Trafik logs showed the IP from '10.244.1.1' and not the real IP of the user who was entering the service. Does anyone know how I could fix this? Isn't there a way to do it? </p>","contentLength":579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Gemma 3n","url":"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/","date":1750957423,"author":"bundie","guid":241,"unread":true,"content":"<img src=\"https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemma3n_Wagtial_RD1-V02.original.jpg\" alt=\"Introducing Gemma 3n: The developer guide\"><div><p data-block-key=\"0lwbc\">The <a href=\"https://blog.google/technology/developers/gemma-open-models/\">first Gemma model</a> launched early last year and has since grown into a thriving <a href=\"https://deepmind.google/models/gemma/gemmaverse/\">Gemmaverse</a> of over 160 million collective downloads. This ecosystem includes our family of over a dozen specialized models for everything from safeguarding to medical applications and, most inspiringly, the countless innovations from the community. From innovators like <a href=\"https://deepmind.google/models/gemma/gemmaverse/roboflow/\">Roboflow</a> building enterprise computer vision to the <a href=\"https://deepmind.google/models/gemma/gemmaverse/gemma-2-llama-swallow/\">Institute of Science Tokyo</a> creating highly-capable Japanese Gemma variants, your work has shown us the path forward.</p><p data-block-key=\"8lqqe\">Building on this incredible momentum, we're excited to announce the full release of Gemma 3n. While <a href=\"https://developers.googleblog.com/en/introducing-gemma-3n/\">last month's preview</a> offered a glimpse, today unlocks the full power of this mobile-first architecture. Gemma 3n is designed for the developer community that helped shape Gemma. It’s supported by your favorite tools including Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama, MLX, and many others, enabling you to fine-tune and deploy for your specific on-device applications with ease. This post is the developer deep dive: we'll explore some of the innovations behind Gemma 3n, share new benchmark results, and show you how to start building today.</p><p data-block-key=\"10f0e\">Gemma 3n represents a major advancement for on-device AI, bringing powerful multimodal capabilities to edge devices with performance previously only seen in last year's cloud-based frontier models.</p></div><div><ul><li data-block-key=\"b4rlm\"> Gemma 3n natively supports image, audio, video, and text inputs and text outputs.</li></ul><ul><li data-block-key=\"belt8\"> Engineered with a focus on efficiency, Gemma 3n models are available in two sizes based on <a href=\"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/#per-layer-embeddings-(ple):-unlocking-more-memory-efficiency\"></a> parameters: E2B and E4B. While their raw parameter count is 5B and 8B respectively, architectural innovations allow them to run with a memory footprint comparable to traditional 2B and 4B models, operating with as little as 2GB (E2B) and 3GB (E4B) of memory.</li></ul><ul><li data-block-key=\"19kb\"><b>Groundbreaking architecture:</b> At its core, Gemma 3n features novel components like the MatFormer architecture for compute flexibility, Per Layer Embeddings (PLE) for memory efficiency, and new audio and MobileNet-v5 based vision encoders optimized for on-device use cases.</li></ul><ul><li data-block-key=\"83m4e\"> Gemma 3n delivers quality improvements across multilinguality (supporting 140 languages for text and multimodal understanding of 35 languages), math, coding, and reasoning. The E4B version achieves an LMArena score over 1300, making it the first model under 10 billion parameters to reach this benchmark.</li></ul></div><div><p data-block-key=\"0lwbc\">Achieving this leap in on-device performance required rethinking the model from the ground up. The foundation is Gemma 3n’s unique mobile-first architecture, and it all starts with MatFormer.</p><p data-block-key=\"26ehe\">At the core of Gemma 3n is the <a href=\"https://arxiv.org/abs/2310.07707\"></a><b> (🪆Matryoshka Transformer)</b>, a novel nested transformer built for elastic inference. Think of it like Matryoshka dolls: a larger model contains smaller, fully functional versions of itself. This approach extends the concept of <a href=\"https://huggingface.co/papers/2205.13147\">Matryoshka Representation Learning</a> from just embeddings to all transformer components.</p></div><div><p data-block-key=\"0lwbc\">During the MatFormer training of the 4B effective parameter (E4B) model, a 2B effective parameter (E2B) sub-model is simultaneously optimized within it, as shown in the figure above. This provides developers two powerful capabilities and use cases today:</p><p data-block-key=\"2r57\">1: You can directly download and use either the main E4B model for the highest capabilities, or the standalone E2B sub-model which we have already extracted for you, offering up to 2x faster inference.</p><p data-block-key=\"dsd6a\">2:<b> Custom sizes with Mix-n-Match:</b> For more granular control tailored to specific hardware constraints, you can create a spectrum of custom-sized models between E2B and E4B using a method we call Mix-n-Match. This technique allows you to precisely slice the E4B model's parameters, primarily by adjusting the feed forward network hidden dimension per layer (from 8192 to 16384) and selectively skipping some layers. We are releasing the <a href=\"https://goo.gle/gemma3n-matformer-lab\">MatFormer Lab</a>, a tool that shows how to retrieve these optimal models, which were identified by evaluating various settings on benchmarks like MMLU.</p></div><div><p data-block-key=\"0lwbc\">Looking ahead, the MatFormer architecture also paves the way for. While not part of today’s launched implementations, this capability allows a single deployed E4B model to dynamically switch between E4B and E2B inference paths on the fly, enabling real-time optimization of performance and memory usage based on the current task and device load.</p><h2 data-block-key=\"xe7xu\">Per-Layer Embeddings (PLE): Unlocking more memory efficiency</h2><p data-block-key=\"2ilaf\">Gemma 3n models incorporate <b>Per-Layer Embeddings (PLE)</b>. This innovation is tailored for on-device deployment as it dramatically improves model quality without increasing the high-speed memory footprint required on your device's accelerator (GPU/TPU).</p><p data-block-key=\"i1dh\">While the Gemma 3n E2B and E4B models have a total parameter count of 5B and 8B respectively, PLE allows a significant portion of these parameters (the embeddings associated with each layer) to be loaded and computed efficiently on the CPU. This means only the core transformer weights (approximately 2B for E2B and 4B for E4B) need to sit in the typically more constrained accelerator memory (VRAM).</p></div><div><h2 data-block-key=\"5alsx\">KV Cache sharing: Faster long-context processing</h2><p data-block-key=\"6472b\">Processing long inputs, such as the sequences derived from audio and video streams, is essential for many advanced on-device multimodal applications. Gemma 3n introduces KV Cache Sharing, a feature designed to significantly accelerate time-to-first-token for streaming response applications.</p><p data-block-key=\"1tdep\">KV Cache Sharing optimizes how the model handles the initial input processing stage (often called the \"prefill\" phase). The keys and values of the middle layer from local and global attention are directly shared with all the top layers, delivering a notable 2x improvement on prefill performance compared to Gemma 3 4B. This means the model can ingest and understand lengthy prompt sequences much faster than before.</p><h2 data-block-key=\"pctk9\">Audio understanding: Introducing speech to text and translation</h2><p data-block-key=\"uijl\">Gemma 3n uses an advanced audio encoder based on the <a href=\"https://arxiv.org/abs/2303.01037\">Universal Speech Model (USM)</a>. The encoder generates a token for every 160ms of audio (about 6 tokens per second), which are then integrated as input to the language model, providing a granular representation of the sound context.</p><p data-block-key=\"dlc74\">This integrated audio capability unlocks key features for on-device development, including:</p><ul><li data-block-key=\"6mapr\"><b>Automatic Speech Recognition (ASR):</b> Enable high-quality speech-to-text transcription directly on the device.</li></ul><ul><li data-block-key=\"caq44\"><b>Automatic Speech Translation (AST):</b> Translate spoken language into text in another language.</li></ul><p data-block-key=\"doduj\">We've observed particularly strong AST results for translation between English and Spanish, French, Italian, and Portuguese, offering great potential for developers targeting applications in these languages. For tasks like speech translation, leveraging Chain-of-Thought prompting can significantly enhance results. Here’s an example:</p></div><div><pre><code>&lt;bos&gt;&lt;start_of_turn&gt;user\nTranscribe the following speech segment in Spanish, then translate it into English: \n&lt;start_of_audio&gt;&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model</code></pre></div><div><p data-block-key=\"7ree3\">At launch time, the Gemma 3n encoder is implemented to process audio clips up to 30 seconds. However, this is not a fundamental limitation. The underlying audio encoder is a streaming encoder, capable of processing arbitrarily long audios with additional long form audio training. Follow-up implementations will unlock low-latency, long streaming applications.</p><h2 data-block-key=\"m5455\">MobileNet-V5: New state-of-the-art vision encoder</h2><p data-block-key=\"3r73m\">Alongside its integrated audio capabilities, Gemma 3n features a new, highly efficient vision encoder, , delivering state-of-the-art performance for multimodal tasks on edge devices.</p><p data-block-key=\"1qj1b\">Designed for flexibility and power on constrained hardware, MobileNet-V5 gives developers:</p><ul><li data-block-key=\"c1qs0\"><b>Multiple input resolutions</b>: Natively supports resolutions of 256x256, 512x512, and 768x768 pixels, allowing you to balance performance and detail for your specific applications.</li></ul><ul><li data-block-key=\"648dt\"><b>Broad visual understanding</b>: Co-trained on extensive multimodal datasets, it excels at a wide range of image and video comprehension tasks.</li></ul><ul><li data-block-key=\"k1qf\">: Processes up to 60 frames per second on a Google Pixel, enabling real-time, on-device video analysis and interactive experiences.</li></ul><p data-block-key=\"1a8ge\">This level of performance is achieved with multiple architectural innovations, including:</p><ul><li data-block-key=\"48us4\">An advanced foundation of MobileNet-V4 blocks (including Universal Inverted Bottlenecks and Mobile MQA).</li></ul><ul><li data-block-key=\"12p8\">A significantly scaled up architecture, featuring a hybrid, deep pyramid model that is 10x larger than the biggest MobileNet-V4 variant.</li></ul><ul><li data-block-key=\"73gq1\">A novel Multi-Scale Fusion VLM adapter that enhances the quality of tokens for better accuracy and efficiency.</li></ul><p data-block-key=\"49ved\">Benefiting from novel architectural designs and advanced distillation techniques, MobileNet-V5-300M substantially outperforms the baseline SoViT in Gemma 3 (trained with SigLip, no distillation). On a Google Pixel Edge TPU, it <b>delivers a 13x speedup with quantization (6.5x without), requires 46% fewer parameters, and has a 4x smaller memory footprint</b>, all while providing significantly higher accuracy on vision-language tasks</p><p data-block-key=\"e7u3j\">We’re excited to share more about the work behind this model. Look out for our upcoming MobileNet-V5 technical report, which will deep dive into the model architecture, data scaling strategies, and advanced distillation techniques.</p><p data-block-key=\"emlv5\">Making Gemma 3n accessible from day one has been a priority. We're proud to partner with many incredible open source developers to ensure broad support across popular tools and platforms, including contributions from teams behind AMD, Axolotl, Docker, Hugging Face, llama.cpp, LMStudio, MLX, NVIDIA, Ollama, RedHat, SGLang, Unsloth, and vLLM.</p><p data-block-key=\"354di\">But this ecosystem is just the beginning. The true power of this technology is in what you will build with it. That’s why we’re launching the <a href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\">Gemma 3n Impact Challenge.</a> Your mission: use Gemma 3n's unique on-device, offline, and multimodal capabilities to build a product for a better world. With $150,000 in prizes, we're looking for a compelling video story and a \"wow\" factor demo that shows real-world impact. <a href=\"https://www.kaggle.com/competitions/google-gemma-3n-hackathon\">Join the challenge</a> and help build a better future.</p><h2 data-block-key=\"tra7k\">Get started with Gemma 3n today</h2><p data-block-key=\"9t8b1\">Ready to explore the potential of Gemma 3n today? Here's how:</p><ul><li data-block-key=\"cvr4m\"> Use <a href=\"https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it\">Google AI Studio</a> to try Gemma 3n in just a couple of clicks. Gemma models can also be deployed directly to Cloud Run from AI Studio.</li></ul><ul><li data-block-key=\"1va1s\"> Dive into our <a href=\"https://ai.google.dev/gemma/docs/gemma-3n\">comprehensive documentation</a> to quickly integrate Gemma into your projects or start with our inference and fine-tuning guides.</li></ul></div>","contentLength":10319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44389202"},{"title":"Rust in Production at 1Password: 500k lines of Rust, 600 crates, 100 engineers - How they secure millions of passwords","url":"https://corrode.dev/podcast/s04e06-1password/","date":1750954316,"author":"/u/mre__","guid":639,"unread":true,"content":"<p>Handling secrets is extremely hard.\nYou have to keep them safe (obviously), while at the same time you need to integrate with a ton of different systems and always provide a great user-experience, because otherwise people will just find a way around your system.\nWhen talking to peers, a lot of people mention 1Password as a company that nailed this balance.</p><p>In today’s episode, I talk to Andrew about how 1Password uses Rust to build critical systems that must never fail, how Rust helps them handle secrets for millions of users, and the lessons they learned when adopting Rust in their stack.</p><div><p>\n    CodeCrafters helps you become proficient in Rust by building real-world,\n    production-grade projects. Learn hands-on by creating your own shell, HTTP\n    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.\n  </p><p>\n    Start for free today and enjoy 40% off any paid plan by using\n    <a href=\"https://app.codecrafters.io/join?via=mre\">this link</a>.\n  </p></div><p>1Password is a password manager that helps users securely store and manage their passwords, credit card information, and other sensitive data. It provides a user-friendly interface and strong security features to protect users’ secrets across multiple devices.</p><p>Andrew is a Senior Rust Developer at 1Password in the Product Foundations org, on the Frameworks team and specifically on the Core Platform squad handling the asynchronous frameworks other developers use to build features (i.e. requests into the Rust core from the Native clients, data sync, etc.).\nHe specifically specialized in that synchronization process, getting data federated from cloud to clients to native apps and back.</p><ul><li><a href=\"https://github.com/1Password/typeshare\">typeshare</a> - Generate types for multiple languages from Rust code</li><li><a href=\"https://github.com/1Password/zeroizing-alloc\">zeroizing-alloc</a> - 1Password’s minimal secure heap zero-on-free implementation for Rust</li><li><a href=\"https://github.com/1Password/arboard\">arboard</a> - Cross-platform clipboard manager written in Rust</li><li><a href=\"https://github.com/1password/passkey-rs\">passkey-rs</a> - Pure Rust implementation of the WebAuthn Passkey specification</li><li><a href=\"https://tokio.rs/\">tokio</a> - The de facto standard async runtime for Rust</li><li><a href=\"https://github.com/rust-lang/rust-clippy\">Clippy</a> - A collection of lints to catch common mistakes in Rust</li><li><a href=\"https://github.com/EmbarkStudios/cargo-deny\">cargo-deny</a> - Cargo plugin for linting dependencies, licenses, and security advisories</li><li><a href=\"https://nixos.org/\">Nix</a> - Purely functional package manager for reproducible builds</li><li><a href=\"https://nixos.wiki/wiki/Flakes\">Nix Flakes</a> - Experimental feature for hermetic, reproducible Nix builds</li><li><a href=\"https://direnv.net/\">direnv</a> - Load and unload environment variables based on current directory</li><li><a href=\"https://github.com/tokio-rs/axum\">axum</a> - Ergonomic and modular web framework built on tokio and tower</li><li><a href=\"https://github.com/tower-rs/tower\">tower</a> - Library for building robust networking clients and servers</li><li><a href=\"https://github.com/tokio-rs/tracing\">tracing</a> - Application-level tracing framework for async-aware diagnostics</li><li><a href=\"https://github.com/rusqlite/rusqlite\">rusqlite</a> - Ergonomic wrapper for SQLite in Rust</li><li><a href=\"https://docs.rs/mockall/latest/mockall/\">mockall</a> - Powerful mock object library for Rust</li><li><a href=\"https://neon-rs.dev/\">neon</a> - Library for writing native Node.js modules in Rust</li><li><a href=\"https://docs.rs/nom-supreme/latest/nom_supreme/\">nom-supreme</a> - Parser combinator additions and utilities for nom</li><li><a href=\"https://github.com/ipetkov/crane\">crane</a> - Nix library for building Cargo projects</li><li><a href=\"https://github.com/rust-lang/rustlings\">Rustlings</a> - Small exercises to get you used to reading and writing Rust code</li></ul>","contentLength":2833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ll46he/rust_in_production_at_1password_500k_lines_of/"},{"title":"Is it just me or is eBPF configuration becoming a total shitshow?","url":"https://www.reddit.com/r/kubernetes/comments/1ll3bwq/is_it_just_me_or_is_ebpf_configuration_becoming_a/","date":1750952343,"author":"/u/Tiny_Habit5745","guid":623,"unread":true,"content":"<p>Seriously, what's happening with eBPF configs lately? </p><p>Getting PRs with random eBPF programs copy-pasted from Medium articles, zero comments, and when I ask \"what does this actually do?\" I get \"it's for observability\" like that explains anything. </p><p>Had someone deploy a Falco rule monitoring every syscall on the cluster. Performance tanked, took 3 hours to debug, and their response was \"but the tutorial said it was best practice.\" </p><p>Another team just deployed some Cilium eBPF config into prod because \"it worked in kind.\" Now we have packet drops and nobody knows why because nobody actually understands what they deployed. </p><p>When did everyone become an eBPF expert? Last month half these people didn't know what a syscall was. </p><p>Starting to think we need to treat eBPF like Helm charts - proper review, testing, docs. But apparently I'm an asshole for suggesting we shouldn't just YOLO kernel-level code into production. </p><p>Anyone else dealing with this? How do you stop people from cargo-culting eBPF configs? </p><p>Feels like early Kubernetes when people deployed random YAML from Stack Overflow.</p>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Firefox 141 Beta Lowering RAM Use On Linux But Still Benchmarking Behind Chrome","url":"https://www.phoronix.com/review/firefox-141-linux-ram","date":1750952151,"author":"/u/lebron8","guid":626,"unread":true,"content":"<p>Following this week's release of <a href=\"https://www.phoronix.com/news/Firefox-140\">Firefox 140</a>, Firefox 141 was promoted to beta. Most exciting for Linux users with next month's Firefox 141 release is finally lowering system RAM use! I've been running some benchmarks looking at the impact.</p><p>With this week's release of <a href=\"https://www.mozilla.org/en-US/firefox/141.0beta/releasenotes/\">Firefox 141 Beta</a>, it notes an exciting change for Linux users:</p><blockquote>\"On Linux Firefox uses less memory and no longer requires a forced restart after an update has been applied by a package manager.\"</blockquote><p>Less memory use by Firefox on Linux is certainly welcome as it's become quite bloated... I've grown frustrated myself with the direction of Firefox and the insane RAM use in recent times. While being a long time Firefox user since the Firebird days, I've contemplated switching to Chrome given the rampant memory use of Firefox on Linux with my 64GB RAM laptop triggering the OOM daemon routinely due to excessive memory use with Firefox.</p><p>Thus with the Firefox 141 Beta release, I was curious to run some benchmarks of Firefox 140 vs. Firefox 141 Beta on a test system for seeing the difference.</p><p>With this testing I was just looking at the single-tab/window impact for reproducibility while running various web browser benchmarks but that alone was enough to show a nice improvement over Firefox 140.</p><p>This round of benchmarks were done on an AMD Ryzen 9 9950X desktop running Ubuntu 25.04 with the Linux 6.15 kernel and NVIDIA graphics.</p>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ll390m/firefox_141_beta_lowering_ram_use_on_linux_but/"},{"title":"FLUX.1 Kontext [Dev] – Open Weights for Image Editing","url":"https://bfl.ai/announcements/flux-1-kontext-dev","date":1750951918,"author":"minimaxir","guid":240,"unread":true,"content":"<p>Up until today, all capable generative image editing models were only available as proprietary tools. Today, that changes. We release FLUX.1 Kontext [dev], our developer version of <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://bfl.ai/models/flux-kontext\"></a>, which delivers proprietary-level image editing performance in a 12B parameter model that can run on consumer hardware.</p><p>Making model weights openly accessible is fundamental to technological innovation. <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev\"></a> is now available as an open-weight model under the FLUX.1 Non-Commercial License, providing free access for research and non-commercial use. FLUX.1 Kontext [dev] is compatible with the existing FLUX.1 [dev] inference code and comes with day-0 support for popular inference frameworks like ComfyUI, HuggingFace Diffusers and TensorRT.</p><p>The technical report is available <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2506.15742\"></a>.</p><h2>Setting New Standards in Open Image Editing</h2><p>FLUX.1 Kontext [dev] focuses exclusively on editing tasks. The model enables iterative editing, excels at character preservation across a diverse set of scenes and environments, and allows both precise local and global edits.</p><p>At Black Forest Labs, we remain committed to providing researchers and developers with best-in-class open tools that are competitive with existing proprietary solutions. To validate the performance of FLUX.1 Kontext [dev], we conducted extensive evaluation across multiple image editing benchmarks.</p><p>Human preference evaluations on <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://huggingface.co/datasets/black-forest-labs/kontext-bench\"></a>, our newly released image editing benchmark, demonstrate that FLUX.1 Kontext [dev] outperforms existing open image editing models, (Bytedance Bagel, HiDream-E1-Full) and closed models (Google's Gemini-Flash Image) across many categories. Independent evaluations run by <a target=\"_self\" rel=\"noopener noreferrer\" href=\"http://artificialanalysis.ai/text-to-image/arena?tab=leaderboard&amp;input=image\"></a> confirm these findings.</p><h2>Optimized for NVIDIA Blackwell Architecture</h2><p>We have collaborated with NVIDIA to build optimized TensorRT weights specifically designed for the new <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/?ncid=pa-srch-goog-139553\"></a> architecture which brings greatly improved inference speed and reduces memory usage while maintaining high-quality image editing performance.</p><p>Additionally to the original FLUX.1 Kontext [dev] weights, we’re making available these BF16, FP8 and FP4 TensorRT variants in our <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev-onnx\"></a>, giving developers the flexibility to balance speed, efficiency, and quality tailored to their use case. These optimized weights ensure that FLUX.1 Kontext [dev] can take full advantage of the latest hardware capabilities.</p><h2>Streamlined Commercial Access: The BFL Self-Serve Portal</h2><p>We are releasing a <a target=\"_self\" rel=\"noopener noreferrer\" href=\"http://bfl.ai/pricing/licensing\"></a> with transparent terms and standardized commercials for simplifying commercial access to all of our open weights models. This includes the novel FLUX.1 Kontext [dev] as well as the FLUX.1 Tools [dev] and the popular text-to-image model FLUX.1 [dev].</p><p>Our self-serve portal provides transparent licensing terms that enable businesses to confidently integrate FLUX.1 models into their commercial products and services. Commercial Licenses to our open weights models can now be purchased with only a few clicks, accelerating the path from development to deployment. More information on self-serve licensing can be found at the <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://help.bfl.ai/collections/6939000511-licensing\"></a>.</p><ol><li> We edited the definition of “Non-Commercial Purpose” to better clarify what constitutes Non-Commercial Purposes under the FLUX.1 [dev] Non-Commercial License.</li><li> To prevent the creation and dissemination of unlawful or infringing content, the FLUX.1 [dev] Non-Commercial License requires content filters or manual review to be used with the FLUX.1 [dev] models. We’ve also made corresponding adjustments to the indemnification of the license.</li><li>Users of FLUX.1 [dev] Models under a FLUX.1 [dev] Non-Commercial License must follow applicable law for content provenance under the license.</li><li> We made some clarifications on what are not permitted uses of FLUX.1 [dev] Models under a FLUX.1 [dev] Non-Commercial License.</li></ol><p><em>We're just getting started. If you want to join us on our mission, we are actively hiring talented individuals across multiple roles. Apply <a target=\"_self\" rel=\"noopener noreferrer\" href=\"https://job-boards.greenhouse.io/blackforestlabs\"></a></em></p>","contentLength":3842,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44388387"},{"title":"Learn open source machine learning with Skolar","url":"https://www.youtube.com/watch?v=70_ZuhDBvac","date":1750950006,"author":"probabl","guid":398,"unread":true,"content":"<article>Train_test your skills in open-source ML 💥 Skolar is the FREE learning platform by the makers of scikit-learn. Learn real-world machine learning, prep for certification, and go from zero to hero — the open-source way.\nGo to skolar.probabl.ai\n#probabl #datascience #machinelearning #scikitlearn #sklearn #ai</article>","contentLength":311,"flags":null,"enclosureUrl":"https://www.youtube.com/v/70_ZuhDBvac?version=3","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I built an AI dataset generator","url":"https://github.com/metabase/dataset-generator","date":1750949912,"author":"matthewhefferon","guid":223,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44388093"},{"title":"Launch HN: Issen (YC F24) – Personal AI language tutor","url":"https://news.ycombinator.com/item?id=44387828","date":1750948348,"author":"mariano54","guid":239,"unread":true,"content":"Hey HN, we're Mariano and Anton from ISSEN (<a href=\"https://issen.com\">https://issen.com</a>), a foreign language voice tutor app that adapts to your interests, goals, and needs.<p>We started this company after struggling to find great tools to practice speaking Japanese and French. Having a tutor can be awesome, but there are downsides: they can be expensive (since you pay by the hour), difficult to schedule, and have a high upfront cost (finding a tutor you like often forces you to cycle through a few that you don’t).</p><p>We wanted something that would talk with us — realistically, in full conversations — and actually help us improve. So we built it ourselves.\nThe app relies on a custom voice AI pipeline combining STT (speech-to-text), TTS (text-to-speech), LLMs, long term memory, interruptions, turn-taking, etc. Getting speech-to-text to work well for learners was one of the hardest parts — especially with accents, multi-lingual sentences, and noisy environments. We now combine Gemini Flash, Whisper, Scribe, and GPT-4o-transcribe to minimize errors and keep the conversation flowing.</p><p>We didn’t want to focus too much on gamification. In our experience, that leads to users performing well in the app, achieving long streaks and so on, without actually getting fluent in the language you're wanting to learn.</p><p>With ISSEN you instantly speak and immerse yourself in the language, which, while not easy, is a much more efficient way to learn.</p><p>We combine this with a word bank and SRS flashcards for new words learned in the AI voice chats, which allows very rapid improvement in both vocabulary and speaking skills. We also create custom curriculums for each student based on goals, interests, and preferences, and fully customizable settings like speed, turn taking, formality, etc.</p><p>App: <a href=\"https://issen.com\">https://issen.com</a> (works on web, iOS, Android)\nPricing: 20 min free trial, $20–29/month (depending on duration and specific geography)</p><p>We’d love your feedback — on the tech, the UX, or what you’d wish from a tool like this. Thanks!</p>","contentLength":2009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44387828"},{"title":"So Long, Image Layouts: Simplifying Vulkan Synchronisation","url":"https://www.khronos.org/blog/so-long-image-layouts-simplifying-vulkan-synchronisation","date":1750948092,"author":"/u/GamerY7","guid":630,"unread":true,"content":"<p>Synchronization in Vulkan® has long been one of its most notorious challenges, something developers haven’t been shy about reminding us. The Khronos® Vulkan Working Group has been steadily working to <a href=\"https://www.youtube.com/watch?v=NM-SzTHAKGo&amp;t=864s\">make Vulkan a joy to use</a>, and simplifying the synchronization model has been high on our priority list. One of the most frequent developer frustrations has been the complexity of managing image layouts, a pain point we’re tackling head-on with the new <a href=\"https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_KHR_unified_image_layouts.html\"><code>VK_KHR_unified_image_layouts</code></a> extension, which aims to eliminate the need for most layout transitions entirely.</p><p>It’s been over a decade since Vulkan’s synchronization model was first designed. At the time, the graphics development community strongly favored explicit control over synchronization, and the Vulkan Working Group responded with an API that prioritized performance on the hardware of that era, delivering maximum flexibility and control to application developers.</p><p>In hindsight—and perhaps unsurprisingly—synchronization has turned out to be one of Vulkan’s most challenging aspects, tripping up both new and seasoned developers. The synchronization model is complex enough that many applications struggle to issue synchronization commands that are efficient <a href=\"https://www.lunarg.com/wp-content/uploads/2024/01/Guide-to-Vulkan-Synchronization-Validation-FINAL-01-18-2024.pdf#page=5\">or even correct</a>. The <a href=\"https://registry.khronos.org/vulkan/specs/latest/man/html/VK_KHR_synchronization2.html\"></a> extension introduced several incremental improvements while largely preserving the foundational principles of Vulkan 1.0’s original approach.</p><p>Since then, modern GPUs have evolved significantly, including how they handle synchronization, making many of Vulkan’s original design constraints increasingly outdated. Recognizing this, the Vulkan Working Group is now actively working to streamline synchronization, aiming to improve usability and make high-performance graphics development more accessible. The newly released <code>VK_KHR_unified_image_layouts</code> extension is a major milestone on that path.</p><p>Vulkan 1.0 introduced several image layouts, with later extensions adding a few more. In practice, though, these layouts typically map to just a handful of actual physical layouts or compression formats. One notable exception is <code>VK_IMAGE_LAYOUT_UNDEFINED</code>, which isn’t a physical layout at all but instead serves to initialize internal metadata for newly created images.</p><p>Image layout transitions in Vulkan exist primarily for three reasons:</p><ol><li> – Transitions from <code>VK_IMAGE_LAYOUT_UNDEFINED</code> serve as image initialization operations.</li><li> – When transferring ownership to or from external queue families or presenting images to the display, transitions may be required to ensure compatibility with external components that don’t support Vulkan’s internal compression schemes.</li><li> – Even within the same Vulkan device, some subsystems may not understand the framebuffer compression used by others. For instance, using depth/stencil or multisampled color attachments outside of a render pass may require decompression on certain hardware.</li></ol><p>With <a href=\"https://registry.khronos.org/vulkan/specs/1.3-extensions/man/html/VK_KHR_unified_image_layouts.html\"><code>VK_KHR_unified_image_layouts</code></a>, the Vulkan Working Group recognizes that the third case— internal incompatibility—is no longer relevant for most modern GPUs. This extension allows developers to bypass the majority of layout transitions, significantly simplifying synchronization and reducing boilerplate. Better yet, nearly all GPU vendors are ready to support this extension on current-generation hardware. It’s already on the Vulkan roadmap, with the goal of including it in the core API.</p><p>At its heart, <code>VK_KHR_unified_image_layouts</code> is a simple but powerful from the driver to the application: it guarantees that  can be used efficiently in nearly all cases. Aside from a few specific scenarios, such as image initialization or presentation, <strong>developers no longer need to use layout transitions at all, just use</strong>!</p><p>Validation layer support is expected in the July Vulkan SDK. As always—and especially with this extension—we strongly recommend enabling <a href=\"https://www.lunarg.com/wp-content/uploads/2024/01/Guide-to-Vulkan-Synchronization-Validation-FINAL-01-18-2024.pdf\">Synchronization Validation</a> in the Vulkan validation layers. Several common synchronization errors are caught by mismatched image layouts, but standard validation will no longer be able to detect these if the layout is set to . </p><p>Vulkan’s core principles—explicit control and low-level access to modern GPU features—aren’t going anywhere. But the Vulkan Working Group is placing greater emphasis on developer experience. Instead of just building new features, we’re revisiting older ones to make them easier to use. <code>VK_KHR_unified_image_layouts</code> is the latest example, and more improvements are on the way.</p><p>We’re excited about a future where synchronization is no longer a source of frustration. But we can’t get there without your valuable feedback. Join the conversation on the Vulkan Discord, or leave a comment below. Let us know how this extension is working for you, what challenges you’re facing, and which pain points you’d like us to tackle next!</p>","contentLength":4814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ll1ju8/so_long_image_layouts_simplifying_vulkan/"},{"title":"Google DeepMind Releases AlphaGenome","url":"https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/","date":1750947360,"author":"i_love_limes","guid":238,"unread":true,"content":"<div><p data-block-key=\"vrizt\">Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.</p><p data-block-key=\"2ot99\">The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.</p><p data-block-key=\"3j9or\">Today, we introduce <a href=\"https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf\" rel=\"noopener\" target=\"_blank\">AlphaGenome</a>, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.</p><p data-block-key=\"166qa\">To advance scientific research, we’re making AlphaGenome available in preview via our <a href=\"https://github.com/google-deepmind/alphagenome\" rel=\"noopener\" target=\"_blank\">AlphaGenome API</a> for non-commercial research, and planning to release the model in the future.</p><p data-block-key=\"9n1f9\">We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.</p><p data-block-key=\"9pi95\">Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.</p><p data-block-key=\"6p84u\">Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including <a href=\"http://encodeproject.org/\" rel=\"noopener\" target=\"_blank\">ENCODE</a>, <a href=\"https://www.gtexportal.org/\" rel=\"noopener\" target=\"_blank\">GTEx</a>, <a href=\"https://4dnucleome.org/\" rel=\"noopener\" target=\"_blank\">4D Nucleome</a> and <a href=\"https://fantom.gsc.riken.jp/5/\" rel=\"noopener\" target=\"_blank\">FANTOM5,</a> which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.</p></div><div><p data-block-key=\"vrizt\">The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.</p><p data-block-key=\"dl5ep\">This model builds on our previous genomics model, <a href=\"https://deepmind.google/discover/blog/predicting-gene-expression-with-ai/\" rel=\"noopener\" target=\"_blank\">Enformer</a> and is complementary to <a href=\"https://deepmind.google/discover/blog/a-catalogue-of-genetic-mutations-to-help-pinpoint-the-cause-of-diseases/\" rel=\"noopener\" target=\"_blank\">AlphaMissense</a>, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.</p></div><div><h2 data-block-key=\"vrizt\">AlphaGenome’s distinctive features</h2><p data-block-key=\"ev8si\">AlphaGenome offers several distinctive features compared to existing DNA sequence models:</p><h3 data-block-key=\"2728b\">Long sequence-context at high resolution</h3><p data-block-key=\"7d57b\">Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.</p><p data-block-key=\"24mqo\">Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.</p><h3 data-block-key=\"86rg9\">Comprehensive multimodal prediction</h3><p data-block-key=\"bft7s\">By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.</p><h3 data-block-key=\"8hmh5\">Efficient variant scoring</h3><p data-block-key=\"b84ad\">In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.</p><h3 data-block-key=\"81hcn\">Novel splice-junction modeling</h3><p data-block-key=\"860mh\">Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.</p><h2 data-block-key=\"6vomo\">State-of-the-art performance across benchmarks</h2><p data-block-key=\"2hvqt\">AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.</p></div><div><p data-block-key=\"vrizt\">When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.</p><p data-block-key=\"4p9mo\">This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in <a href=\"https://storage.googleapis.com/deepmind-media/papers/alphagenome.pdf\" rel=\"noopener\" target=\"_blank\">our preprint</a>.</p><h2 data-block-key=\"f4akh\">The benefits of a unifying model</h2><p data-block-key=\"3ist1\">AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.</p><p data-block-key=\"13g3v\">Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.</p><p data-block-key=\"al5d1\">Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.</p></div><figure><blockquote><p data-block-key=\"o5onv\">It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.</p></blockquote><figcaption><p data-block-key=\"o3wzw\">Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center</p></figcaption></figure><div><p data-block-key=\"e5ukd\">AlphaGenome's predictive capabilities could help several research avenues:</p><ol><li data-block-key=\"8b7n9\"> By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.</li><li data-block-key=\"43h16\"> Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.</li><li data-block-key=\"efr5o\"> It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.</li></ol><p data-block-key=\"7gonn\">For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing <a href=\"https://www.science.org/doi/10.1126/science.1259037\" rel=\"noopener\" target=\"_blank\">study of patients with T-cell acute lymphoblastic leukemia (T-ALL)</a>, researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called <a href=\"https://alphafold.ebi.ac.uk/entry/P17542\" rel=\"noopener\" target=\"_blank\">TAL1</a> by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.</p></div><figure><blockquote><p data-block-key=\"o5onv\">AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.</p></blockquote><figcaption><p data-block-key=\"o3wzw\">Professor Marc Mansour, University College London</p></figcaption></figure><div><p data-block-key=\"9c6av\">AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.</p><p data-block-key=\"fhnk4\">Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.</p><p data-block-key=\"4lpo\">We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.</p><p data-block-key=\"7qtv9\">We’re continuing to improve our models and gathering feedback to help us address these gaps.</p><h2 data-block-key=\"cvanp\">Enabling the community to unlock AlphaGenome's potential</h2><p data-block-key=\"2ouf5\">AlphaGenome is now available for non-commercial use via our <a href=\"https://github.com/google-deepmind/alphagenome\" rel=\"noopener\" target=\"_blank\">AlphaGenome API</a>. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.</p><p data-block-key=\"2lg85\">Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the <a href=\"https://www.alphagenomecommunity.com/\" rel=\"noopener\" target=\"_blank\">community forum</a>.</p><p data-block-key=\"8fl9p\">We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.</p><p data-block-key=\"2p4vh\">Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.</p></div><section><div><div><p data-block-key=\"4d4h5\">We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.</p><p data-block-key=\"59d3j\">This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.</p><p data-block-key=\"75u3a\">We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.</p></div></div></section>","contentLength":12182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44387659"},{"title":"Helm chart testing","url":"https://www.reddit.com/r/kubernetes/comments/1ll0ame/helm_chart_testing/","date":1750944874,"author":"/u/calm-machine-beater","guid":620,"unread":true,"content":"<p>For all the Helm users here: are you using some kind of testing framework to perform unit testing on your helm charts? If so, do you deem it reliable?</p>","contentLength":150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Over 80% of all Smartphones are powered by Linux","url":"https://linuxblog.io/80-percent-smartphones-linux/","date":1750944416,"author":"/u/modelop","guid":629,"unread":true,"content":"<p data-pm-slice=\"1 1 []\">Linux-powered smartphones are still the dominant force in the global smartphone market in 2025. While some may be surprised by this fact, <a href=\"https://linuxcommunity.io/\" target=\"_blank\" rel=\"noopener\">Linux enthusiasts</a> have every reason to be happy about the continued success and influence of Linux in the mobile world.</p><p data-pm-slice=\"1 1 []\">Read on for the latest stats, updated links and proof that Linux is still at the heart of the smartphone ecosystem. This is a follow-up to my previous articles in <a href=\"https://web.archive.org/web/20131216223204/https://haydenjames.io/81-percent-smartphones-powered-by-linux/\" target=\"_blank\" rel=\"noopener\">2013</a> and <a href=\"https://web.archive.org/web/20210315133957/https://haydenjames.io/85-of-all-smartphones-are-powered-by-linux/\" target=\"_blank\" rel=\"noopener\">2021</a>.</p><p>Google formalized their efforts to keep Android closer to upstream Linux by introducing the <a href=\"https://source.android.com/docs/core/architecture/kernel/generic-kernel-image\" target=\"_blank\" rel=\"noopener\">Generic Kernel Image (GKI)</a> project in 2019. Since then, the GKI project has matured a lot, with ongoing upstream kernel development focused on reducing fragmentation, upstreaming Android-specific features and establishing a stable <a href=\"https://source.android.com/docs/core/architecture/kernel/stable-kmi\" target=\"_blank\" rel=\"noopener\">Kernel Module Interface (KMI)</a>.</p><p>All of which has strengthened Android’s connection to the mainline Linux kernel and fostered closer collaboration between Google, device vendors, and the Linux community.</p><h2>Android and Chrome OS: Still Linux at the Core</h2><p><a href=\"https://www.android.com/\" target=\"_blank\" rel=\"nofollow noopener\">Google’s Android</a> and <a href=\"https://www.google.com/chromebook/chrome-os/\" target=\"_blank\" rel=\"nofollow noopener\">Chrome OS</a> are operating systems originally based on the <a href=\"https://linuxblog.io/best-linux-distro/\" target=\"_blank\" rel=\"nofollow noopener\">Linux kernel</a>. Android, in particular, has continued to evolve, now running on long-term support (LTS) Linux kernels, such as versions 4.19, 5.4, and more recently, 6.x series, with regular merges from the Linux mainline into the Android-common kernel tree.</p><h2>2025 Smartphone Market Share: Linux Still Leads</h2><p>Recent <a href=\"https://canalys.com/newsroom/worldwide-smartphone-market-q1-2025\" target=\"_blank\" rel=\"nofollow noopener\">Canalys (Omdia) research</a> shows that in Q1 2025, the global smartphone market shipped 296.9 million units, with Android-based devices continuing to dominate. Samsung led with 60.5 million units (20% market share), followed by Apple with 55.0 million units (19%). <a href=\"https://en.wikipedia.org/wiki/MIUI\" target=\"_blank\" rel=\"noopener\">Xiaomi</a>, <a href=\"https://en.wikipedia.org/wiki/Funtouch_OS\" target=\"_blank\" rel=\"noopener\">vivo</a>, and <a href=\"https://en.wikipedia.org/wiki/ColorOS\" target=\"_blank\" rel=\"noopener\">OPPO</a> rounded out the top five.</p><p><em><strong>Combining Android, Xiaomi, Vivo, OPPO, and other Android-based OSes,</strong></em><em><strong>Linux powers around 80% of the global smartphone market.</strong></em></p><p>Android’s share remains robust, with estimates consistently placing it at over 80% of global smartphone shipments. Yup, more than a decade after my first article on this, the vast majority of smartphones in use today are still powered by the Linux kernel.</p><h2>Alternative Linux Smartphones in 2025</h2><p>While Android dominates, several alternative Linux-based smartphones continue to attract enthusiasts and privacy-focused users:</p><ul><li> Purism’s security- and privacy-focused smartphone, running on Debian Linux, continues to receive updates and improvements. It remains a leading choice for those who value open-source hardware and software.</li><li> Pine64’s PinePhone and PinePhone Pro offer mainline Linux support and hardware kill switches, appealing to developers and tinkerers.</li><li> The Pro¹-X, an updated version of the original Pro1, features a physical keyboard and support for multiple Linux-based OS options, including LineageOS and Ubuntu Touch.</li><li> KDE’s Plasma Mobile continues to evolve, supporting a range of devices including the PinePhone and select Android handsets via postmarketOS.</li></ul><h2>Checking the Linux Kernel Version on Your Android</h2><p>To see which Linux kernel your Android device is running, open  and tap on  or .&nbsp;For rooted devices, you can use <a href=\"https://play.google.com/store/apps/details?id=com.termux\" target=\"_blank\" rel=\"nofollow noopener\">Termux</a> and run:</p><p>This command will display your device’s kernel version and build information.</p><h2>Conclusion: Linux’s Enduring Dominance</h2><p>Android’s open-source nature and the flexibility of the Linux kernel continue to drive innovation and user empowerment in the smartphone market. With Android holding steady at around 80% global market share in 2025, Linux’s influence is as strong as ever.</p><p>Smartphones powered by Linux continue to dominate the global smartphone market in 2025. While some may still be surprised by this fact, Linux enthusiasts have every reason to celebrate the ongoing success and influence of Linux in the mobile world.</p><p>With the fall of the Windows phone, almost 100% of all smartphones are powered by Unix and Unix-like systems. Meanwhile, alternative Linux and Android-based smartphones such as those listed above, offer exciting options for those seeking greater privacy, control, or experimentation.</p>","contentLength":3992,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ll04eh/over_80_of_all_smartphones_are_powered_by_linux/"},{"title":"Learnings from building AI agents","url":"https://www.cubic.dev/blog/learnings-from-building-ai-agents","date":1750941904,"author":"pomarie","guid":237,"unread":true,"content":"<p>I’m Paul, cofounder of&nbsp;<a href=\"http://cubic.dev/\" rel=\"noopener\">cubic</a>—an \"AI-native GitHub.\" One of our core features is an AI code review agent that performs an initial review pass, catching bugs, anti-patterns, duplicated code, and similar issues in pull requests.</p><p>When we first released this agent back in April, the main feedback we got was straightforward: it was too noisy.</p><p>Even small PRs often ended up flooded with multiple low-value comments, nitpicks, or outright false positives. Rather than helping reviewers, it cluttered discussions and obscured genuinely valuable feedback.</p><img alt=\"\" height=\"236\" src=\"https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png\" srcset=\"https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=512 512w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=1024 1024w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png?scale-down-to=2048 2048w,https://framerusercontent.com/images/0Ap1gf4atqffqos1sHElEMB26DA.png 2052w\" width=\"1026\" data-framer-original-sizes=\"\" sizes=\"(min-width: 1200px) 100vw, (min-width: 1000px) and (max-width: 1199px) 100vw, (max-width: 999px) 100vw\"><p>We decided to take a step back and thoroughly investigate why this was happening.</p><p>After three major architecture revisions and extensive offline testing, we managed to reduce false positives by  without sacrificing recall.</p><p>Many of these lessons turned out to be broadly useful—not just for code review agents but for designing effective AI systems in general.</p><h2>1. The Face‑Palm Phase: A Single, Do‑Everything Agent</h2><p>Our initial architecture was straightforward but problematic:</p><div><div><div><div><div><div aria-labelledby=\"/example.md-:Rb8na5b8m:-tab\" role=\"tabpanel\"><div aria-autocomplete=\"list\" aria-label=\"Code Editor for example.md\" aria-multiline=\"true\" role=\"textbox\" tabindex=\"0\" translate=\"no\"><pre>\n↓\n\n↓\n</pre></div></div></div></div></div></div></div><p>It looked clean in theory but quickly fell apart in practice:</p><ul><li data-preset-tag=\"p\"><p>Excessive false positives: The agent often mistook style issues for critical bugs, flagged resolved issues, and repeated suggestions our linters had already addressed.</p></li><li data-preset-tag=\"p\"><p>Users lost trust: Developers quickly learned to ignore the comments altogether. When half the comments feel irrelevant, the truly important ones get missed.</p></li><li data-preset-tag=\"p\"><p>Opaque reasoning: Understanding why the agent made specific calls was practically impossible. Even explicit prompts like \"ignore minor style issues\" had minimal effect.</p></li></ul><p>We tried standard solutions—longer prompts, adjusting the model's temperature, experimenting with sampling—but saw little meaningful improvement.</p><p>After extensive trial-and-error, we developed an architecture that significantly improved results and proved effective in real-world repositories. These solutions underpin the 51% reduction in false positives currently running in production.</p><h4>2.1 Explicit Reasoning Logs</h4><p>We required the AI to explicitly state its reasoning before providing any feedback:</p><div><div><div><div><div><div aria-labelledby=\"/example.js-:Rb97a5b8m:-tab\" role=\"tabpanel\"><div aria-autocomplete=\"list\" aria-label=\"Code Editor for example.js\" aria-multiline=\"true\" role=\"textbox\" tabindex=\"0\" translate=\"no\"><pre></pre></div></div></div></div></div></div></div><p>This approach provided critical benefits:</p><ul><li data-preset-tag=\"p\"><p>Enabled us to clearly trace the AI’s decision-making process. If reasoning was flawed, we could quickly identify and exclude the pattern in future iterations.</p></li><li data-preset-tag=\"p\"><p>Encouraged structured thinking by forcing the AI to justify its findings first, significantly reducing arbitrary conclusions.</p></li><li data-preset-tag=\"p\"><p>Created a foundation to diagnose and resolve root causes behind other issues we faced.</p></li></ul><p>Initially, the agent had extensive tooling—Language Server Protocol (LSP), static analysis, test runners, and more. However, explicit reasoning logs revealed most analyses relied on a few core tools, with extra complexity causing confusion and mistakes.</p><p>We streamlined the toolkit to essential components only—a simplified LSP and a basic terminal.</p><p>With fewer distractions, the agent spent more energy confirming genuine issues, significantly improving precision.</p><h4>2.3 Specialized Micro-Agents Over Generalized Rules</h4><p>Initially, our instinct was to continuously add more rules into a single large prompt to handle edge cases:</p><ul><li data-preset-tag=\"p\"><p>“Ignore unused variables in .test.ts files.”</p></li><li data-preset-tag=\"p\"><p>“Skip import checks in Python’s .py.”</p></li><li data-preset-tag=\"p\"><p>“Don't lint markdown files.”</p></li></ul><p>This rapidly became unsustainable and was largely ineffective as the AI frequently overlooked many rules.</p><p>Our breakthrough came from employing specialized micro-agents, each handling a narrowly-defined scope:</p><ul><li data-preset-tag=\"p\"><p>: Quickly assesses changes and identifies necessary checks.</p></li><li data-preset-tag=\"p\"><p>: Detects vulnerabilities such as injection or insecure authentication.</p></li><li data-preset-tag=\"p\"><p>: Flags repeated or copied code.</p></li><li data-preset-tag=\"p\"><p>: Handles typos and documentation consistency.</p></li></ul><p>Specializing allowed each agent to maintain a focused context, keeping token usage efficient and precision high. The main trade-off was increased token consumption due to overlapping context, managed through effective caching strategies.</p><p>These architecture and prompt improvements led to meaningful results across hundreds of real pull requests from active open-source and private repositories. Specifically, over the past six weeks:</p><ul><li data-preset-tag=\"p\"><p>51% fewer false positives, directly increasing developer trust and usability.</p></li><li data-preset-tag=\"p\"><p>Median comments per pull request cut by half, helping teams concentrate on genuinely important issues.</p></li><li data-preset-tag=\"p\"><p>Teams reported notably smoother review processes, spending less time managing irrelevant comments and more time effectively merging changes.</p></li></ul><p>Additionally, the reduced noise significantly improved developer confidence and engagement, making reviews faster and more impactful.</p><ol><li data-preset-tag=\"p\"><p>Explicit reasoning improves clarity. Require your AI to clearly explain its rationale first—this boosts accuracy and simplifies debugging.</p></li><li data-preset-tag=\"p\"><p>Simplify the toolset. Regularly evaluate your agent's toolkit and remove tools rarely used (less than 10% of tasks).</p></li><li data-preset-tag=\"p\"><p>Specialize with micro-agents. Keep each AI agent tightly focused on a single task, reducing cognitive overload and enhancing precision.</p></li></ol>","contentLength":4937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44386887"},{"title":"Malicious npm eslint-config-airbnb-compat Package Hides Detection with Payload Splitting","url":"https://safedep.io/digging-into-dynamic-malware-analysis-signals/","date":1750941506,"author":"/u/Ok_Possibility1445","guid":632,"unread":true,"content":"<p>In our previous blog, we discussed building a large scale <a href=\"https://safedep.io/dynamic-analysis-oss-package-at-scale\">Dynamic Analysis System</a> for malicious open source package identification. Dynamic analysis help in observing actual runtime behaviors and activities for a package, installed in a sandbox environment. It complements our Static Analysis system and helps:</p><ul><li>To verify and correlate static analysis findings with actual runtime behavior</li><li>To overcome the limitations of static analysis which may have false positives and negatives</li><li>To observe how packages actually behave when executed in a controlled environment</li><li>To identify malicious packages that might evade static code analysis</li><li>To reduce the need for human intervention (manual analysis) over time</li></ul><p>The goal of this blog is to share our learnings and approach for identifying malicious packages based on dynamic analysis signals. This is a step towards having a sound and reliable baseline for package installation analysis so that it can be used to identify outliers and anomalies.</p><p>We tracked nearly  events generated from our Dynamic Analysis System, analyzing more than  packages since we started operating this infrastructure. As events are generated, we needed a way to find  activities and potential malicious packages that can be subjected to manual review. This can only be found using heuristics and patterns, not manual review of all events generated by all packages.</p><p>As a first step, we decided to aggregate following information from the events:</p><ul><li>Network Connection, IP Aggregator</li><li>Binary execution on install</li></ul><p>The rationale for these metrics are:</p><ul><li>Irrespective of specific TTP,  payload will eventually be executed by a malicious package</li><li>The payload will either make a network call or execute a command (eg. ) in an unusual way</li></ul><p>This hypothesis can be substantiated by our past observations where we observed multiple malicious packages eventually downloads a 2nd-stage payload from a remote C2 server or uploads data to a remote server (exfiltration).</p><p>Any package installation process always triggers network connections, especially to the source registries such as , ,  etc. We cannot only detect network connections, we have to identify  in the distribution of these connections. To identify outliers, we log every  for these connections and analyze the distribution of these . The following chart shows the distribution of  for network connections:</p><p>Looking at the data, we can spot some interesting patterns. A few IP addresses are  in the distribution. From a security perspective, these rare connections raise red flags since legitimate package installations typically connect to well known, frequently accessed endpoints. The chart below highlights some of these suspicious one-off connections that may be suitable candidates for further investigation:</p><p>For example, looking up IP  on <a href=\"https://www.virustotal.com/\">VirusTotal</a> shows it has been classified as malicious by multiple security vendors.</p><p>Next, we performed  to identify the hostnames of these IP addresses. Below are some of the IP addresses that were resolved to a hostname.</p><p>Looking at the hostnames, domains like , , , and <code>mail.sms-system-alert.com</code> are known malicious domains. In particular,  (Out-of-band Application Security Testing) domains are commonly used for malicious purposes like data exfiltration and command &amp; control. We have seen this in our previous post <a href=\"https://safedep.io/burp-collaborator-for-dependency-confusion-attack\">Burp Collaborator used in Malicious npm Packages</a>.</p><h3>Abnormal Binary Execution</h3><p>Packages introducing pre-compiled binaries during installation is a common observation. For example, top binaries shipped with  and  packages or expected to be present in the system are , , , , ,  and more. They are executed for legitimate purposes, but can be used to harm the system as soon as the package is installed. The following chart shows the distribution of these binaries:</p><p>Looking at the distribution, we can spot suspicious binaries like , , and  that appear infrequently. These unknown executables pose potential security risks, as they can execute malicious code immediately upon package installation, potentially compromising the system before any security controls can detect and prevent the threat.</p><p>While the system is at an early research stage, we present a case study of a real malicious package that was identified using the approach described above. This package is <a href=\"https://www.npmjs.com/package/eslint-config-airbnb-compat\">eslint-config-airbnb-compat</a> and was not detected as malicious by our static analysis system. Following are the high level chain of events that led to the identification of this package:</p><ol><li>Suspicious IP address  was detected in the network connection logs.</li><li>Events were correlated with the package <code>eslint-config-airbnb-compat</code> for which our <a href=\"https://platform.safedep.io/community/malysis/01JVCJ683EREK1JTTRK1HTDW4S\">static analysis report</a> was blind</li><li>Manual analysis did not conclusively identify root cause of this network activity</li><li>Dependency graph analysis identified  as a transitive dependency with stage-2 loader code</li><li>Manual analysis tied the two packages together and confirmed the malicious intent</li></ol><p>For this analysis, the trigger was a low key IP address  that appeared in the network connection logs. The reverse DNS lookup revealed the hostname <code>mail.sms-system-alert.com</code> that was flagged as malicious by VirusTotal. This gave us enough confidence to investigate the package further.</p><p>We backtrack the package which is associated with this event, making connection to this  IP (who’s host was <code>mail.sms-system-alert.com</code>), and found <a href=\"https://www.npmjs.com/package/eslint-config-airbnb-compat\">eslint-config-airbnb-compat</a>. This package appears to impersonate legitimate <a href=\"https://www.npmjs.com/package/eslint-config-airbnb\">eslint-config-airbnb</a> possibly with the goal of starjacking and spoofing its origin to automated security tools.</p><p>We found, <code>eslint-config-airbnb-compat</code> contains a post install script declared in  to execute . This is not totally unusual for a large number of  packages, although it does raise security concerns.</p><pre data-language=\"shell\" tabindex=\"0\"><code></code></pre><p>However, manual analysis revealed multiple unusual behavior. Likely to avoid identification, the  does not have any malicious code. It simply does the following:</p><ul><li>Copy the embedded  to </li></ul><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><ul><li>The  file contains the following</li></ul><pre data-language=\"txt\" tabindex=\"0\"><code></code></pre><blockquote><p> The host  resolves to our target IP address </p></blockquote><ul><li>Execute  if  directory is not present</li></ul><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><p>At this point, we were fairly confident that this package is malicious. However, we needed to identify the root cause of this malicious behavior. We started by analyzing the dependency graph of this package and found  as a transitive dependency with stage-2 loader code. The package  in turn has a post install script:</p><pre data-language=\"shell\" tabindex=\"0\"><code></code></pre><p>The  in  contains interesting code:</p><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><p>When introduced through <code>eslint-config-airbnb-compat</code>, it will have <code>proxy=https://proxy.eslint-proxy.site</code> in the  call above. The above fetch call is expected to fail to trigger  function with  provided error message.</p><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><p>The remote server at <code>https://proxy.eslint-proxy.site</code> can return a  message such as <code>{\"error\": \"&lt;JS Payload&gt;\"}</code> which in turn will be passed to  as an Error object.</p><p>The error handler in turn does the following:</p><ul><li>Decode the message as  string</li></ul><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><ul><li>Constructs a function from the decoded string</li></ul><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><ul><li>Finally executes the remote code</li></ul><pre data-language=\"js\" tabindex=\"0\"><code></code></pre><p>This pretty much confirm the malicious behavior of the entire attack chain. It implements a multi-stage remote code execution attack using a transitive dependency to hide the malicious code.</p><p>Dynamic analysis provides a complementary approach for detecting malicious open source packages that might evade static analysis. By monitoring network connections and binary executions during package installation, we can identify suspicious behaviors that indicate potential threats. The multi-stage attack discovered in the <code>eslint-config-airbnb-compat</code> package demonstrates how sophisticated these attacks can be, using transitive dependencies and obfuscation techniques to hide malicious code.</p><p>As attackers continue to develop increasingly complex methods to compromise the open source software supply chains, combining static and dynamic analysis approaches is essential for effective detection. By focusing on abnormal signals and patterns during runtime, we can better protect our software supply chains against evolving threats and maintain the integrity of the open source ecosystems.</p>","contentLength":7952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lkz2oa/malicious_npm_eslintconfigairbnbcompat_package/"},{"title":"Rust 1.88: 'If-Let Chain' syntax stabilized","url":"https://releases.rs/docs/1.88.0/","date":1750939066,"author":"/u/thurn2","guid":641,"unread":true,"content":"<blockquote><ul><li>Released on: </li><li>Branched from master on: </li></ul></blockquote><p>These previously stable APIs are now stable in const contexts:</p>","contentLength":100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lky9d2/rust_188_iflet_chain_syntax_stabilized/"},{"title":"Command Pattern as an API Architecture Style","url":"https://ymz-ncnk.medium.com/command-pattern-as-an-api-architecture-style-be9ac25d6d94","date":1750938487,"author":"/u/ymz-ncnk","guid":614,"unread":true,"content":"<p>Would you like a fast and flexible interaction with your API? To achieve this goal, it is necessary to:</p><ol><li>Decide on the API architectural style.</li><li>Choose the serialization format.</li><li>Use efficient implementations of both.</li></ol><p>It’s always better to solve a general problem by breaking it down into smaller parts, so let’s take a closer look at each.</p><p>We already have numerous API architectural styles, such as REST, RPC, and SOAP, to name a few. I would like to add our old friend to them — the <a href=\"https://en.wikipedia.org/wiki/Command_pattern\" rel=\"noopener ugc nofollow\" target=\"_blank\">Command Pattern</a>, or more precisely, the Command Pattern over the network. Offering the following advantages, it can be a good candidate for this role:</p><ul><li>Provides a way to model transactions. Commands share a common interface and can perform multiple actions at a time, making them ideal for this purpose.</li><li>Allows to save user actions as a list of Commands, which can be useful for logging user activities, replaying operations, or implementing auditing mechanisms.</li><li>Provides Undo/Redo functionality.</li><li>Follows the open/closed principle. New Commands can be added easily without modifying existing code.</li><li>Enhances testability, as Commands can be tested in isolation.</li></ul><p>Let’s compare it with RPC.</p><p>Why RPC? Because they are similar — both approaches involve performing an arbitrary action on the server.</p><p>The most obvious difference is that the Command Pattern operates with Commands, whereas RPC relies on functions.</p><p><strong>II. Similar Representation</strong></p><p>Despite that, they look the same when transmitted over the network:</p><pre></pre><p>That’s quite unexpected, isn’t it?</p><p><strong>III. Number of Actions at a Time</strong></p><p>Unlike the Command Pattern, RPC can perform only one action at a time, which is not very convenient. Let’s look at the following function composition:</p><p>RPC suggests to make two requests or have a function like , to reduce the number of round trips. The latter option, by the way, is not ideal. The desire to get rid of latency problems will affect the communication interface — it will become more broad and complex. This is, actually, what the guys from <a href=\"https://capnproto.org\" rel=\"noopener ugc nofollow\" target=\"_blank\">Cap’n Proto</a> say. Offering own <a href=\"https://capnproto.org/rpc.html\" rel=\"noopener ugc nofollow\" target=\"_blank\">solution</a> they describe this problem in more detail.</p><p>On the other hand, function composition is not an issue for the Command Pattern:</p><pre></pre><p>In general it allows to perform an unlimited number of actions with a single request, without adding interface complexity or compromising performance.</p><p><strong>IV. Commands Inside Functions</strong></p><p>RPC can actually be implemented using the Command Pattern. In this case, functions can simply send Commands to the server:</p><pre></pre><p>Thus, all we need to know is how to deal with Commands. This knowledge is more versatile and allows to improve even existing RPC systems.</p><h2><strong>Command Pattern Challenges</strong></h2><p>While offering a more flexible and general abstraction, the Command Pattern also introduces some challenges:</p><ul><li>Server should somehow distinguish one Command from another. : Before each Command, send its type.</li><li>Commands must return results, and each Command can have its own. : Instead of returning, the Command itself can be responsible for sending one or more results back.</li></ul><pre></pre><ul><li>During execution, a Command may encounter an error. How should it be handled? : If we want the client to know about this error, the Command can send it back as a result. In another case, the Command can terminate the connection with the client, returning an error to the Invoker.</li></ul><pre></pre><ul><li>To limit its execution time, the Command must know when it was received by the server. This time may differ from the start of execution. : The Command can receive it as a parameter.</li></ul><pre></pre><p>That’s how the Command Pattern, adapted to our needs, might look. To see it in action, we need to consider one more thing.</p><p>To send data somewhere, it must first be converted into a sequence of bytes. This can be done in various ways, which is why so many serialization formats exist. One of the most important metrics to consider is the number of bytes used by the format. The fewer bytes we need to transfer over the network, the faster our application will be.</p><p>The MUS format was created with these thoughts in mind. It uses almost no metadata and is actually a fairly simple format. I don’t want to repeat myself a lot, so here’s a <a href=\"https://medium.com/@ymz-ncnk/mus-serialization-format-21d7be309e8d\" rel=\"noopener\">link</a> where you can read more about it.</p><p>And that’s all for the theory.</p><p>The ideas described above have already been implemented for Go in the form of two libraries:  and .</p><p><a href=\"https://github.com/cmd-stream/cmd-stream-go\" rel=\"noopener ugc nofollow\" target=\"_blank\">cmd-stream-go</a> is a high-performance client-server library that implements the Command Pattern and:</p><ul><li>Can work over TCP, TLS or mutual TLS.</li><li>Has an asynchronous client, that uses only one connection for both sending Commands and receiving Results.</li><li>Supports the server streaming, i.e. a Command can send back multiple Results.</li><li>Provides reconnect and keepalive features.</li><li>Supports the Circuit Breaker pattern.</li><li>Has OpenTelemetry integration.</li><li>Can work with various serialization formats.</li><li>Has a modular architecture.</li></ul><p><a href=\"https://github.com/mus-format/mus-go\" rel=\"noopener ugc nofollow\" target=\"_blank\">mus-go</a> is a MUS format serializer, it:</p><ul><li>Represents a set of serialization primitives that can be used to implement not only MUS but also other serialization formats.</li><li>Can run on both 32 and 64-bit systems.</li><li>Can validate and skip data while unmarshalling.</li><li>Can serialize data structures such as graphs or linked lists.</li><li>Supports data versioning.</li><li>Supports out-of-order deserialization.</li><li>Supports zero allocation deserialization.</li></ul><p>In addition, as you can see in the <a href=\"https://github.com/ymz-ncnk/go-serialization-benchmarks\" rel=\"noopener ugc nofollow\" target=\"_blank\">benchmarks</a>, it demonstrates excellent performance:</p><pre></pre><ul><li>NS/OP — Nanoseconds per operation.</li><li>B/SIZE — Number of bytes used to encode the data.</li><li>B/OP — Number of bytes allocated per operation.</li><li>ALLOCS/OP — Number of allocations per operation.</li></ul><p>Among these <a href=\"https://github.com/cmd-stream/cmd-stream-examples-go\" rel=\"noopener ugc nofollow\" target=\"_blank\">examples</a>, you can find <a href=\"https://github.com/cmd-stream/cmd-stream-examples-go/tree/main/rpc\" rel=\"noopener ugc nofollow\" target=\"_blank\">one</a> where  is used as a communication tool for the RPC approach. As for <a href=\"https://github.com/ymz-ncnk/go-client-server-benchmarks\" rel=\"noopener ugc nofollow\" target=\"_blank\">benchmarks</a>,  is about 3 times faster than :</p><p>Sending Commands is a pretty good abstraction. It’s similar to RPC, but doesn’t limit us to just one action. Moreover, the Command Pattern can either replace RPC or be used as a tool for building it. Also it offers several advantages mentioned above and already has the high-performance implementation. All of this makes the Command Pattern a great choice for an API architectural style.</p><p><a rel=\"noopener\" href=\"https://ymz-ncnk.medium.com/command-pattern-as-an-api-architecture-style-part-ii-beeae1da0594\" data-discover=\"true\">Command Pattern Over the Network →</a></p>","contentLength":6026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1lky2ps/command_pattern_as_an_api_architecture_style/"},{"title":"\"Why is the Rust compiler so slow?\"","url":"https://sharnoff.io/blog/why-rust-compiler-slow","date":1750935847,"author":"/u/jahmez","guid":638,"unread":true,"content":"<div><p>I spent a month repeatedly building my website in Docker, and now have horrors to share.</p></div><p>My website (the one you're reading right now) is mainly served by a single Rust binary.\nFor  now, every time I wanted to make a change, I would:</p><ol><li>Build a new statically linked binary (with <code>--target=x86_64-unknown-linux-musl</code>)</li></ol><p>So instead, I'd like to switch to deploying my website with containers (be it Docker, Kubernetes, or otherwise),\nmatching the vast majority of software deployed any time in the last decade.</p><p>The only issue is that fast Rust builds with Docker are not simple.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#basics\"></a><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-simple\"><h3>Rust in Docker, the simple way</h3></a><p>To get your Rust program in a container, the typical approach you might find would be something\nlike:</p><pre><code></code></pre><p><strong>Unfortunately, this will rebuild everything from scratch whenever there's any change</strong>.</p><p>In my case, building from scratch takes about 4 minutes (including 10s to download the crates every time).</p><pre><code></code></pre><p>Sure, it could be worse. But I've grown accustomed to speedy local builds, thanks to incremental compilation — I don't\nwant to wait that long on every tiny change!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-cargo-chef\"><h3>Rust in Docker, with better caching</h3></a><p>Thankfully, there's a tool to help with this!</p><p>Luca Palmieri's <a href=\"https://github.com/LukeMathWalker/cargo-chef\"></a> makes it easy to pre-build all of the dependencies as a separate layer in the docker\nbuild cache, so that changes in your codebase only trigger re-compilation of your codebase (and not your dependencies).</p><p>I'll save the detailed explanation for <a href=\"https://lpalmieri.com/posts/fast-rust-docker-builds/\">Luca's blog post</a>, but broadly  creates a simplified \"recipe\" file from\nthe current workspace, which can be \"cooked\" to cache the dependencies without being invalidated by changes in the\nworkspace.</p><p>My website pulls in a few hundred dependencies, so this  help!</p><pre><code>...\n\n</code></pre><p>Unfortunately though, it doesn't have quite the speedup we're looking for — most of the time is still in the final\nbinary:</p><pre><code></code></pre><p>Weirdly, only 25% of the time is actually spent on the dependencies! As far as I could tell, my code isn't doing\nanything fundamentally unreasonable. It's ~7k lines of gluing together various larger dependencies (, ,\n, among others.)</p><p><em>(Just to double-check, I tried running  with . It really was just a single\ninvocation of  that took almost 3 minutes!)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-rustc-doing\"><h2>What's  doing for all that time?</h2></a><pre><code></code></pre><p>In addition to that <code>cargo-timing-&lt;timestamp&gt;.html</code> file, there's also a . We'll\njust copy out the canonical version:</p><pre><code>...\n\n</code></pre><p>And with a little bit of container wrangling...</p><pre><code> cargo-timing.html\n container </code></pre><p>... we should be able to see what's going on! Let's have a look:</p><p> There's not really much information there!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#cargo-timings-weirdness\"></a><p> shows a bunch of information about <em>how long each crate took to compile</em>. But here, we only care\nabout the compilation time of the final crate!</p><p>That aside, this does help give us more accurate timing. Measuring outside the compiler adds some extra moving\npieces, or requires searching the output of  — so using 's self-reported timings will make more\nprecise analysis a bit easier, later on.</p><p>Just to check, the value here of 174.1s roughly matches the \"2m 54s\" we saw from the  output.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rustc-self-profile\"><h2>Actually asking  this time</h2></a><p>The post from fasterthanlime had one more tip we can use — 's self-profiling feature, via the \nflag.</p><p>Normally, you'd probably run something like:</p><pre><code> rustc  --  self-profile\n</code></pre><p><em>(note: This is using  to pass extra flags to , with  to allow using the \nunstable flags on a stable compiler.)</em></p><p>Unfortunately, this won't work here — the change in arguments will invalidate the cached dependencies from\n, and there's no equivalent way to pass additional  flags through .</p><p>Instead, we can funnel everything via the  environment variable:</p><pre><code> chef cook .\n\n build .\n</code></pre><p>This gives us files like <code>web_http_server-&lt;random-number&gt;.mm_profdata</code>, which we can move and extract from the image in\nthe same way as we did for .</p><p><em>(note: It's much easier to automate if we remove the profiling data that was added from  before the\nfinal build. That's omitted here, for brevity.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#using-rustc-profdata\"><h3>Actually using the profdata</h3></a><ul><li> – produces plaintext output summarizing the profiling data</li></ul><p>But let's install a couple of these to take a look at what we've got:</p><pre><code> https://github.com/rust-lang/measureme flamegraph summarize\n</code></pre><p>I personally use Firefox, so we'll hold off on the chrome tracing stuff for now.</p><p>First, with  (which itself has the  and  subcommands):</p><pre><code></code></pre><p>So at a high level, the two biggest things are <a href=\"https://www.llvm.org/docs/LinkTimeOptimization.html\">link-time optimization</a> (LTO) and\n<code>LLVM_module_codegen_emit_obj</code>, whatever that is.</p><p>Let's see if we can dig a bit deeper with the flamegraph:</p><pre><code></code></pre><p><em>(It's interactive! If you're curious, you can click through and play around with it yourself.)</em></p><p>So there's presumably some inter-mingling going on between codegen and LTO: <code>codegen_module_perform_lto</code> ends up falling\nthrough to both / and .</p><p>But either way, we've got a problem with LTO: <code>codegen_module_perform_lto</code> took ~80% of the total time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#about-lto\"><h2>It's time to talk about LTO</h2></a><p>The Rust compiler splits up crates into \"<a href=\"https://doc.rust-lang.org/rustc/codegen-options/index.html#codegen-units\">codegen units</a>\", handing each to LLVM as a separate module to compile.\n, optimizations take place within each codegen unit, and then they're linked together at the end.</p><p>LTO controls the set of optimizations that LLVM will make during that link-time — for example, inlining or\noptimization across codegen units.</p><ul><li>\"thin\" LTO — in theory, similar performance benefits to \"fat\" LTO, but less expensive to run</li><li>\"fat\" LTO — maximum amount of LTO, across all crates at the same time</li></ul><p>And if the LTO option is not specified,  uses \"thin local LTO\", which limits \"thin\" LTO only to a single crate at\na time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-current-settings\"><h3>What are the current settings</h3></a><p>Turns out that a few years back, I had set  in my :</p><pre><code></code></pre><p>And, while we're at it, <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#debug\"></a> enables all debug symbols (where they'd normally be excluded by default for the\n profile). Maybe we should take a look at that as well.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-tweaking-settings\"><h3>Tweaking the (normal) settings</h3></a><p>Let's take a look at the compile times and binary sizes for a variety of  and  settings (using\n like before, for more precise timing).</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>At a high level: It seems like the worst cases here are full debug symbols adding 30-50% to the compilation time, and\n\"fat\" LTO taking about  than with LTO fully disabled.</p><p>That mostly tracks with what we'd expect from the documentation — yeah, fat LTO takes longer. But when we disable\neverything, we're still looking at 50 seconds compiling the final binary!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note\"><h2>A brief note: 50 seconds is , actually!</h2></a><p>Look, 50 seconds is already a great improvement — and if it requires disabling LTO and debug symbols... my website\ngets approximately zero load. <em>It would be totally fine.</em> It would be perfectly sustainable, even!</p><p><strong>There's no practical reason to keep digging here.</strong></p><p>But where's the fun in leaving it there? We should be able to do better, right?</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note-2\"><h2>Another brief note: Can't we just use incremental compilation?</h2></a><p>It's slightly more complicated, but yes, absolutely — for local development, at least. Consistently loading the build\ncache isn't straightforward, but you'd want to make the  directory accessible with a <a href=\"https://docs.docker.com/build/cache/optimize/#use-cache-mounts\">\"cache mount\"</a> in the\ndockerfile, and persist that target directory between builds.</p><p>That said, I value that  have a clean environment every time, and I think it's worthwhile to go\nthrough docker's own caching system — which is why I'm using  in the first place.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#et-tu-llvm_module_optimize\"><h2>Digging deeper: Et tu, ?</h2></a><p>If we disable LTO and debug symbols, compiling the final binary still takes 50 seconds to do... something.</p><p>Let's re-run the self-profiling to check out what's going on.</p><p>It's ~70% just  — i.e. where LLVM is optimizing the code. Before diving into LLVM itself, let's\nsee if there's any easier knobs we can tune.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#tuning-optimization\"></a><p>The  profile uses  by default — maybe if we reduce the optimization level, we'll spend less\ntime on it.</p><p>We can actually do one better — since our dependencies are cached, and we only care about the final binary, we can get\nmost of the benefits by only reducing optimizations on the final binary:</p><pre><code></code></pre><p>Like the previous options, there's a handful of <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#opt-level\">s</a> we can choose from:</p><ul><li>, , and  enable increasing levels of optimizations</li><li> and  are different flavors of prioritizing binary size</li></ul><p>Going through a handful of combinations here again:</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><ul><li>The baseline for  level of optimizations on the final binary is about 50 seconds</li><li>If we disable all optimizations, then it's pretty quick: only ~15s</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm\"></a><p>Rust relies pretty heavily on optimizations, and while it'd probably be fine to just blanket-disable them for the final\nbinary, it'd be pretty cool if we can at least keep  optimizations!</p><p><strong>So let's try to figure out what's taking so long.</strong>'s self-profiling doesn't give us any more detail though,\nso we'll have to get it from LLVM.</p><ul><li> – emit LLVM profiling information as plaintext</li><li> – emit LLVM profiling information in the chrome tracing format (again with that one!)</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-text\"><h3>Profiling LLVM with  — plain text</h3></a><p>Like before, let's skip the chrome tracing format for now, and see what we can get from plain text.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>... Unfortunately if you try to  again, you'll immediately hit something like:</p><pre><code>[output clipped, log limit 2MiB reached]\n</code></pre><p>This is because <a href=\"https://docs.docker.com/build/buildkit/\">BuildKit</a> (if you're using ~recent Docker on Linux) has default output limits that are pretty small.</p><p>So after getting unlimited  output on the terminal, what's in it?\n — probably not what you want to be copying from your terminal, anyways.</p><p>So, redirecting to a file inside docker and copying that out like before, we get a bunch of pass/analysis timing\nreports. They each look something like this:</p><pre><code>===-------------------------------------------------------------------------===\n                          Pass execution timing report\n===-------------------------------------------------------------------------===\n  Total Execution Time: 0.0428 seconds (0.0433 wall clock)\n\n   ---User Time---   --System Time--   --User+System--   ---Wall Time---  — Name ---\n   0.0072 ( 19.2%)   0.0015 ( 27.4%)   0.0086 ( 20.2%)   0.0087 ( 20.0%)  InstCombinePass\n   0.0040 ( 10.8%)   0.0006 ( 10.8%)   0.0046 ( 10.8%)   0.0047 ( 10.8%)  InlinerPass\n   0.0024 (  6.4%)   0.0010 ( 18.0%)   0.0034 (  7.9%)   0.0034 (  7.8%)  SimplifyCFGPass\n   0.0022 (  5.9%)   0.0002 (  4.5%)   0.0025 (  5.7%)   0.0024 (  5.6%)  EarlyCSEPass\n   0.0021 (  5.5%)   0.0001 (  1.5%)   0.0021 (  5.0%)   0.0022 (  5.0%)  GVNPass\n   0.0015 (  4.0%)   0.0001 (  2.2%)   0.0016 (  3.8%)   0.0018 (  4.2%)  ArgumentPromotionPass\n\n   ... entries here continue, and more passes below, for hundreds of thousands of lines ...\n</code></pre><p>It certainly is  to parse and analyze these! But it's also hard to be certain about what you're looking at\nwhen each pass execution is emitted separately and multi-threading can interfere with timing.</p><p>Let's see if there's a better way to get good data.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-trace\"><h3>Profiling LLVM with  — actual tracing this time</h3></a><p>We skipped  earlier because it emits the chrome tracing format.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>It produces a bunch of <code>$package-$hash.llvm_timings.json</code> files, alongside the normal compilation artifacts:</p><pre><code></code></pre><p><em>(Why ? Setting up rootless docker didn't work when I tried it a few years back, and I haven't bothered since)</em></p><p>So, deleting  between  and the final build, we can extract the singular profile for the\nfinal binary into <code>web_http_server.llvm_timings.json</code>.</p><p>There's just one minor hiccup:</p><pre><code></code></pre><p>It's . It's also all one single line!</p><p>In theory though, a wide variety of tools should be able to process this:</p><p>None of these options worked for me — but it's a big JSON file with a <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0\">known format</a>, how hard can it be?</p><p>Turns out, a 1.4GiB single line of JSON makes all the normal tools complain:</p><ul><li>If you try to view it with , scrolling blocks on processing the entire file</li><li>If you try to process it with , it has to load the entire 1.4GiB into 's internal format (which expectedly\ntakes up  more than the original 1.4GiB)</li><li>Vim hangs when you open it</li><li>And you probably don't want to just  it to the terminal — again, it's 1.4GiB!</li></ul><p>So instead, we can just look at a few hundred characters, at the start and end of the file:</p><pre><code></code></pre><p>Matching this to the <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0#heading=h.q8di1j2nawlp\">\"JSON Object Format\"</a> from the chrome tracing spec, it seems we have a single JSON object like:</p><pre><code>\n    ...\n  </code></pre><p>We'd be able to process it with normal tools if we split each event into its own object. That could be something like:</p><pre><code> web_http_server.llvm_timings.json  web-http-server.llvm_timings.jsonl\n</code></pre><p><em>(i.e.: turn  into a newline, strip the start of the object, strip the end of the object)</em></p><p>And  we can process this.</p><pre><code></code></pre><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-in-llvm-trace\"><h2>What's in LLVM's trace events?</h2></a><p>It looks like these events all have .</p><p>According to the spec, the  field gives the type of event, and  refers to \"complete\" events, recording how long a\nparticular piece of work took on a given thread (). The duration in microseconds is given by .</p><p>Aside from that, we also have  events:</p><pre><code></code></pre><p>These are \"metadata\" events — in our case, not much useful information.</p><p>And aside from these, there's nothing else:</p><pre><code></code></pre><p>Going back to those  events — there were a bunch of them with . What else do we have?</p><pre><code></code></pre><p>Neat! It looks like we might be able to demangle some of the symbols to get timings on individual functions.</p><p>If we track what's being run and how long it takes, we should be able to get a better sense of why our compile time is\nso long.</p><p>Later on, there's aggregate information for certain types of events, like . These are equivalent to\nthe sum of the duration for that event type (in this case, ). Let's see kind of operations are taking the\nmost time:</p><pre><code></code></pre><p>This particular run took ~110 seconds on a 16-core machine, so it's clear that some passes are being double-counted\n(which makes sense — we see both , and it looks like \nprobably just calls ).</p><p>But broadly, it seems like optimization () and inlining () are the two parts taking a lot of\ntime — let's see if we can do anything about it.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-inlinerpass-faster\"><h2>Can we make  any faster?</h2></a><p>LLVM has a bunch of arguments that can be configured, which  exposes through the  flag. At time of\nwriting (June 2025), there's somewhere in the region of ~100 options that mention inlining (via <code>rustc -C llvm-args='--help-list-hidden'</code>).\nIn particular, there's a <a href=\"https://github.com/llvm/llvm-project/blob/c7063380205d8776e281f7a6603119aa8ea28c12/llvm/lib/Analysis/InlineCost.cpp#L58-L176\">bunch of relevant options</a> in the file controlling the cost analysis.</p><p>Now, I'll be honest, I know  about LLVM's inlining. Most of the options refer to the \"cost\" associated\nwith the inlining, or with the function being inlined, etc. <strong>I'm flying mostly bind here.</strong> But there's a few arguments\nthat seem like decent candidates for tuning:</p><ul><li><code>--inlinedefault-threshold=225</code> — \"Default amount of inlining to perform\"</li><li> — \"Control the amount of inlining to perform\"</li><li><code>--inlinehint-threshold=325</code> — \"Threshold for inlining functions with inline hint\"</li></ul><p>For all of these, the \"threshold\" roughly means \"allow inlining functions with cost  the threshold\", so a higher\nthreshold means more inlining.</p><p>So if we set all of these to some value (e.g., ), we should see that there's less inlining, and in turn faster\ncompile times.</p><pre><code>.\n</code></pre><p><em>(Why separate ? I couldn't find a way to make the whitespace happy through the  environment\nvariable — maybe it's possible if you set  in , but this solution worked 🤷)</em></p><p>In any case, reducing to a threshold of 50  end up faster! About 42.2s, down from 48.8s.</p><p>Here's what that looks like across a handful of values:</p><p><em>(note: The smallest value is 1, and not zero. Why 1? Sometimes zero has special behavior – setting to one seemed like a safer bet.)</em></p><p>Of these, it's hard to say exactly what the best value is, but for my use case (remember: my website gets ~zero load!),\nsetting the thresholds to 10 looks promising. We'll hold off on that for now though.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-optfunction-faster\"><h2>Can we make  any faster?</h2></a><p>Optimizing functions was the other expensive task we saw.</p><p>The knobs here are much less clear to me (we're already at , and  compeltely disables\noptimizations). So, let's see what exactly is taking so long.</p><p>First, a brief look at the event format:</p><pre><code></code></pre><p>In its raw form, each of the events'  field has the mangled symbol of the function being optimized. We can\n\"demangle\" these back to the original Rust symbols with <a href=\"https://github.com/luser/rustfilt\"></a> — for example:</p><pre><code></code></pre><p>It's worth noting that in the list above, while there's several <code>serde_json::value::to_value</code> items, they actually have\ndistinct hashes:</p><pre><code></code></pre><p>... which makes sense, given that <code>serde_json::value::to_value</code> is a generic function — it might be that it's being\noptimized with different generic parameters (\"monomorphizations\").</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#why-optimizing-other-crates\"><h3>Wait, why are we optimizing functions from other crates?</h3></a><p>The short answer is that optimization is done <em>in the context of the crate where a function is monomorphized</em>. So if we\ndefine a type  and then call methods on , those methods  will first exist in the\ncontext of our crate — meaning it gets compiled and optimized with the same configuration as our crate.</p><p>With some knowledge about how the compiler works under the hood, this should hopefully make some sense — but from the\noutside, it's certainly a little odd!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-are-we-optimizing\"><h3>What's actually taking so long?</h3></a><p>Now that we know what we're looking at, we can start doing some analysis. For example, by finding the individual\nfunctions we spent the most time optimizing:</p><pre><code></code></pre><p><em>(Why two separate  invocations? If we did just one, the / call would load the entire file\ninto a single array before any processing, which is one of the key operations we're trying to avoid)</em></p><p>This is a surprising amount of time on individual functions! Profiling roughly doubled the total time to compile, but\neven 1 second optimizing a single function is quite a long time!</p><p>But let's look into more detail here. We've got:</p><ul><li><code>web_http_server::photos::PhotosState::new::{{closure}}</code> — this is  closure inside a giant, 400-line async\nfunction that does the setup for <a href=\"https://sharnoff.io/photos\">https://sharnoff.io/photos</a></li><li><code>web_http_server::run::{{closure}}</code> — this is inside the main entrypoint (also async), but all the closures are small\nerror-handling, like <code>.wrap_err_with(|| format!(\"failed to bind address {addr:?}\"))</code><ul><li>Maybe there's something weird going on here!</li></ul></li></ul><p>... and a handful of dependencies that also took a while:</p><p>, we could break it down by the outermost crate:</p><pre><code></code></pre><p>This is, of course, a very imperfect measure — the outermost crate isn't necessarily the best one to attribute the\ncompilation time to, and there's a lot of items like  that aren't captured by this simple filtering.\nBut all that aside, it's still surprising that there's so much from !</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#closures-mangling-v0\"><h3>Digging more into closures, with mangling v0</h3></a><p>The long compile times for closures seems very suspicious — maybe it's worth digging further. There's just one\nproblem: the symbols all end with  without saying  is taking all the time.</p><p>As it turns out, there's an easy fix! As of June 2025,  currently uses the \"legacy\" symbol mangling format by\ndefault, but there's a newer option with more information: the <a href=\"https://doc.rust-lang.org/rustc/symbol-mangling/v0.html\">v0 format</a>.</p><p>We can enable it by adding <code>RUSTFLAGS=\"-C symbol-mangling-version=v0\"</code> to our existing flags, which now look something\nlike:</p><pre><code>RUSTC_BOOTSTRAP=1 RUSTFLAGS=\"-Csymbol-mangling-version=v0 -Zllvm-time-trace\" cargo build --timings ...\n</code></pre><p><em>(aside: The issue for that feature's been open for 6 years, why hasn't it been merged yet? Turns out, there's a lot of\nupstream work required to add support in common tools like  and . A lot of that has been done, but not yet\neverything.)</em></p><p>The end result of this is that we get  better symbols coming out of the LLVM trace. As an example, here's what\nthose <code>serde_json::value::to_value</code> symbols look like now:</p><pre><code></code></pre><p>So not only do we get better closure labeling (see e.g. ) but we also get full generics for everything!</p><p>Exactly what's taking so long  be much clearer now:</p><pre><code></code></pre><p>... but those first few closures are :</p><pre><code> is_jpg  path s</code></pre><pre><code> app  feed </code></pre><p>And if we remove these closures, replacing them with separately defined functions where possible, LLVM  reports\ntaking a long time to optimize  in the outer function.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#async-closure0\"><h3>So where are those closures coming from?</h3></a><p>After dumping the LLVM IR with <code>RUSTFLAGS=\"--emit=llvm-ir\"</code> (which places it into ) and searching\nthrough the generated functions, I found a line like:</p><p>That  function was a nested async function, defined directly inside  — so why did the\nsymbol say it was defined inside a closure?</p><p>It's because <em>internally represents async functions/blocks with a nested closure</em>. So all of these places that\nwe had async functions where compiling  took a long time were actually just referring to the function itself!</p><p>With some quick github searching (<code>is:issue state:open async fn closure mangle</code>), it turned out there was already an\n<a href=\"https://github.com/rust-lang/rust/issues/104830\">open issue about this</a>!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#big-async-functions-considered-harmful\"><h3>Big async functions considered harmful?</h3></a><p>Going back to our list from before – those async functions where LLVM takes a long time to optimize  are\nreally just spending a long time on the body of the function itself. It would make sense that big functions are hard to\noptimize, and async functions doubly so.</p><p>It's  straightforward to identify all of the functions inside the main crate that are taking a long time:</p><pre><code></code></pre><p>Some of the most expensive functions here are around setup.</p><p><strong>Let's try breaking up just one function, to see if it helps.</strong> We'll start with .</p><p>On the first attempt, I tried breaking it up while also preserving the number of s – it's easy to do both\naccidentally, and this would hopefully isolate which type of complexity is causing problems.</p><p>Interestingly, this didn't help all that much: only reducing the total time from 5.3s to 4.7s.</p><p>So to add to that, I tried merging a handful of neighboring s into their own functions — reducing the total\nnumber from 10 to 3.</p><p>But that took substantially longer! It increased from 4.66s to 6.24s!</p><p>At this point, it seemed like there was something strange happening with async functions. Otherwise, why would splitting\ninto more functions make things worse?</p><p>Under the hood, async functions desugar to a complex state machine. There might be something odd happening there, so if\nwe want to make that simpler in the caller, we can turn the  into a trait object to obscure the implementation\nbehind it (typically ).</p><p>So this time, let's add a new function like:</p><pre><code>\n    futfut</code></pre><p>and using it everywhere we . For example:</p><pre><code> candidates  candidates </code></pre><p><strong>This one worked — down to 2.14s.</strong></p><p>So, a reduction from 5.3s to 2.14s – a notable improvement, albeit with a lot of effort to get there. (and, for the\nrecord, when I wrapped the futures with  instead of a fresh function, it didn't make a difference here).</p><p>Re-running the build without profiling, this gives a total reduction from 48.8s to 46.8s. It's pretty small, but that's\nfrom just a single function!</p><p><em>(Aside: What about ? I tried it with and without – after boxing, compile times weren't any better\nwith inlining disabled for those functions, but it's still helpful for ensuring better attribution on the LLVM\ntimings.)</em></p><p><em>(Aside: What about disabling inlining on the  functions? I also tried wrapping the async functions with a\n implementation having  on its poll function. That helped , but wasn't as good as\nboxing.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#putting-it-together\"></a><p>There's a number of approaches available — let's try:</p><ol><li>Reducing inlining with LLVM args;</li><li>Breaking up expensive functions in the main crate; and</li><li>Removing generics from dependencies to prevent needing to compile it in the main crate</li></ol><p>So, updating the final Dockerfile commands to read:</p><pre><code>\n\n...\n\n</code></pre><p>... and many more small changes to the main crate:</p><pre><code></code></pre><p>... alongside some changes to larger dependencies:</p><p>... gives us a final compile time of .</p><ol><li>Disabling LTO (and debug symbols!) got us to 51s (-71%) </li><li>Changing to  on the final crate got us to 48.8s (-4%)</li><li>Reducing inlining with  got us to 40.7s (-16%) </li><li>Local changes got us to 37.7s (-7%) </li><li>And changes with dependencies got us to 32.3s (-14%) </li></ol><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-now\"></a><p>While I did hit a lot of issues here, the tooling honestly worked really well – and the documentation was sufficient for\nsomeone with relatively little experience to make meaningful improvements to their codebase.</p><p>Some of the issues are straightforward: bugfixes to provide a nicer experience for the next person that finds themselves\nin a similar mess.</p><p>Others are more complicated:</p><ul><li><p>The compile time of deep call graphs of async functions needs to be improved – perhaps LLVM has a degenerate edge\ncase that's easy to trigger with what  generates, or maybe it's as simple as a bad heuristic that's\nunder-utilized in other languages.</p></li><li><p>It  be worthwhile for  to special-case <code>core::ptr::drop_in_place&lt;T&gt;</code> so that it's compiled in the crate\nthat defines . That approach wouldn't work for everything – for example, generic types – but would prevent\ndownstream crates from needing to re-compile the same destructor multiple times.</p></li><li><p>There might also be room for tooling to help with isolating which parts of a codebase are taking up the most time\nduring compilation (and providing recommendations to mitigate) – although that's a longer project than just this post.</p></li></ul><p><strong>In the meantime, setting  might be just fine :)</strong></p><div><p><em>(questions? comments? Feel free to reach out below!)</em></p></div>","contentLength":24235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lkxad3/why_is_the_rust_compiler_so_slow/"},{"title":"Programming as Theory Building: Why Senior Developers Are More Valuable Than Ever","url":"https://cekrem.github.io/posts/programming-as-theory-building-naur/","date":1750935342,"author":"/u/cekrem","guid":635,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lkx4ts/programming_as_theory_building_why_senior/"},{"title":"Blender 5.0 Introducing HDR Support On Linux With Vulkan + Wayland","url":"https://www.phoronix.com/news/Blender-5.0-HDR-Linux-Wayland","date":1750935019,"author":"/u/B3_Kind_R3wind_","guid":627,"unread":true,"content":"\nThe upcoming Blender 5.0 3D modeling software application is introducing High Dynamic Range (HDR) display support on Linux when making use of Wayland -- no X11 support for HDR -- and Vulkan graphics accelerator.\n<p>HDR support for Blender 5.0 on Linux is currently considered experimental. Enabling the HDR support on Linux for the Blender creator software requires having a High Dynamic Range display (of course) and be running on a Wayland desktop, enabling Vulkan API acceleration rather than OpenGL, and enabling the feature currently deemed experimental.\n</p><p>As the Blender HDR support on Linux has been tested with a limited number of configurations so far, it's currently being treated as experimental. Depending upon testing feedback it may be promoted beyond being an \"experiment\" feature or Blender 5.0, so we'll see. In my testing of the latest Blender 5.0 alpha build on Ubuntu Linux with </p><a href=\"https://www.phoronix.com/review/samsung-g8-g81sf\">Samsung Odyssey OLED G8 G81SF</a> and <a href=\"https://www.phoronix.com/review/linux-hdr-2025\">ASUS ROG Swift OLED PG27UCDM</a> displays, this experimental feature was working out fine in my basic tests.\nMore details on this initial HDR support for Blender 5.0 on Linux with Vulkan/Wayland can see <a href=\"https://devtalk.blender.org/t/vulkan-wayland-hdr-support/41214\">this Blender DevTalk thread</a> for all the details and to share your feedback on any testing.","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lkx1kn/blender_50_introducing_hdr_support_on_linux_with/"},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1lkw82j/weekly_this_week_i_learned_twil_thread/","date":1750932058,"author":"/u/gctaylor","guid":619,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Journey from Java to Go: Why I Think Go's Packages Are Actually Better","url":"https://www.reddit.com/r/golang/comments/1lkvjpi/my_journey_from_java_to_go_why_i_think_gos/","date":1750929424,"author":"/u/hosmanagic","guid":616,"unread":true,"content":"<p>When I was going through <em>The Go Programming Language</em> (Kernighan et al.), I thought I’d just skim the chapter on packages. In Java, at least, it's a relatively unremarkable topic—something you don’t spend much time thinking about.</p><p>But Go is different. Interestingly, Go packages made me think more deeply about code organization than Java packages ever did.</p><p>The more I reflected on Go packages—especially while writing this article—the more they made sense. And to be honest, I think Java should reconsider some of its package conventions, as they might be one of the reasons for its \"notorious\" verbosity.</p>","contentLength":613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Snow - Classic Macintosh emulator","url":"https://snowemu.com/","date":1750928880,"author":"ColinWright","guid":236,"unread":true,"content":"<p>Snow emulates classic (Motorola 680x0-based) Macintosh computers. It features a graphical user interface to operate the emulated machine and provides extensive debugging capabilities. The aim of this project is to emulate the Macintosh on a hardware-level as much as possible, as opposed to emulators that patch the ROM or intercept system calls.</p><p>It currently emulates the Macintosh 128K, Macintosh 512K, Macintosh Plus, Macintosh SE, Macintosh Classic and Macintosh II.</p><p>There is a limited <a href=\"https://demo.snowemu.com/\">online demo</a> available (only the emulated machine, no user interface or other functionality from the full software).</p><p>Currently, only bleeding edge builds are available. These get generated automatically as work progresses\non the emulator.</p>","contentLength":724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44385562"},{"title":"Run web compatible Half-Life or Counter Strike 1.6 dedicated server using xash3d-fwgs and go pion","url":"https://www.reddit.com/r/golang/comments/1lktz9l/run_web_compatible_halflife_or_counter_strike_16/","date":1750923128,"author":"/u/yohimik","guid":615,"unread":true,"content":"<p>Hey there Recently I made a cgo wrapper for xash3d-fwgs engine which runs hl and cs<p> Furthermore, I added a webrtc example to demonstrate how to connect to the server from the web</p> why go?<p> go has backend session based engines like nakama, so it's easy to run something like cs2 using just cs1.6 and go</p><a href=\"https://github.com/yohimik/goxash3d-fwgs\">https://github.com/yohimik/goxash3d-fwgs</a></p>","contentLength":339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The importance of kindness in engineering","url":"https://ashouri.xyz/post/kindnessinengineering","date":1750922027,"author":"/u/AlexandraLinnea","guid":633,"unread":true,"content":"<p>There are many traits I have seen successful engineers have over my years observing and being part of development teams but there is one trait above all others that the most successful engineers have and that is kindness.</p><p>I can hear your eyes rolling from here but hear me out.</p><p>Being kind does not mean being a doormat, minding your p’s and q’s or biting your lip when your team takes an action you disagree with. Sometimes it can mean being direct, arguing or even being a bit too abrupt.</p><p>Remember when you just started out and a senior sat with you and explained some basic concepts behind their code without judgement and patience?</p><p>Remember when you saw a colleague working on a gnarly problem and you stepped in to pair with them or vice versa?</p><p>Remember when you were extremely tired and someone chased you for an update on a piece of work that was not a priority. Instead of snapping at them you took a breath and explained why you could not look into it right now but would circle back to them in a week or so?</p><p>Kindness is not only about reactive patience and being helpful but also influences the way we work. For me a feature of kindness in engineering is keeping documentation like readme’s up to date because you want to help cut down on the time it would take engineers to make progress in future on the same codebase.</p><p>Kindness can also be applied to writing code. For example when you write code you optimise on your team’s ability to read and comprehend the codebase because you want your colleagues to have an easier time onboarding and delivering features in the future rather than optimising on your own personal velocity.</p><p>Kindness can even mean you choose not to make code more readable because your empathy leads you to believing that rearranging a pattern that the team has come to become familiar with would lead to them having to spend more time to understand changes in a codebase with low churn.</p><p>Kindness also means that we write error messages with the user in mind and prioritise product features over and above technical implementation detail because we have of empathy for our users.</p><p>Kindness means that you prioritise tickets that enable the team to ship quicker by reducing setup time through things like docker compose or makefiles.</p><p>Kindness leads to greater cooperation, collaboration and transparency. The most productive teams I have worked on were not smartest but the kindest and the least productive team I ever worked on was ironically probably the smartest.</p><p>There are lots of good engineers in the world but in my opinion the best engineers are the kindest engineers because they not only deliver world class products but they also raise the effectiveness of the entire team.</p>","contentLength":2709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lktp8d/the_importance_of_kindness_in_engineering/"},{"title":"Optimizing Change-Driven Architectures - A New Cloud-Native Model with Drasi","url":"https://www.youtube.com/watch?v=PsY1OilC3A0","date":1750921222,"author":"CNCF [Cloud Native Computing Foundation]","guid":390,"unread":true,"content":"<article>Building change-driven solutions that respond to specific changes in distributed data is challenging.  This talk introduces Drasi, a CNCF Sandbox project that simplifies the design and implementation of change-driven architectures by codifying the continuous query and reaction patterns, removing the need to write custom code.</article>","contentLength":327,"flags":null,"enclosureUrl":"https://www.youtube.com/v/PsY1OilC3A0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Looking for an Open Source Kubernetes Replication Tool for Periodic Cluster Sync (Disaster Recovery Use Case)","url":"https://www.reddit.com/r/kubernetes/comments/1lktgyy/looking_for_an_open_source_kubernetes_replication/","date":1750921143,"author":"/u/Tulpar007","guid":621,"unread":true,"content":"<p>I have 2 Kubernetes clusters: one is production, the other is a standby. I want to periodically replicate all data (pods, PVCs, configs, etc.) from the prod cluster to the standby cluster.</p><p>Goal: if prod goes down, the standby can quickly take over with minimal data loss.</p><p>Looking for an open source tool that supports:</p><ul><li>PVC + resource replication</li></ul><p>So far I’ve seen: Velero, VolSync, TrilioVault CE, Stash — any recommendations or real-world experiences?</p>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM code generation may lead to an erosion of trust","url":"https://jaysthoughts.com/aithoughts1","date":1750918069,"author":"CoffeeOnWrite","guid":235,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44384610"},{"title":"How much code does that proc macro generate?","url":"https://nnethercote.github.io/2025/06/26/how-much-code-does-that-proc-macro-generate.html","date":1750916451,"author":"/u/nnethercote","guid":637,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lks869/how_much_code_does_that_proc_macro_generate/"},{"title":"CNL: Integrating MCP metadata in your internal developer platform","url":"https://www.youtube.com/watch?v=MhyMLykUMUc","date":1750913422,"author":"CNCF [Cloud Native Computing Foundation]","guid":389,"unread":true,"content":"<article>MCP, Model Context Protocol, is clearly the hot topic of 2025 and while we are seeing more and more interesting use cases around this, no one has really yet focused on all the metadata that MCP brings to the table: Tools description, Tool parameters description, prompt description. All of this is really useful information that can be used by the developer building AI Infused applications.\n\nIt’s also totally aligned with the Platform Engineering vision which tries to streamline the service catalogs to its platform user.\n\nJoin me in the mainly live coding session to see how to integrate MCP Metadata into your Platform Engineering strategy.</article>","contentLength":647,"flags":null,"enclosureUrl":"https://www.youtube.com/v/MhyMLykUMUc?version=3","enclosureMime":"","commentsUrl":null},{"title":"How Do You Handle Orphaned Processes?","url":"https://www.reddit.com/r/golang/comments/1lknghk/how_do_you_handle_orphaned_processes/","date":1750901224,"author":"/u/Hamguy1234","guid":612,"unread":true,"content":"<p>For a little bit of context, I'm currently writing a library to assist in the creation of a chess GUI. This library implements the UCI chess protocol, and as part of that it will be necessary to run a variety of uci compatible chess engines.</p><p>The straightforward approach is to use , and then if the engine begins to misbehave call . The obvious issue with this is that child processes are not killed and in the case of a chess engine these child processes could run for a very long time while taking a lot of cpu. To me it seems like it comes down to two options, but if Go has something more graceful than either of these I would love to know.</p><ul><li>Ignore child processes and hope they terminate promptly, (this seems to put too much faith in the assumption that other programmers will prevent orphaned processes from running for too long.)</li><li>Create OS dependent code for killing a program (such as posix process groups).</li></ul><p>The second option seems to be the most correct, but it is more work on my side, and it forces me to say my library is only supported on certain platforms. </p>","contentLength":1067,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you see the current state and future of Rust? And, will Rust get popular in game dev?","url":"https://www.reddit.com/r/rust/comments/1lkmloc/how_do_you_see_the_current_state_and_future_of/","date":1750898741,"author":"/u/lettsten","guid":636,"unread":true,"content":"<p>I'm a hobbyist who've been eyeing Rust for a while, dabbled a bit. As a hobbyist I don't have my finger on the industrial pulse and would like to hear your thoughts and insights about the current state of Rust in general—things that are hard for me to look up on a wiki page and that requires the insights of those of you who work with it regularly or semi-regularly.</p><p><strong>What do you think about the current state of Rust as a language, ecosystem and community?</strong></p><p>I've seen some flak about async in Rust. Do you agree with it? How happy are you about the current state of the language? Is Rust your favourite language? What are your biggest gripes with the language, and do you think they will be resolved within the next 2-5 years?</p><p>From what I understand, Rust jobs are rare. Is your impression that they are becoming more common? Do you think Rust will become more prevalent than C or C++ at some point?</p><p>Are you happy with the Rust ecosystem, tooling, library availability and so on? Which areas shine, and which are most lacking? What are your opinions on the Rust community, in terms of demographics, friendliness, activity, open-source work and so on?</p><p>My impression is that Rust is most suited to systems level programming, especially critical components where correctness is essential. Do you see Rust taking over other segments or domains?</p><p>Reason I ask these questions is honestly because I would love to get psyched about Rust again, and because I would like an honest and well-informed impression of the current state of the language.</p><p>Any and all insights are very welcome!</p><p>Edit: <strong><em>I'm mostly interesting in the state of Rust as a whole</em></strong>, the gamedev question from the subject is secondary.</p>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DockedUp: A Terminal Dashboard for Docker Containers, Built in Python","url":"https://github.com/anilrajrimal1/dockedup","date":1750898553,"author":"/u/Fragrant_Letter8595","guid":631,"unread":true,"content":"<p>I’ve been working on , a CLI tool that makes monitoring Docker containers easier and more intuitive. If you’re tired of juggling , , and switching terminals to check logs or restart containers, this might be for you!</p><p>DockedUp is a real-time, interactive dashboard that displays your Docker containers’ status, health, CPU, and memory usage in a clean, color-coded terminal view. It automatically groups containers by docker-compose projects and uses emojis to make status (Up 🟢, Down 🔴) and health (Healthy ✅, Unhealthy ⚠️) instantly clear. Navigate containers with arrow keys and use hotkeys to: - : View live logs - : Restart a container - : Stop a container - : Open a shell inside a container</p><p>DockedUp is designed for developers and DevOps engineers who work with Docker containers and want a quick, unified view of their environment without leaving the terminal. It’s ideal for those managing docker-compose stacks in development or small-scale production setups. Whether you’re a Python enthusiast, a CLI lover, or a DevOps pro looking to streamline workflows, DockedUp is built to save you time and hassle.</p><p>Unlike  and , which require multiple commands and terminal switching, DockedUp offers a single, live-updating dashboard with interactive controls. Compared to tools like Portainer (web-based) or lazydocker (another CLI), DockedUp is lightweight, focuses on docker-compose project grouping, and integrates emoji-based visual cues for quick status checks. It’s Python-based, easy to install via PyPI, and doesn’t need a web server, making it a great fit for terminal-centric workflows.</p><p>It’s on PyPI and takes one command to install (I recommend  for CLI tools): <code>bash pipx install dockedup </code> Or: <code>bash pip install dockedup </code> Then run  to start the monitor. Check out the <a href=\"https://github.com/anilrajrimal1/dockedup\">GitHub repo</a> for more details and setup instructions. If you like the project, I’d really appreciate a ⭐ on GitHub to help spread the word!</p><p>I’d love to hear your thoughts—any features you’d like to see or issues you run into? Contributions are welcome (it’s MIT-licensed). </p><p>What’s your go-to way to monitor Docker containers?</p><p>Thanks for checking it out! 🚀</p>","contentLength":2171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lkmja0/dockedup_a_terminal_dashboard_for_docker/"},{"title":"ArgoCD deploying sensitive non-Secrets","url":"https://www.reddit.com/r/kubernetes/comments/1lkmgnf/argocd_deploying_sensitive_nonsecrets/","date":1750898341,"author":"/u/nullvar2000","guid":622,"unread":true,"content":"<p>Happy Wednesday fellow Kubernetes enthusiasts! I have a homelab cluster that I've spent quite a bit of time learning and implementing Gitops using ArgoCD. I'm still planning out my secrets management, but I've run into a question that's somewhat related. How do I manage sensitive parameters in non-secrets? I'm talking about things like hostnames, domains, IP addresses, etc. </p><p>For example, ingresses have my purchased domain included and even though I'm only using internal DNS records for them, I'd rather not have that kind of information public on Github.</p><p>After some research, it would seem FluxCD has a post build variable substitution capability that could take care of this, but I'd like to find a solution using Kustomize or ArgoCD. Does anybody have another solution to this kind of data? Am I just being too paranoid about this?</p>","contentLength":836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"lightning-image-viewer 0.2.0","url":"https://github.com/shatsky/lightning-image-viewer/releases/tag/v0.2.0","date":1750896610,"author":"/u/shatsky","guid":624,"unread":true,"content":"<div><p>Fast and lightweight desktop image viewer featuring minimalistic \"transparent fullscreen overlay\" UI/UX with controls similar to map apps. This is 1st release featuring pre-built binaries (for Ubuntu 25.04 and Windows, built on GitHub CI/CD) and web demo ( <a href=\"https://shatsky.github.io/lightning-image-viewer/\">https://shatsky.github.io/lightning-image-viewer/</a> )</p></div>   submitted by   <a href=\"https://www.reddit.com/user/shatsky\"> /u/shatsky </a>","contentLength":338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lkluyb/lightningimageviewer_020/"},{"title":"Puerto Rico's Solar Microgrids Beat Blackout","url":"https://spectrum.ieee.org/puerto-rico-solar-microgrids","date":1750894876,"author":"ohjeez","guid":234,"unread":true,"content":"<p>When power went out across all of <a href=\"https://spectrum.ieee.org/tag/puerto-rico\">Puerto Rico</a> on 16 April, a lot of the lights in the town of Adjuntas stayed on. There, nestled in the mountains on the midwestern side of the island, a combination of experimental <a href=\"https://spectrum.ieee.org/microgrid\" target=\"_self\">microgrids</a>, <a href=\"https://spectrum.ieee.org/tag/solar-panels\">solar panels</a>, and storage kept power on for many businesses and residents. The rest of the island waited over 24 hours, and in some cases longer, for electricity to be restored.</p><p>The blackout was the latest in a series of power interruptions that have come to define Puerto Rico’s aging <a href=\"https://spectrum.ieee.org/tag/electrical-grid\">electrical grid</a>. Vegetation was to blame for April’s blackout, according to <a href=\"https://lumapr.com/?lang=en\" rel=\"noopener noreferrer\" target=\"_blank\">LUMA Energy</a>, the private company that <a href=\"https://spectrum.ieee.org/the-privatization-of-puerto-rico-power-grid-mired-in-controversy\" target=\"_self\">manages the island’s grid</a>. A faulty old cable triggered the near total blackout on New Year’s Eve 2024, the company said. Tropical storm Ernesto’s <a href=\"https://www.npr.org/2024/08/14/nx-s1-5075256/hurricane-ernesto-puerto-rico-power-outage\" rel=\"noopener noreferrer\" target=\"_blank\">strong winds </a>knocked out half of the island’s power in August 2024. </p><p>The problems are the result of decades of mismanagement and disinvestment in the island’s grid infrastructure. Neglecting to keep up with regular maintenance and failing to meet increasing demand for <a href=\"https://spectrum.ieee.org/tag/power-generation\">power generation</a> have contributed to the disarray. The long-standing issues set the stage for the grid to be crushed in 2017 by <a href=\"https://spectrum.ieee.org/tag/hurricane-maria\">Hurricane Maria</a>, the United States’ <a href=\"https://www.cbsnews.com/news/top-10-deadliest-hurricanes-in-us-history-katrina-maria-galveston/\" rel=\"noopener noreferrer\" target=\"_blank\">second deadliest,</a> which plunged Puerto Rico into months-long darkness and claimed nearly 3,000 lives. </p><p>After that <a href=\"https://spectrum.ieee.org/tag/hurricane\">hurricane</a>, the island’s state-run utility, Puerto Rico <a href=\"https://spectrum.ieee.org/tag/electric-power\">Electric Power</a> Authority (PREPA), contracted with private entities for power generation, transmission, and distribution in the hopes of fixing the grid. Over <a href=\"https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2025/01/10/fact-sheet-biden-harris-administrations-historic-investments-in-puerto-ricos-energy-grid/\" rel=\"noopener noreferrer\" target=\"_blank\">$20 billion</a> in U.S. federal <a href=\"https://spectrum.ieee.org/tag/disaster-relief\">disaster relief</a> was awarded by the Federal Emergency Management Agency (<a href=\"https://spectrum.ieee.org/tag/fema\">FEMA</a>) to improve the grid and boost its resilience. Yet bureaucratic red tape and politics in Puerto Rico and on the U.S. mainland have hindered much of that money from being spent.</p><p>Now, the U.S. <a href=\"https://spectrum.ieee.org/tag/department-of-energy\">Department of Energy</a> plans to redirect$365 million previously earmarked for <a href=\"https://spectrum.ieee.org/tag/rooftop-solar\">rooftop solar</a> toward infrastructure on Puerto Rico’s <a href=\"https://www.eia.gov/state/print.php?sid=RQ\" target=\"_blank\">majority</a> fossil-fuel-powered grid, according to an <a href=\"https://www.energy.gov/articles/energy-department-redirect-365-million-support-grid-resilience-efforts-puerto-rico\" rel=\"noopener noreferrer\" target=\"_blank\">announcement</a> from the agency on May 21. The money will  support “practical fixes and emergency activities that offer a faster, more impactful solution to the current crisis,” the agency said. This will include “system flexibility and response, power flow and control, component strength, supply security, and safety,” according to the announcement. </p><p>The move <a href=\"https://apnews.com/article/puerto-rico-federal-funds-energy-department-solar-379e4dc7b361268819bf60dd0cf9b0dc\" rel=\"noopener noreferrer\" target=\"_blank\">sparked an outcry</a> from Puerto Rico’s solar industry and U.S. Representative Nydia Velazquez of New York. Velazquez, who is from Puerto Rico, called the move “shameful” in a <a href=\"https://x.com/NydiaVelazquez/status/1925263284703248607\" rel=\"noopener noreferrer\" target=\"_blank\">post on X</a>, saying the money was designed to serve vulnerable communities on the island.</p><h2>Solar Energy’s Role in Puerto Rico’s Grid</h2><p>The ongoing political turmoil and bottlenecked federal funding have prompted the widespread development of solar-plus-storage systems across the island that are privately financed via leases, loans, or Power Purchase Agreements (PPAs). Each month, the island sees around 4,000 solar-plus-battery storage systems come online, Rúa-Jovet says. These installations are connected to the grid but can also operate during <a href=\"https://spectrum.ieee.org/tag/blackouts\">blackouts</a>.</p><p>At the end of March, LUMA <a href=\"https://energia.pr.gov/wp-content/uploads/sites/7/2025/05/Resumen-Metricas-Master_April2025-.xlsx\" rel=\"noopener noreferrer\" target=\"_blank\">reported</a> over 1.14 gigawatts of grid-connected distributed solar capacity, with an additional 2.34 gigawatt-hours of distributed <a href=\"https://spectrum.ieee.org/tag/batteries\">batteries</a> connected to the grid. <a href=\"https://spectrum.ieee.org/tag/solar-power\">Solar power</a> produces over 2 terawatt-hours of electricity each year, which accounts for more than 12.5 percent of Puerto Rico’s total residential electricity consumption annually. The majority of that power is generated from residential solar, and capacity continues to grow as more residents install systems with private financing. </p><p>Adjuntas, which has a population of about 18,000, took a more <a href=\"https://spectrum.ieee.org/microgrid\" target=\"_blank\">experimental approach</a>. The town’s local environmental nonprofit <a href=\"https://casapueblo.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Casa Pueblo</a> teamed up with researchers from the U.S. Department of Energy’s <a href=\"https://www.ornl.gov/\" rel=\"noopener noreferrer\" target=\"_blank\">Oak Ridge National Laboratory</a> in Oak Ridge, Tenn., to develop a way to connect multiple <a href=\"https://spectrum.ieee.org/tag/microgrids\">microgrids</a> to exchange power with one another, all without having to be hooked up to Puerto Rico’s grid. The strategy, called grid orchestration, ensures that if power is knocked out on one of the installations, the others aren’t compromised. It’s what kept multiple areas in Adjuntas electrified during April’s island-wide blackout.</p><p>During the blackout, Casa Pueblo and the Oak Ridge researchers were completing the testing of the orchestration strategy with three of the five microgrids connected in Adjuntas. These three microgrids are connected to the grid via <a href=\"https://spectrum.ieee.org/tag/net-metering\">net metering</a>. The remaining two grids are isolated.</p><p>“By decentralizing, it’s creating a more resilient and redundant energy setup,” says <a href=\"https://spectrum.ieee.org/u/arturo-massol-deya\" target=\"_self\">Arturo Massol-Deyá</a>, Casa Pueblo’s executive director. “Engineers will say: If you have redundancy, that’s more resilient; that’s better.”</p><p>The teams demonstrated trading energy from one microgrid to the other, and vice versa. This kind of transfer enables the system to overcome energy limitations during peak demand times and draw from additional storage at night when the sun is down. Together, the town’s five microgrids provide 228 kilowatts of <a href=\"https://spectrum.ieee.org/tag/photovoltaic\">photovoltaic</a> capacity and an additional 1.2 megawatt-hours of storage, which serve residences and 15 commercial businesses. It’s a small amount of power, but an example of a way for systems to operate independently from the grid. </p><h2>Expanding Microgrid Connections in Adjuntas</h2><p>Moving forward, Massol-Deyá’s plan is to continue improving and expanding the bottom-up approach to microgrid connections. On April 20, Casa Pueblo launched a lab in Adjuntas called the <a href=\"https://www.hasercambio.org/en/casapueblo/\" rel=\"noopener noreferrer\" target=\"_blank\">Community Laboratory for the Energy Transition</a> with the goal of bringing together academics and industry experts to test new microgrid technology as it develops. </p><p>The next milestone, Massol-Deyá says, will be successfully connecting microgrids that are not in close geographic proximity. “In Adjuntas, we’re bridging the gap between simulation and theoretical work with a real application,” he says.</p><p>As warmer months approach, Puerto Rico is gearing up island-wide for a <a href=\"https://www.sanjuandailystar.com/post/energy-czar-puerto-rico-likely-to-experience-blackouts-this-summer\" rel=\"noopener noreferrer\" target=\"_blank\">season of power failures</a> as energy demand will likely exceed Puerto Rico’s generation capacity. This will likely be compounded by a <a href=\"https://www.noaa.gov/news-release/noaa-predicts-above-normal-2025-atlantic-hurricane-season\" rel=\"noopener noreferrer\" target=\"_blank\">stronger-than-normal</a> Atlantic hurricane season. </p><p>Rúa-Jovet maintains that solar and batteries are an easily dispatchable resource that make a “good dent” in <a href=\"https://spectrum.ieee.org/tag/resiliency\">resiliency</a> against island-wide power failures. Massol-Deyá agrees and says that even with the government turning toward what he calls an “obsolete” model of fossil fuel power, the Puerto Rican people are embracing solar.</p><p>“It’s not top-down: It’s not by LUMA, it’s not by the government. It has been pushed by the people. You have a huge and significant investment by the people on solar,” Massol-Deyá says. </p>","contentLength":6776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44382834"},{"title":"Define policy forbidding use of AI code generators","url":"https://github.com/qemu/qemu/commit/3d40db0efc22520fa6c399cf73960dced423b048","date":1750894015,"author":"todsacerdoti","guid":268,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44382752"},{"title":"Wails goroutine question","url":"https://www.reddit.com/r/golang/comments/1lkjzr3/wails_goroutine_question/","date":1750891602,"author":"/u/New_Okra5546","guid":613,"unread":true,"content":"<p><code>Wails is a lightweight and fast Electron/Tauri alternative for Go, and uses WebKit2</code></p><p><code>What makes Wails special is the following:</code></p><ul><li><code>Bind your Go code to the frontend so it can be called from JavaScript</code></li></ul><p>Now if you created lets say a new Wails + React project. It has a bound go method Greet, which you can call via JavaScript. I noticed, that the go methods like Greet are by default executed on another goroutine id. So I don't have to add goroutines myself. Of course, I don't want to hang my app while e.g. an api call is underway.</p><p>I've searched for \"go \" and \"go func\", but didn't see it. Thanks</p><p>Edit 1: In the frontend, this calls the bound go method:</p><p><code>window.WailsInvoke('C' + JSON.stringify(payload));</code></p><p>Then in the go side I think this receives the message, which starts with 'C' if not obfuscated:</p><p>and on Line 45 <code>registeredMethod.Call(args)</code></p><p>and on Line 72 , but I didn't see a go keyword yet, so I am still wondering where it goes into a different goroutine id to not block id 1. The go library 'reflect' is involved, but AI says \"The Go  package itself does not provide any functionality to run methods in a different goroutine \"secretly\" or automatically.\"</p><p>Am I wrong and I have to implement a goroutine myself in each bound go method to not block the wails app, that runs on id 1?</p>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop talking about Fedora change proposals like they have already decided on it.","url":"https://www.reddit.com/r/linux/comments/1lkjj8l/stop_talking_about_fedora_change_proposals_like/","date":1750890433,"author":"/u/wowieniceusername","guid":628,"unread":true,"content":"<p>Seriously. Everytime some controversial change gets proposed on Fedora, someone reports on it without making it clear that it only  get through after enough thought and discussion, and the entire comment section devolves into people yelling about this and that even though literally anybody can propose a change over there. And alot of the time those proposals don't even get through.</p><p>I get that potential major change is big news and a good source for discussions but dear god in the past week alone I've seen two different news about a Fedora change proposal where people act like the developers have already decided on it and it has zero pushback and is going to happen soon (removing 32-bit support being one of them). I don't even use Fedora but it gets really annoying. Atleast make it clear.</p><p>With that said I realized that readers will probably just be stupid and will overreact regardless but I don't think it hurts to be as clear as possible.</p>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bazzite Would Shut Down If Fedora Goes Ahead With Removing 32-Bit","url":"https://linux.slashdot.org/story/25/06/25/2042242/bazzite-would-shut-down-if-fedora-goes-ahead-with-removing-32-bit?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750889400,"author":"BeauHD","guid":296,"unread":true,"content":"If Fedora drops 32-bit support, the gaming-focused Bazzite project would be forced to shut down, according to its founder Kyle Gospodnetich. \"As much as I'd like this change to happen, it's too soon,\" said Gospodneitch in a post. \"This change would kill off projects like Bazzite entirely right as Fedora is starting to make major headway in the gaming space. Neal Gompa already pointed out basic use cases that would be broken even if someone built the packages Steam itself needs to function.\"\n \nHe continued: \"It's also causing irreparable damage to Fedora from a PR standpoint. I have been inundated all day with people sharing news articles and being genuinely concerned Steam is gong to stop working on their Fedora/Bazzite machines. I would argue not only should this change be rejected, the proposal should be rescinded to limit further damage to Fedora as a project. Perhaps open a separate one to talk about changing build architecture to build fewer 32-bit packages?\"\n \nWhen pushed further, Gospodnetich said: \"I'm speaking as it's founder, if this change is actually made as it is written the best option for us is to just go ahead and disband the project.\"","contentLength":1169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are your must have Go packages?","url":"https://www.reddit.com/r/golang/comments/1lkioqb/what_are_your_must_have_go_packages/","date":1750888287,"author":"/u/fenugurod","guid":617,"unread":true,"content":"<div><p>I've been using for many years and I tend to use the same stack all the time because it works and I know the packages well enough, but I'm wondering if there is anything new that it's worth exploring.</p><p>This is a very open question so feel free to answer whatever you want. For example this is what I need for my Go services:</p><ul><li>Tests: testify and testcontainers</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/fenugurod\"> /u/fenugurod </a>","contentLength":387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A new pyramid-like shape always lands the same side up","url":"https://www.quantamagazine.org/a-new-pyramid-like-shape-always-lands-the-same-side-up-20250625/","date":1750881667,"author":"robinhouston","guid":233,"unread":true,"content":"<p>Achieving the right balance between the weight of the loading zone and the weight of the rest of the tetrahedron is easy in the abstract realm of mathematics —&nbsp;you can define the weight distribution without a care for whether it’s physically possible. You might, for instance, let parts of the shape weigh nothing at all, while concentrating a large amount of mass in other parts.</p><p>But that wasn’t entirely satisfying to the mathematicians. Almádi, Dawson and Domokos wanted to hold the shape in their hands. Was it possible to make a monostable tetrahedron in the real world, with real materials?</p><p>The team returned to their computer search. They considered the various ways in which monostable tetrahedra might tip onto their stable face. For instance, one kind of tetrahedron might follow a very simple path: Face A tips to Face B, which tips to Face C, which tips to Face D. But in a different tetrahedron, Face A might tip to Face B, and both Face B and Face D will tip to Face C.</p><p>The loading zones for these different tetrahedra look very different. The team calculated that to get one of these “falling patterns” to work, they would need to construct part of the shape out of a material about 1.5 times as dense as the sun’s core.</p><p>They focused on a more feasible falling pattern. Even so, part of their tetrahedron would have to be about 5,000 times as dense as the rest of it. And the materials had to be stiff — light, flimsy materials that could bend would ruin the project, since it’s easy to make a round or smooth shape (like the roly-poly) monostable.</p><p>In the end, they designed a tetrahedron that was mostly hollow. It consisted of a lightweight carbon fiber frame and one small portion constructed out of tungsten carbide, which is denser than lead. For the lighter portions to have as little weight as possible, even the carbon fiber frames had to be hollow.</p><p>With this blueprint in hand, Domokos got in touch with a <a href=\"https://cncnagykft.hu/\">precision engineering company</a> in Hungary to help build the tetrahedron. They had to be incredibly accurate in their measurements, even when it came to the weight of the tiny amounts of glue used to connect each of the shape’s faces. Several frustrating months and several thousand euros later, the team had a lovely model that didn’t work at all. Then Domokos and the chief engineer of the model spotted a glob of stray glue clinging to one of its vertices. They asked a technician to remove it. About 20 minutes later, the glue was gone and Almádi received a text from Domokos.</p><p>“It works,” the message read. Almádi, who was on a walk, started jumping up and down in the street. “Seeing the lines on the computer is very far from reality,” he said. “That we designed it, and it works, it’s kind of fantastic.”</p><p>“I wanted to be an architect,” he added. “So this is still very strange for me — how did I end up here?”</p><p>In the end, the work on monostable tetrahedra didn’t involve any particularly sophisticated math, according to <a href=\"https://www.math.brown.edu/reschwar/\">Richard Schwartz</a> of Brown University. But, he said, it’s important to ask this kind of question in the first place. It’s the kind of problem that’s often easiest to overlook. “It’s a surprising thing, a leap, to conjecture that these things would exist,” Schwartz said.</p><p>At the moment, it’s not clear what new theoretical insights the model of the monostable tetrahedron will provide — but experimenting with it might help mathematicians uncover other intriguing questions to ask about polyhedra. In the meantime, Domokos and Almádi are working to apply what they learned from their construction to help engineers design lunar landers that can turn themselves right side up after falling over.</p><p>In any case, sometimes you just need to see something to believe it, Schwartz said. “Even for theoretical math, geometry especially, people are kind of right to be skeptical because it’s quite hard to reason spatially. And you can make mistakes, people do.”</p><p>“Conway didn’t say anything about it, he just suggested it — never proved it, never proved it wrong, nothing. And now here we are, I don’t know, 60 years later,” Almádi said. “If he were still alive, we could put this on his desk and show him: You were right.”</p>","contentLength":4243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44381297"},{"title":"-2000 Lines of code","url":"https://www.folklore.org/Negative_2000_Lines_Of_Code.html","date":1750881193,"author":"xeonmc","guid":232,"unread":true,"content":"<p>In early 1982, the Lisa software team was trying to buckle down for the big push to ship the software within the next six months.  Some of the managers decided that it would be a good idea to track the progress of each individual engineer in terms of the amount of code that they wrote from week to week.  They devised a form that each engineer was required to submit every Friday, which included a field for the number of lines of code that were written that week.</p><p>\nBill Atkinson, the author of Quickdraw and the main user interface designer, who was by far the most important Lisa implementer, thought that lines of code was a silly measure of software productivity.  He thought his goal was to write as small and fast a program as possible, and that the lines of code metric only encouraged writing sloppy, bloated, broken code.</p><p>\nHe recently was working on optimizing Quickdraw's region calculation machinery, and had completely rewritten the region engine using a simpler, more general algorithm which, after some tweaking, made region operations almost six times faster.  As a by-product, the rewrite also saved around 2,000 lines of code.</p><p>\nHe was just putting the finishing touches on the optimization when it was time to fill out the management form for the first time.  When he got to the lines of code part, he thought about it for a second, and then wrote in the number: -2000.</p><p>\nI'm not sure how the managers reacted to that, but I do know that after a couple more weeks, they stopped asking Bill to fill out the form, and he gladly complied.\n  </p>","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44381252"},{"title":"Writing Toy Software Is A Joy","url":"https://blog.jsbarretto.com/post/software-is-joy","date":1750880866,"author":"/u/NXGZ","guid":634,"unread":true,"content":"<p>I am a huge fan of Richard Feyman’s famous quote:</p><blockquote><p>“What I cannot create, I do not understand”</p></blockquote><p>I think it’s brilliant, and it remains true across many fields (if you’re willing to be a little creative with the\ndefinition of ‘create’). It is to this principle that I believe I owe everything I’m truly good at. Some will tell you\nto avoid reinventing the wheel, but they’re wrong: you  build your own wheel, because it’ll teach you more about\nhow they work than reading a thousand books on them ever will.</p><p>In 2025, the beauty and craft of writing software is being eroded. AI is threatening to replace us (or, at least, the\nmost joyful aspects of our craft) and software development is being increasingly commodified, measured, packaged, and\nindustrialised. Software development needs more simple joy, and I’ve found that creating toy programs is a great way to\nremember why I started working with computers again.</p><p>Toy programs follow the 80:20 rule: 20% of the work, 80% of the functionality. The point is  to build\nproduction-worthy software (although it is true that some of the best production software began life as a toy).\nAggressively avoid over-engineering, restrict yourself to only whatever code is necessary to achieve your goal. Have\nevery code path panic/crash until you’re forced to implement it to make progress. You might be surprised by just how\neasy it is to build toy versions of software you might previously have considered to be insummountably difficult to\ncreate.</p><p>I’ve been consistently surprised by just how often some arcane nugget of knowledge I’ve acquired when working on a toy\nproject has turned out to be immensely valuable in my day job, either by giving me a head-start on tracking down a\nproblem in a tool or library, or by recognising mistakes before they’re made.</p><p>Understanding the constraints that define the shape of software is vital for working with it, and there’s no better way\nto gain insight into those constraints than by running into them head-first. You might even come up with some novel\nsolutions!</p><p>Here is a list of toy programs I’ve attempted over the past 15 years, rated by difficulty and time required. These\nratings are estimates and assume that you’re already comfortable with at least one general-purpose programming language\nand that, like me, you tend to only have an hour or two per day free to write code. Also included are some suggested\nresources that I found useful.</p><h3>Regex engine (difficulty = 4/10, time = 5 days)</h3><p>A regex engine that can read a POSIX-style regex program and recognise strings that match it. Regex is simple yet\nshockingly expressive, and writing a competent regex engine will teach you everything you need to know about using the\nlanguage too.</p><h3>x86 OS kernel (difficulty = 7/10, time = 2 months)</h3><p>A multiboot-compatible OS kernel with a simple CLI, keyboard/mouse driver, ANSI escape sequence support, memory manager,\nscheduler, etc. Additional challenges include writing an in-memory filesystem, user mode and process isolation, loading\nELF executables, and supporting enough video hardware to render a GUI.</p><h3>GameBoy/NES emulator (difficulty = 6/10, time = 3 weeks)</h3><p>A crude emulator for the simplest GameBoy or NES games. The GB and the NES are classics, and both have relatively simple\ninstruction sets and peripheral hardware. Additional challenges include writing competent PPU (video) and PSG (audio)\nimplementations, along with dealing with some of the more exotic cartridge formats.</p><h3>GameBoy Advance game (difficulty = 3/10, time = 2 weeks)</h3><p>A sprite-based game (top-down or side-on platform). The GBA is a beautiful little console to write code for and there’s\nan active and dedicated development community for the console. I truly believe that the GBA is one of the last game\nconsoles that can be fully and completely understood by a single developer, right down to instruction timings.</p><h3>Physics engine (difficulty = 5/10, time = 1 week)</h3><p>A 2D rigid body physics engine that implements Newtonian physics with support for rectangles, circles, etc. On the\nsimplest end, just spheres that push away from one-another is quite simple to implement. Things start to get complex\nwhen you introduce more complex shapes, angular momentum, and the like. Additional challenges include making collision\nresolution fast and scaleable, having complex interactions move toward a steady state over time, soft-body interactions,\netc.</p><h3>Dynamic interpreter (difficulty = 4/10, time = 1-2 weeks)</h3><p>A tree-walking interpreter for a JavaScript-like language with basic flow control. There’s an unbounded list of extra\nthings to add to this one, but being able to write programs in my own language still gives me child-like elation. It\nfeels like a sort of techno-genesis: once you’ve got your own language, you can start building the universe within it.</p><h3>Compiler for a C-like (difficulty = 8/10, time = 3 months)</h3><p>A compiler for a simply-typed C-like programming language with support for at least one target archtecture. Extra\nchallenges include implementing some of the most common optimisations (inlining, const folding, loop-invariant code\nmotion, etc.) and designing an intermediate representation (IR) that’s general enough to support multiple backends.</p><h3>Text editor (difficulty = 5/10, time = 2-4 weeks)</h3><p>This one has a lot of variability. At the blunt end, simply reading and writing a file can be done in a few lines of\nPython. But building something that’s closer to a daily driver gets more complex. You could choose to implement the UI\nusing a toolkit like QT or GTK, but I personally favour an editor that works in the console. Properly handling unicode,\nsyntax highlighting, cursor movement, multi-buffer support, panes/windows, tabs, search/find functionality, LSP support,\netc. can all add between a week or a month to the project. But if you persist, you might join the elite company of those\ndevelopers who use an editor of their own creation.</p><h3>Async runtime (difficulty = 6/10, time = 1 week)</h3><p>There’s a lot of language-specific variability as to what ‘async’ actually means. In Rust, at least, this means a\nlibrary that can ingest  tasks and poll them concurrently until completion. Adding support for I/O waking\nmakes for a fun challenge.</p><h3>Hash map (difficulty = 4/10, time = 3-5 days)</h3><p>Hash maps (or sets/dictionaries, as a higher-level language might call them) are a programmer’s bread &amp; butter. And yet,\nsurprisingly few of us understand how they really work under the bonnet. There are a plethora of techniques to throw\ninto the mix too: closed or open addressing, tombstones, the robin hood rule, etc. You’ll gain an appreciation for when\nand why they’re fast, and also when you should just use a vector + linear search.</p><h3>Rasteriser / texture-mapper (difficulty = 6/10, time = 2 weeks)</h3><p>Most of us have played with simple 3D graphics at some point, but how many of us truly understand how the graphics\npipeline works and, more to the point, how to fix it when it doesn’t work? Writing your own software rasteriser will\ngive you that knowledge, along with a new-found appreciation for the beauty of vector maths and half-spaces that have\napplications across many other fields. Additional complexity involves properly implementing clipping, a Z-buffer, N-gon\nrasterisation, perspective-correct texture-mapping, Phong or Gouraud shading, shadow-mapping, etc.</p><h3>SDF Rendering (difficulty = 5/10, time = 3 days)</h3><p>Signed Distance Fields are a beautifully simple way to render 3D spaces defined through mathematics, and are perfectly\nsuited to demoscene shaders. With relatively little work you can build yourself a cute little visualisation or some\nmoving shapes like the graphics demos of the 80s. You’ll also gain an appreciation for shader languages and vector\nmaths.</p><h3>Voxel engine (difficulty = 5/10, time = 2 weeks)</h3><p>I doubt there are many reading this that haven’t played Minecraft. It’s surprisingly easy to build your own toy voxel\nengine cut from a similar cloth, especially if you’ve got some knowledge of 3D graphics or game development already. The\nsimplicity of a voxel engine, combined with the near-limitless creativity that can be expressed with them, never ceases\nto fill me with joy. Additional complexity can be added by tackling textures, more complex procedural generation,\nfloodfill lighting, collisions, dynamic fluids, sending voxel data over the network, etc.</p><h3>Threaded Virtual Machine (difficulty = 6/10, time = 1 week)</h3><p>Writing interpreters is great fun. What’s more fun? . If you keep pushing interpreters as far as\nthey can go without doing architecture-specific codegen (like AOT or JIT), you’ll eventually wind up (re)discovering\n (not to be confused with multi-threading, which is a very different beast). It’s a beautiful way of\nweaving programs together out highly-optimised miniature programs, and a decent implementation can even give an AOT\ncompiler a run for its money in the performance department.</p><h3>GUI Toolkit (difficulty = 6/10, time = 2-3 weeks)</h3><p>Most of us have probably cobbled together a GUI program using tkinter, GTK, QT, or WinForms. But why not try writing\nyour GUI toolkit? Additional complexity involves implementing a competent layout engine, good text shaping (inc.\nunicode support), accessibility support, and more. Fair warning: do not encourage people to use your tool unless it’s\n - the world has enough GUIs with little-to-no accessibility or localisation support.</p><h3>Orbital Mechanics Sim (difficulty = 6/10, time = 1 week)</h3><p>A simple simulation of Newtonian gravity can be cobbled together in a fairly short time. Infamously, gravitational\nsystems with more than two bodies cannot be solved analytically, so you’ll have to get familiar with iterative\n methods. Additional complexity comes with implementing more precise and faster integration methods,\naccounting for relativistic effects, and writing a visualiser. If you’ve got the maths right, you can even try plugging\nin real numbers from NASA to predict the next high tide or full moon.</p><h3>Bitwise Challenge (difficulty = 3/10, time = 2-3 days)</h3><p>Here’s one I came up with for myself, but I think it would make for a great game jam: write a game that only persists 64\nbits of state between subsequent frames. That’s 64 bits for everything: the entire frame-for-frame game state should be\nreproducible using only 64 bits of data. It sounds simple, but it forces you to get incredibly creative with your game\nstate management. Details about the rules can be found on the GitHub page below.</p><h3>An ECS Framework (difficulty = 4/10, time = 1-2 weeks)</h3><p>For all those game devs out there: try building your own <a href=\"https://en.wikipedia.org/wiki/Entity_component_system\">ECS</a>\nframework. It’s not as hard as you might think (you might have accidentally done it already!). Extra points if you can\nbuild in safety and correctness features, as well as good integration with your programming language of choice’s type\nsystem features.</p><p>I built a custom ECS for my <a href=\"https://www.youtube.com/watch?v=nS5rj80L-pk\">Super Mario 64 on the GBA</a> project due to the\nunique performance and memory constraints of the platform, and enjoyed it a lot.</p><h3>CHIP-8 Emulator (difficulty = 3/10, time = 3-6 days)</h3><p>The <a href=\"https://en.wikipedia.org/wiki/CHIP-8\">CHIP-8</a> is a beautifully simple virtual machine from the 70s. You can write\na fully compliant emulator in a day or two, and there are an enormous plethora of fan-made games that run on it.\n<a href=\"https://github.com/zesterer/emul8/raw/refs/heads/master/test/test.ch8\">Here’s</a> a game I made for it.</p><h3>Chess engine (difficulty = 5/10, time = 2-5 days)</h3><p>Writing a chess engine is great fun. You’ll start off with every move it makes being illegal, but over time it’ll get\nsmart and smarter. Experiencing a loss to your own chess engine really is a rite of passage, and it feels magical.</p><h3>POSIX shell (difficulty = 4/10, time = 3-5 days)</h3><p>We interact with shells every day, and building one will teach you can incredible amount about POSIX - how it works, and\nhow it doesn’t. A simple one can be built in a day, but compliance with an existing shell language will take time and\nteach you more than you ever wanted to know about its quirks.</p><h2>A note on learning and LLMs</h2><p>Perhaps you’re a user of LLMs. I get it, they’re neat tools. They’re useful for certain kinds of learning. But I might\nsuggest resisting the temptation to use them for projects like this. Knowledge is not supposed to be fed to you on a\nplate. If you want that sort of learning, read a book - the joy in building toy projects like this comes from an\nexploration of the unknown, without polluting one’s mind with an existing solution. If you’ve been using LLMs for a\nwhile, this cold-turkey approach might even be painful at first, but persist. There is no joy without pain.</p><p>The runner’s high doesn’t come to those that take the bus.</p>","contentLength":12598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1lkfk1c/writing_toy_software_is_a_joy/"},{"title":"Coccinelle for Rust progress report","url":"https://www.collabora.com/news-and-blog/blog/2025/06/25/coccinelle-for-rust-progress-report/","date":1750879445,"author":"/u/mfilion","guid":625,"unread":true,"content":"<p><em>In collaboration with Inria (the French Institute for Research in Computer Science and Automation), Tathagata Roy shares the progress made over the past year on the CoccinelleForRust project, co-sponsored by Collabora.</em></p><p>Coccinelle is a tool for automatic program matching and transformation that was originally developed for making large-scale changes to the Linux kernel source code (i.e., C code). Matches and transformations are driven by user-specific transformation rules in the form of abstracted patches, referred to as . As the Linux kernel—and systems software more generally—is starting to adopt Rust, we are developing <a href=\"https://rust-for-linux.com/coccinelle-for-rust\" target=\"_blank\" rel=\"noopener\">Coccinelle for Rust</a> to make the power of Coccinelle available to Rust codebases.</p><p><a href=\"https://fuchsia.googlesource.com/third_party/rust/+/70e85d146fee03c09e28a02c9c18d56e74254f3c%5E2..70e85d146fee03c09e28a02c9c18d56e74254f3c/\" target=\"_blank\" rel=\"noopener\">This diff</a> illustrates a patch in which the  function was being called  confirming that the target item’s trait was implemented. A straightforward CfR-based fix is to find every expression of the form</p><p><code>\nexpression.type_of(impl_id)\n</code></p><p><code>\nexpression.type_of(impl_id).subst_identity()\n</code></p><p>There are roughly fifty occurrences of this pattern in the diff, so updating them all by hand would be quite tedious. The accompanying Semantic Patch to perform this transformation automatically is:</p><pre><code>@change_ty_of@\nexpression exp1, impl_id;\n@@\n\n-exp1.type_of(impl_id)\n+exp1.type_of(impl_id).subst_identity()\n</code></pre><p>While the above example could be achieved with a complicated and unreadable regex pattern, things can quickly become more complex.</p><p>The following patch changes a function signature and all the related calls.</p><pre><code>@change_sig@\nexpression x;\nidentifier fname, sname;\n@@\n\nimpl sname {\n    ...\n    pub(crate) fn fname(&amp;self,\n-       guard: &amp;RevocableGuard&lt;'_&gt;\n    ) -&gt; Result { ... }\n    ...\n}\n\n@modify_calls@\nexpression x, guard;\nidentifier change_sig.fname;\n@@\n\nx.fname(\n    ...\n-   guard\n)\n</code></pre><p>Rule  finds all the occurrences of functions which take a  of type  and removes that parameter. The rule  updates the calls to that method.</p><p>This semantic patch can be used on a whole code-base where once a  variable is no longer needed, it can be removed. It can also serve as an integration test to check that no such code is present in new pull requests.</p><h3>Developments (2024–Present)</h3><ol><li>Core pattern-matching engine using CTL.</li><li>Shared with C version, adapted for Rust’s expression-heavy syntax.</li><li>Performance optimized with s and hash tables.</li></ol><ol><li>SmPL includes Rust + custom syntax (, , , disjunctions).</li><li>Custom parser for SmPL constructs; uses Rust Analyzer for Rust code.</li></ol><ol><li>Define code transformations scoped to an environment.</li><li>Support for rule inheritance.</li><li>Rule dependencies not yet implemented.</li></ol><ol><li>Connects code segments within the same control flow.</li><li>Higher complexity due to Rust’s flexible syntax.</li></ol><p> not yet supported.</p><ol><li>Alternative code match options.</li><li>Can combine with ellipses for complex patterns.</li></ol><h3>Developments in detail: 2024–Present</h3><p>Computational Tree Logic (CTL) is the heart of Coccinelle, which takes semantic patches and generalizes them over Rust files. Prior to using this engine, CfR used an ad-hoc method for matching patterns of code. This engine is the same as the one used for Coccinelle for C, with a few minor changes. Most of the changes were idiomatic but to the same effect. More information on the engine and its language (CTL-VW) can be found in the <a href=\"https://coccinelle.gitlabpages.inria.fr/website/papers/popl09.pdf\" target=\"_blank\" rel=\"noopener\">POPL Paper</a>. With a standard engine, each step of the matching process can be logged, allowing us to learn and reuse the same design patterns from Coccinelle for C, including critical test cases.</p><p>The expression-dominated nature of Rust makes the matching and transformation process a bit different from that of C. For example, in the following semantic patch:</p><pre><code>@@\nexpression e\n@@\n\n-foo(e);\n</code></pre><p>for C,  would be guaranteed to be present as an immediate child of a block, i.e.:</p><pre><code>{           // &lt;- start of a block\n    foo(e); // &lt;- this statement\n}\n</code></pre><p>Blocks in C are present only in specific parts of the Abstract Syntax Tree, like in function definitions, loops, or conditional blocks. However, in Rust, blocks are expressions, which can appear anywhere an expression is allowed. For example:</p><pre><code>while { f(&amp;mut a); a &gt; 1 } {\n    //\n}\n</code></pre><p>This makes searching much more computationally intensive. Thus, several optimizations were implemented in CfR to address this problem, including replacing lists in the CTL engine with s and hash tables.</p><h4>Semantic Patch Language (SmPL)</h4><p>While developing the parser for SmPL, we decided not to reinvent the wheel by writing a parser for the Rust language from scratch. SmPL contains custom syntax such as dots (), disjunctions, and modifiers ( and ). In the latest version, we parse only these constructs ourselves and hand off the rest to Rust Analyzer.</p><p>A  refers to a set of changes given an environment. Multiple rules can inherit values from one another to transform code in different parts of a file.</p><p>Used in SmPL as , ellipses connect two blocks of code:</p><pre><code>@@\nexpression q;\n@@\n\ndrop_queue(q);\n...\npop(q);\n</code></pre><p>This is implemented in CTL using the  term.</p><p>Disjunctions allow for conditional matching:</p><pre><code>f1(10);\n(   // &lt;--- disjunction start\nfoo(1);\n|\nbar(10);\n)   // &lt;--- disjunction end\nf2();\n</code></pre><p>Transforming macros posed a problem because of their non-standard nature. For example, should the following semantic patch match</p><pre><code>@@\nexpression e;\n@@\n\nfoo!(\n- e\n+ 2\n);\n</code></pre><p>To avoid discrepancies, we support only macros which look like function calls. For example,  or .</p><ol><li>Pretty Printing has been improved. The transformed code is formatted using  and it is then compared with the formatting from the original code. This way only the transformed code is formatted without messing up the original file formatting.  Pretty printing is still a work-in-progress for rust macros as they are notoriously hard to deal with and  thus leaves them alone.</li><li>Better tests have been added.</li><li>A more robust UI has been implemented, with various debugging flags.</li></ol><p>Our current aim is to bring Coccinelle For Rust at par with Coccinelle For C in terms of basic functionalities. In the following months we are going on to work on:</p><ul><li>Rule Dependance - It lets a rule run only if a condition is satisfied by the rules before it.</li><li>Scripting - Lets the user run arbitrary code for each match, allowing them to perform more things that are out of scope for now. This includes counting instances, matching with regex and performing other arbitrary operations.</li></ul><p>If you want to try out CoccinelleForRust it is available on <a href=\"https://gitlab.inria.fr/coccinelle/coccinelleforrust\" target=\"_blank\" rel=\"noopener\">Gitlab</a>. Please feel free to reach out to us at the email addresses on our website <a href=\"https://rust-for-linux.com/coccinelle-for-rust\" target=\"_blank\" rel=\"noopener\">CoccinelleForRust</a>, we would be happy to answer your questions!</p>","contentLength":6439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1lkeyev/coccinelle_for_rust_progress_report/"},{"title":"OpenAI is Ditching TypeScript to Rebuild Codex CLI with Rust","url":"https://analyticsindiamag.com/global-tech/openai-is-ditching-typescript-to-rebuild-codex-cli-with-rust/","date":1750876865,"author":"/u/GeneReddit123","guid":640,"unread":true,"content":"<p>When OpenAI launched Codex CLI, the aim was to make it easier for developers to interact with AI in the terminal using a familiar stack—TypeScript and React-based Ink.&nbsp;</p><p>While it tries to compete with Claude Code and <a href=\"https://analyticsindiamag.com/ai-features/why-openais-codex-is-not-as-good-as-devin-or-replit/\">similar tools</a>, the team decided to revamp its foundation for better performance. “We’ve been working on a rewrite of Codex CLI into Rust,” <a href=\"https://www.linkedin.com/in/fouadmatin/\">Fouad Matin</a>, a member of technical staff at OpenAI, said in a <a href=\"https://github.com/openai/codex/discussions/1174\">GitHub discussion thread</a>.&nbsp;</p><p>Although the TypeScript version was productive for fast prototyping, it started to show its limits as the Codex CLI matured with various use cases.&nbsp;</p><p>Now, OpenAI plans to retire the TypeScript CLI entirely in favour of Rust. Matin mentioned that the TypeScript version will continue to receive bugfixes for now and that the focus is on bringing the native Rust build to feature parity and eventually making it the default.</p><h2>4 Benefits of Rust at Its Core</h2><p>While <a href=\"https://analyticsindiamag.com/ai-features/when-rust-rusts-the-developer-experience/\">Rust has its own set of problems</a> and benefits, the switch isn’t about language ideology. As Matin put it, “We want to use the best tool for the job.” Codex CLI may have launched with “a neat terminal UI” built on React.&nbsp;</p><p>However, he said that, at its core, the CLI functions as a tool working in a loop that keeps talking to the AI model and working with the system, instead of simply displaying a nice terminal interface. For that kind of repeated interaction with local system resources and APIs, TypeScript began to fall short.</p><p>“We wanted to improve a few areas,” Matin explained.&nbsp;</p><p>First, the installation experience required improvement, as the current version “requires Node v22+, which can be frustrating or a blocker for some users”. Second, improvements were to be made in native security bindings. “We already ship a Rust for Linux sandboxing since the bindings were available.” And third, the focus was on runtime performance. “No runtime garbage collection, resulting in lower memory consumption.”</p><p>Beyond performance, Rust offered architectural breathing room. Matin said OpenAI is developing a “wire protocol” for Codex CLI, which will enable developers to extend the agent using various languages, such as TypeScript/JavaScript and Python. Rust is already supported for MCPs.</p><p>In other words, Codex CLI isn’t just a tool; it aims to evolve into a cross-language, plug-in-friendly runtime for model-based automation.</p><h2>Work in Progress, but ‘Butter Smooth’</h2><p>While the new Rust version is still under development, the response so far has been optimistic. One developer <a href=\"https://github.com/rizwankce\">reported</a> that “codex native is butter smooth so far”, even though there are still some discrepancies between the TypeScript and native versions. This includes configuration file support and the ability to use the free tier mode or log in with an OpenAI account.</p><p>OpenAI is systematically addressing those gaps. In a separate <a href=\"https://github.com/openai/codex/issues/1262\">GitHub thread</a>, <a href=\"https://www.linkedin.com/in/michael-bolin-7632712/\">Michael Bolin</a>, a member of technical staff at OpenAI, categorised the remaining work as P0 (must-fix), P1 (feature parity), and P2 (quality-of-life).&nbsp;</p><p>High-priority features for the native Rust version include ‘Sign in with ChatGPT’ and improved interruption handling. Other features, like session management and prompt suggestions, are set to follow after the feature parity is handled.</p><p>“We will ultimately be retiring the TypeScript version of the CLI in favour of the Rust one,” Bolin wrote in the <a href=\"https://github.com/openai/codex/issues/1262\">GitHub thread</a>. The roadmap reflects a methodical upgrade path rather than a rushed rewrite.</p><h2>Native is the New Normal?</h2><p>The move fits a larger industry narrative. On Hacker News, a <a href=\"https://news.ycombinator.com/item?id=44150814\">user</a> called it part of a “recent resurgence of tools going native”.&nbsp;</p><p>The user explained that the notion of JIT (just-in-time) interpreters becoming better and eliminating the need for native languages is increasingly being challenged.</p><p>Meanwhile, another user <a href=\"https://news.ycombinator.com/item?id=44154144\">noted</a> that Rust and Go have made native development far more accessible. “The package management is better, and statically linked native binaries eliminate so many deployment headaches,” the user wrote.</p><p>With Rust, OpenAI isn’t just changing the codebase; it’s changing what kind of software Codex CLI can be. From terminal utility to programmable agent harness, the CLI is being rebuilt not just for speed, but also for flexibility, portability, and long-term maintainability.&nbsp;</p><p>If the TypeScript version was deployed for a playground, the Rust rewrite is planned to be ready for the real world.</p>","contentLength":4406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lkdu2m/openai_is_ditching_typescript_to_rebuild_codex/"},{"title":"Better Auth, by a self-taught Ethiopian dev, raises $5M from Peak XV, YC","url":"https://techcrunch.com/2025/06/25/this-self-taught-ethiopian-dev-built-an-authentication-tool-and-got-into-yc/","date":1750874822,"author":"bundie","guid":231,"unread":true,"content":"<p>It’s rare to see a solo founder building a widely adopted developer infrastructure tool. Even more so if the founder happens to be from Africa. <a href=\"https://et.linkedin.com/in/bekacru\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Bereket Engida</a>, a self-taught programmer from Ethiopia, is quietly building what some developers say is the best authentication tool they’ve ever used.</p><p>Engida’s startup, <a href=\"https://www.better-auth.com/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">Better Auth</a>, offers an open source framework that promises to simplify how developers manage user authentication, and it’s caught the attention of some big name investors.&nbsp;It recently raised about $5 million in seed funding from Peak XV (formerly Sequoia India and Southeast Asia), Y Combinator, P1 Ventures, and Chapter One.&nbsp;</p><p>But the most interesting part here isn’t who’s on the startup’s cap table: Engida says he built the entire product back home in Ethiopia before he set foot in the U.S.</p><p>Engida told TechCrunch that he started programming at 18 after a friend declined to help him build an e-commerce search app, and he started working on the project himself. He went on to land some remote software jobs and eventually built a web analytics platform that lets developers monitor user behavior on their websites. </p><p>But throughout his various jobs, Engida says he kept seeing an issue popping up everywhere: authentication. Every app needs to manage how users sign in and out and reset passwords, and sometimes administrators need to handle permissions and user roles. But he found existing tools were either too limited or too rigid — companies like Auth0, Firebase, and NextAuth offer managed services, but they store user data externally, limit customization, and are expensive at scale.</p><p>“I remember needing an organization feature. It’s a very common use case for most SaaS applications, but it wasn’t available from these providers,” Engida told TechCrunch. “So I had to build it from scratch. It took me about two weeks, and I remember thinking, ‘This is crazy; there has to be a better way to solve this.’”</p><p>He then scrapped that project and began working on a TypeScript-based authentication framework that would let developers access user data via open source libraries, support common permissions use cases — like teams and roles — out of the box, and scale with plug-ins.</p><p>“The idea was that you could add advanced features in just two or three lines of code,” Engida said.</p><p>Over six months working mostly from his bedroom in Ethiopia, Engida built the first version of the library that would go on to become Better Auth. When he posted it to GitHub in September 2024, developers quickly saw the potential.&nbsp;</p><p>Since then, Better Auth has clocked 150,000+ weekly downloads, 15,000+ GitHub stars, and a community of over 6,000 Discord members, the startup claims.&nbsp;</p><p>Better Auth’s pitch is simple: Let developers implement everything from simple authentication flows to enterprise-grade systems directly on their databases and embed it all on the back end. Unlike hosted services, Better Auth is an open source library that developers can integrate directly into their codebase, keeping all user data on premise, in their database. For companies wary of handing over critical user information to third parties, this feature alone is a major point.</p><p>The library has also found unexpected traction among early-stage AI startups, which need to build custom authentication flows that integrate with proprietary APIs, manage tokens securely, and be able to scale without racking up high costs.</p><p>“We first heard about the product from numerous startups we’ve worked with,” said Arnav Sahu, partner at Peak XV and former principal at Y Combinator. “Their auth product has seen phenomenal adoption among the next generation of AI startups.” </p><p>Better Auth marks Peak XV’s first direct investment in an African founder.</p><p>Engida says Better Auth, currently free to use, will focus on improving its core features and launch a paid enterprise infrastructure that plugs into its open source base. This will give developers the flexibility to self-host or opt for Better Auth’s cloud add-ons as needed.</p><p>He’s also thinking about how to scale without trading away the product’s community-built feel. On the roadmap, therefore, is hiring a small team to help maintain the codebase, expand documentation, and support enterprise users. For now, though, Engida is still writing most of the code himself.</p><p>Better Auth, which just graduated from YC’s recent spring batch, is the third Ethiopian startup to pass through the accelerator, following drone-based digital health platform Avion, and food delivery platform BeU Delivery.&nbsp;</p><p>“Building this feels important not just because people love the product, but because of what it represents,” said Engida. “There aren’t many Ethiopian founders building global products. For many, it feels almost impossible. So seeing that traction gives hope for other people to try to be more ambitious.”</p>","contentLength":4889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44380185"},{"title":"'The Computer-Science Bubble Is Bursting'","url":"https://developers.slashdot.org/story/25/06/25/1730250/the-computer-science-bubble-is-bursting?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750872600,"author":"msmash","guid":311,"unread":true,"content":"theodp writes: The job of the future might already be past its prime,\" writes The Atlantic's Rose Horowitch in The Computer-Science Bubble Is Bursting. \"For years, young people seeking a lucrative career were urged to go all in on computer science. From 2005 to 2023, the number of comp-sci majors in the United States quadrupled. All of which makes the latest batch of numbers so startling. This year, enrollment grew by only 0.2 percent nationally, and at many programs, it appears to already be in decline, according to interviews with professors and department chairs. At Stanford, widely considered one of the country's top programs, the number of comp-sci majors has stalled after years of blistering growth. Szymon Rusinkiewicz, the chair of Princeton's computer-science department, told me that, if current trends hold, the cohort of graduating comp-sci majors at Princeton is set to be 25 percent smaller in two years than it is today. The number of Duke students enrolled in introductory computer-science courses has dropped about 20 percent over the past year.\" \n\n\"But if the decline is surprising, the reason for it is fairly straightforward: Young people are responding to a grim job outlook for entry-level coders. In recent years, the tech industry has been roiled by layoffs and hiring freezes. The leading culprit for the slowdown is technology itself. Artificial intelligence has proved to be even more valuable as a writer of computer code than as a writer of words. This means it is ideally suited to replacing the very type of person who built it. A recent Pew study found that Americans think software engineers will be most affected by generative AI. Many young people aren't waiting to find out whether that's true.\" \n\nMeanwhile, writing in the Communications of the ACM, Orit Hazzan and Avi Salmon ask: Should Universities Raise or Lower Admission Requirements for CS Programs in the Age of GenAI? \"This debate raises a key dilemma: should universities raise admission standards for computer science programs to ensure that only highly skilled problem-solvers enter the field, lower them to fill the gaps left by those who now see computer science as obsolete due to GenAI, or restructure them to attract excellent candidates with diverse skill sets who may not have considered computer science prior to the rise of GenAI, but who now, with the intensive GenAI and vibe coding tools supporting programming tasks, may consider entering the field?","contentLength":2470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build and Host AI-Powered Apps with Claude – No Deployment Needed","url":"https://www.anthropic.com/news/claude-powered-artifacts","date":1750871675,"author":"davidbarker","guid":230,"unread":true,"content":"<p>Today, we’re introducing the ability to build, host, and share interactive AI-powered apps directly in the Claude app. Now developers can iterate faster on their AI apps without worrying about the complexity and cost of scaling for a growing audience.</p><h2>Build and host Claude-powered apps</h2><p>Here’s what we built: Claude can now create artifacts that interact with Claude through an API— turning these artifacts into AI-powered apps, where the economics actually work for sharing.</p><p>When someone uses your Claude-powered app:</p><ul><li>They authenticate with their existing Claude account</li><li>Their API usage counts against  subscription, not yours</li><li>You pay nothing for their usage</li><li>No one needs to manage API keys</li></ul><p>Claude writes real code that orchestrates complex AI functionality. You can see it, modify it, and share it freely.</p><p>Early users have already used interactive artifacts to build:</p><ul><li> with NPCs that remember conversations and adapt to player choices</li><li> that adjust to individual skill levels and provide personalized tutoring</li><li> where users upload CSVs and ask follow-up questions in natural language</li><li> that help with everything from scripts to technical documentation</li><li> that orchestrate multiple Claude calls for complex tasks</li></ul><p>Start building in the Claude app by enabling this new interactive capability. Simply describe what you want to create, and Claude will write the code for you.</p><p>As you work together, Claude can debug and improve its own code based on your feedback. Once your app is ready, you can share it instantly through a link—no deployment process needed. Claude takes care of the technical details like prompt engineering, error handling, and orchestration logic, allowing you to focus entirely on bringing your idea to life.</p><ul><li>Use a Claude API within your artifacts</li><li>Process files and create rich UIs with React</li><li>See, fork, and customize any artifact</li></ul><ul><li>No external API calls (yet)</li><li>Limited to a text-based completion API</li></ul><p>This capability is available in beta to Free, Pro, and Max plan users.</p>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44379673"},{"title":"America’s incarceration rate is in decline","url":"https://www.theatlantic.com/ideas/archive/2025/06/prisoner-populations-are-plummeting/683310/","date":1750871669,"author":"paulpauper","guid":229,"unread":true,"content":"<p data-flatplan-paragraph=\"true\"><em><small>Updated at 12:35 p.m. ET on June 26, 2025</small></em></p><p data-flatplan-paragraph=\"true\">For more than 40 years, the United States—a nation that putatively cherishes freedom—has had one of the largest prison systems in the world. Mass incarceration has been so persistent and pervasive that reform groups dedicated to <a data-event-element=\"inline link\" href=\"https://www.themarshallproject.org/2015/03/04/how-to-cut-the-prison-population-by-50-percent\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">reducing the prison population by half</a> have often been derided as made up of fantasists. But the next decade could see this goal met and exceeded: After peaking at just more than 1.6 million Americans in 2009, the prison population was <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/library/publications/prisons-report-series-preliminary-data-release-2023\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">just more than 1.2 million at the end of 2023</a> (the most recent year for which data are available), and is on track to fall to about 600,000—a total decline of roughly 60 percent.</p><p data-flatplan-paragraph=\"true\">Discerning the coming prison-population cliff requires understanding the relationship between crime and incarceration over generations. A city jail presents a snapshot of what happened last night (for example, the crowd’s football-victory celebration turned ugly). But a prison is a portrait of what happened five, 10, and 20 years ago. Middle-aged people who have been law-abiding their whole life until “something snapped” and they committed a terrible crime are a staple of crime novels and movies, but in real life, virtually everyone who ends up in prison starts their criminal career in their teens or young adulthood. As of 2016—the most recent year for which data are available—the average man in state prison had been arrested <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/content/pub/pdf/ppi16.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">nine times, was currently incarcerated for his sixth time, and was serving a 16-year sentence</a>.</p><p data-flatplan-paragraph=\"true\">Because of that fundamental dynamic, the explanation for why roughly 1.6 million people—more than 500 for every 100,000 Americans—were in a state or federal prison in 2009 has very little to do with what was happening on the streets or with law-enforcement policies that year. Rather, the causes lay in the final decades of the 20th century.</p><p data-flatplan-paragraph=\"true\">From the end of World War II until the mid-1970s, the proportion of Americans in prison each year <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/content/pub/pdf/sfp2585.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">never exceeded 120 per 100,000</a>. But starting in the late 1960s, a multidecade crime wave swelled in America, and an unprecedented number of adolescents and young adults were criminally active. In response, the anti-crime policies of most local, state, and federal governments became more and more draconian. The combined result was that the prison population exploded. By 1985, the imprisonment rate had doubled from its historical norm, such that <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/content/pub/pdf/sfp2585.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">more than 200 in 100,000 Americans were in a state or federal prison</a>. The number of people in prison increased <a data-event-element=\"inline link\" href=\"https://www.sentencingproject.org/reports/mass-incarceration-trends/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">an average of 8 percent a year for the next decade</a>, breaching the <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/content/pub/pdf/PJI95.PDF\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">1 million mark in 1994</a> and continuing to grow until 2009. This had ramifications that were felt for years: Because <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/sites/g/files/xyckuh236/files/media/document/rpr24s0810yfup0818.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">most people who are released from prison return</a>, the system has been stocked and restocked with the legacy of that American crime-and-punishment wave for a quarter century. That’s why the 2009 peak of U.S. imprisonment came 18 years after the 1991 peak in the violent-crime rate. The prison system is like a badly overloaded tractor trailer—it takes a long time to stop even after the brakes are hit.</p><p data-flatplan-paragraph=\"true\">That tractor trailer is finally slowing down, decades after the <a data-event-element=\"inline link\" href=\"https://www.newyorker.com/magazine/2018/02/12/the-great-crime-decline\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">“great crime decline”</a> began in the 1990s. Until 2009, the lengthier sentences handed down during the preceding crime wave and the tendency of released prisoners to be re-incarcerated kept imprisonment rising even as crime declined. But the falling crime that the U.S. experienced in the 1990s and 2000s is now finally translating into a shrinking prison population.</p><p data-flatplan-paragraph=\"true\">This chart, using data from the U.S. Department of Justice, shows the collapse of criminal arrests of minors in the 21st century. Rapidly declining numbers of youth are committing crimes, getting arrested, and being incarcerated. This matters because young offenders are the raw material that feeds the prison system: As one generation ages out, another takes its place on the same horrid journey. The U.S. had an extremely high-crime generation followed by a lower-crime generation, meaning that the older population is not being replaced at an equal rate. The impact of this shift on the prison population began more than a decade ago but has been little noticed because it takes so long for the huge prison population of longer provenance to clear.</p><p data-flatplan-paragraph=\"true\">But such a transformation is now well under way. One statistic vividly illustrates the change: In 2007, the imprisonment rate for 18- and 19-year-old men was more than five times that of men over the age of 64. But today, men in those normally crime-prone late-adolescent years are imprisoned at half the rate that senior citizens are today.</p><p data-flatplan-paragraph=\"true\">As the snake digests the pig year after year, the American prison system is simply not going to have enough inmates to justify its continued size or staggering costs. Some states that are contemplating expanding their prison capacity will be wasting their money—their facilities will be overbuilt and underused. By 2035, the overall imprisonment rate could be as low as 200 per 100,000 people. States should instead be tearing down their most deteriorated and inhumane correctional facilities, confident that they will not need the space.</p><p data-flatplan-paragraph=\"true\">This optimistic analysis could have been written in 2019, when the imprisonment rate had been falling for more than a decade and hit a <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/content/pub/pdf/p19.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">level not seen since 1995</a>. I thought about writing this article then, but a world turned upside down shook my confidence.</p><p data-flatplan-paragraph=\"true\">COVID initially looked like a boon for decarceration because states reduced prison admissions and accelerated releases in 2020 to reduce transmission, cutting the prison population by 16 percent. But whether it was due to this mass release, COVID, de-policing, other factors, or some combination thereof, crime exploded in 2020 after a long quiescent period, most shockingly with an <a data-event-element=\"inline link\" href=\"https://www.pewresearch.org/short-reads/2021/10/27/what-we-know-about-the-increase-in-u-s-murders-in-2020/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">unprecedented 30 percent increase in homicides</a>. Crime spikes increase <a data-event-element=\"inline link\" href=\"https://www.slowboring.com/p/to-reduce-mass-incarceration-reduce\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">incarceration directly because more people are committing crimes</a> and also because they <a data-event-element=\"inline link\" href=\"https://www.pewresearch.org/short-reads/2024/04/24/what-the-data-says-about-crime-in-the-us/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">lead the public to demand more aggressive policies</a>, which often translate into longer and more frequent prison sentences. If the turmoil of the early 2020s had led to an extended period of high crime and high punishment similar to what the U.S. experienced in the late 20th century, the COVID-era contraction of the prison population could have been immediately nullified and then some when, in the ensuing years, the prison pipeline was eventually replenished.</p><p data-flatplan-paragraph=\"true\">But thankfully, the spike was just a spike, not a new equilibrium. Crime stopped rising sometime in 2022, and fell <a data-event-element=\"inline link\" href=\"https://bjs.ojp.gov/library/publications/federal-justice-statistics-2023\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">in 2023</a> and <a data-event-element=\"inline link\" href=\"https://counciloncj.org/crime-trends-in-u-s-cities-year-end-2024-update/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">2024</a>. The prison population inched up 2 percent in 2022 and again in 2023, and it is possible that a similar rise took place in 2024, but even collectively, this is a fraction of the sudden population decline during the early pandemic. The COVID era ended with prison populations lower rather than higher: <a data-event-element=\"inline link\" href=\"https://vera-institute.files.svdcdn.com/production/downloads/publications/People-in-Jail-and-Prison-in-2024-Report.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">A recent Vera Institute report</a> found that, on balance, from 2019 to the spring of 2024, the number of federal prisoners declined by 11 percent, and the number of state prisoners declined by 13 percent.</p><p data-flatplan-paragraph=\"true\">Accelerating the de-prisoning of America is worthwhile and possible. The benefits of a smaller prison population are not limited to those who would otherwise be locked up and the people who love them. Prisons crowd out other policy priorities that many voters would like the government to spend more money on. In all 50 states, the cost to imprison someone for a year <a data-event-element=\"inline link\" href=\"https://money.cnn.com/infographic/economy/education-vs-prison-costs/\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">significantly exceeds</a> the cost of a year of K–12 education. But even greater than the financial savings would be the prosperity in human terms: Less crime and less incarceration are profound blessings for a society.</p><p data-flatplan-paragraph=\"true\">The simplest available policy to accelerate the decarceration trend is to stop building prisons except in cases where a smaller, modern facility is replacing a larger, decaying institution. Though it will be nonintuitive to many reformers, particularly on the left, opposition to any such new facilities being private should be dropped. The principal political barrier to closing half-full prisons is the power of public-sector unions. In contrast, a private prison can be sent to its reward if its contract is canceled. Individual communities in areas of low employment will also fight to keep their prisons. Prison-closing commissions, analogous to <a data-event-element=\"inline link\" href=\"https://comptroller.defense.gov/Portals/45/Documents/defbudget/fy2023/budget_justification/pdfs/05_BRAC/FY2023_BRAC_Overview.pdf\" rel=\"noopener noreferrer nofollow\" target=\"_blank\">military-base-closing commissions</a>, may be necessary and should coordinate with legislators to provide worker retraining and financial assistance to compensate for the loss of high-wage jobs in communities whose economy revolves around corrections.</p><p data-flatplan-paragraph=\"true\">Finally, America should not let its prison system become the most expensive and inhumane of nursing homes. The rate of recidivism among senior citizens is near zero, and compassionate release of sick and aging inmates should be the default rather than the exception, a reversal of current practice.</p><p data-flatplan-paragraph=\"true\">In any given future year, small rises in imprisonment are possible, but the macro trend is ineluctable: Society is going to experience the benefits of past decades of lower crime throughout its prison system. The imprisonment rate will be lower in five years and lower still in 10. Prisons will still exist then and still be needed, but the rate at which Americans are confined in them could be lower than anything in the preceding half century. This is the fruit of a lower-crime society—good in and of itself, surely, particularly for the low-income and majority-minority communities where most crime occurs. It will also, of course, be a blessing for those who avoid prison, and for the taxpayers who no longer have to pay for it. The decline in the prison population will be something everyone in our polarized society will have reason to celebrate.</p><p data-flatplan-paragraph=\"true\"><em><small>A chart in this article showing the number of juvenile arrests from 1980 to 2020 has been updated to correct labeling on the Y axis.</small></em></p>","contentLength":9841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44379670"},{"title":"What Problems to Solve (1966)","url":"http://genius.cat-v.org/richard-feynman/writtings/letters/problems","date":1750871324,"author":"jxmorris12","guid":228,"unread":true,"content":"<p><em>A former student, who was also once a student of Tomonaga’s, wrote to extend\nhis congratulations. Feynman responded, asking Mr. Mano what he was now doing.\nThe response: “studying the Coherence theory with some applications to the\npropagation of electromagnetic waves through turbulent atmosphere… a humble and\ndown-to-earth type of problem.”</em></p><pre><code>Dear Koichi,\n\nI was very happy to hear from you, and that you have such a position in the\nResearch Laboratories. Unfortunately your letter made me unhappy for you seem\nto be truly sad. It seems that the influence of your teacher has been to give\nyou a false idea of what are worthwhile problems. The worthwhile problems are\nthe ones you can really solve or help solve, the ones you can really contribute\nsomething to. A problem is grand in science if it lies before us unsolved and\nwe see some way for us to make some headway into it. I would advise you to take\neven simpler, or as you say, humbler, problems until you find some you can\nreally solve easily, no matter how trivial. You will get the pleasure of\nsuccess, and of helping your fellow man, even if it is only to answer a\nquestion in the mind of a colleague less able than you. You must not take away\nfrom yourself these pleasures because you have some erroneous idea of what is\nworthwhile.\n\nYou met me at the peak of my career when I seemed to you to be concerned with\nproblems close to the gods. But at the same time I had another Ph.D. Student\n(Albert Hibbs) was on how it is that the winds build up waves blowing over\nwater in the sea. I accepted him as a student because he came to me with the\nproblem he wanted to solve. With you I made a mistake, I gave you the problem\ninstead of letting you find your own; and left you with a wrong idea of what is\ninteresting or pleasant or important to work on (namely those problems you see\nyou may do something about). I am sorry, excuse me. I hope by this letter to\ncorrect it a little.\n\nI have worked on innumerable problems that you would call humble, but which I\nenjoyed and felt very good about because I sometimes could partially succeed.\nFor example, experiments on the coefficient of friction on highly polished\nsurfaces, to try to learn something about how friction worked (failure). Or,\nhow elastic properties of crystals depends on the forces between the atoms in\nthem, or how to make electroplated metal stick to plastic objects (like radio\nknobs). Or, how neutrons diffuse out of Uranium. Or, the reflection of\nelectromagnetic waves from films coating glass. The development of shock waves\nin explosions. The design of a neutron counter. Why some elements capture\nelectrons from the L-orbits, but not the K-orbits. General theory of how to\nfold paper to make a certain type of child’s toy (called flexagons). The energy\nlevels in the light nuclei. The theory of turbulence (I have spent several\nyears on it without success). Plus all the “grander” problems of quantum\ntheory.\n\nNo problem is too small or too trivial if we can really do something about it.\n\nYou say you are a nameless man. You are not to your wife and to your child. You\nwill not long remain so to your immediate colleagues if you can answer their\nsimple questions when they come into your office. You are not nameless to me.\nDo not remain nameless to yourself – it is too sad a way to be. now your place\nin the world and evaluate yourself fairly, not in terms of your naïve ideals of\nyour own youth, nor in terms of what you erroneously imagine your teacher’s\nideals are.\n\nBest of luck and happiness.  Sincerely, Richard P. Feynman.\n</code></pre>","contentLength":3580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44379606"},{"title":"Deaf and Hard of Hearing WG Meeting - 2025-06-24","url":"https://www.youtube.com/watch?v=l5D5oWhczYU","date":1750860839,"author":"CNCF [Cloud Native Computing Foundation]","guid":388,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":381,"flags":null,"enclosureUrl":"https://www.youtube.com/v/l5D5oWhczYU?version=3","enclosureMime":"","commentsUrl":null},{"title":"OpenAI charges by the minute, so speed up your audio","url":"https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/","date":1750857445,"author":"georgemandis","guid":227,"unread":true,"content":"<p>Want to make OpenAI transcriptions faster and cheaper? Just speed up your audio.</p><p>I mean that very literally. Run your audio through <a href=\"https://gist.github.com/georgemandis/4fd62bf5027b7a058f913d5dc32c2040\">ffmpeg</a> at 2x or 3x before transcribing it. You’ll spend fewer tokens and less time waiting with almost no drop in transcription quality.</p><p>Here’s a script combining of all my favorite little toys and tricks to get the job. You’ll need <a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a>, <a href=\"https://ffmpeg.org\">ffmpeg</a> and <a href=\"https://github.com/simonw/llm\">llm</a> installed.</p><pre><code># Extract the audio from the video\nyt-dlp -f 'bestaudio[ext=m4a]' --extract-audio --audio-format m4a -o 'video-audio.m4a' \"https://www.youtube.com/watch?v=LCEmiRjPEtQ\" -k;\n\n# Create a low-bitrate MP3 version at 3x speed\nffmpeg -i \"video-audio.m4a\" -filter:a \"atempo=3.0\" -ac 1 -b:a 64k video-audio-3x.mp3;\n\n# Send it along to OpenAI for a transcription\ncurl --request POST \\\n  --url https://api.openai.com/v1/audio/transcriptions \\\n  --header \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  --header 'Content-Type: multipart/form-data' \\\n  --form <a href=\"https://george.mand.is/cdn-cgi/l/email-protection\" data-cfemail=\"284e41444d15685e414c4d4705495d4c4147051b500645581b\">[email&nbsp;protected]</a> \\\n  --form model=gpt-4o-transcribe &gt; video-transcript.txt;\n\n# Get a nice little summary\n\ncat video-transcript.txt | llm --system \"Summarize the main points of this talk.\"\n</code></pre><p>I just saved you time by jumping straight to the point, but read-on if you want more of a story about how I accidentally discovered this while trying to summarize a 40-minute talk from Andrej Karpathy.</p><p>Also read-on if you’re wondering why I didn’t just use the built-in auto-transcription that YouTube provides, though the short answer there is easy: I’m sort of a doofus and thought—incorrectly—it wasn’t available. So I did things the hard way.</p><h3>I Just Wanted the TL;DW(atch)</h3><p>A former colleague of mine sent me <a href=\"https://www.youtube.com/watch?v=LCEmiRjPEtQ\">this talk</a> from Andrej Karpathy about how AI is changing software. I wasn’t familiar with Andrej, but saw he’d worked at Tesla. That coupled with the talk being part of a Y Combinator series and 40 minutes made me think “Ugh. Do I… really want to watch this? Another 'AI is changing everything' talk from the usual suspects, to the usual crowds?”</p><p>If ever there were a use-case for dumping something into an LLM to get the gist of it and walk away, this felt like it. I respected the person who sent it to me though and wanted to do the noble thing: use AI to summarize the thing for me, blindly trust it and engage with the person pretending I had watched it.</p><p>My first instinct was to pipe the transcript into an LLM and get the gist of it. <a href=\"https://gist.github.com/simonw/9932c6f10e241cfa6b19a4e08b283ca9\">This script</a> is the one I would previously reach for to pull the auto-generated transcripts from YouTube:</p><pre><code>yt-dlp --all-subs --skip-download \\\n  --sub-format ttml/vtt/best \\\n  [url]\n</code></pre><p>For some reason though, no subtitles were downloaded. I kept running into an error!</p><p>Later, after some head-scratching and rereading <a href=\"https://github.com/yt-dlp/yt-dlp?tab=readme-ov-file#subtitle-options\">the documentation</a>, I realized my version (2025.04.03) was outdated.</p><p>: Updating to the latest version (2025.06.09) fixed it, but for some reason I did not try this  going down a totally different rabbit hole. I guess I got this little write-up and exploration out of it though.</p><p>If you care more about summarizing transcripts and less about the vagaries of audio-transcriptions and tokens, this is the correct answer and your off-ramp.</p><h3>My Transcription Workflow</h3><p>I already had an old, home-brewed script that would extract the audio from any video URL, pipe it through <a href=\"https://github.com/openai/whisper\">whisper</a> locally and dump the transcription in a text file.</p><p>That worked, but I was on dwindling battery power in a coffee shop. Not ideal for longer, local inference, mighty as my M3 MacBook Air still feels to me. I figured I would try offloading it to <a href=\"https://platform.openai.com/docs/guides/speech-to-text\">OpenAI’s API</a> instead. Surely that would be faster?</p><h3>Testing OpenAI’s Transcription Tools</h3><p>Okay, using the  model it’s  pretty slow, but it gets the job done. Had I opted for the model I knew and moved on, the story might end here.</p><p>However, out of curiosity, I went straight for the newer  model first. It’s built to handle multimodal inputs and promises faster responses.</p><p>I quickly hit another roadblock: there’s a 25-minute audio limit and my audio was nearly 40 minutes long.</p><h3>Let's Try Something Obvious</h3><p>At first I thought about trimming the audio to fit somehow, but there wasn’t an obvious 14 minutes to cut. Trimming the beginning and end would give me a minute or so at most.</p><p>An interesting, weird idea I thought about for a second but never tried was cutting a chunk or two out of the middle. Maybe I would somehow still have enough info for a relevant summary?</p><p>Then it crossed my mind—<strong>what if I just sped up the audio before sending it over?</strong> People listen to podcasts at accelerated 1-2x speeds all the time.</p><pre><code>ffmpeg -i video-audio.m4a -filter:a \"atempo=2.0\" -ac 1 -b:a 64k video-audio-2x.mp3\n</code></pre><p>Ta-da! Now I had something closer to a 20 minute file to send to OpenAI.</p><p>I uploaded it and… it worked like a charm! <a href=\"https://gist.github.com/georgemandis/b2a68b345262b94782fa6b08e41fbcf2\">Behold the summary</a> bestowed upon me that gave me enough confidence to reply to my colleague as though I had watched it.</p><p>But there was something... interesting here. Did I just stumble across a sort of obvious, straightforward hack? Is everyone in the audio-transcription business already doing this and am I just haphazardly bumbling into their secrets?</p><h3>Why This Works: Our Brains Forgive, and So Does AI</h3><p>There’s an interesting parallel here in my mind with optimizing images. Traditionally you have lossy and lossless file formats. A lossy file-format kind of gives away the game in its description—the further you crunch and compact the bytes the more fidelity you’re going to lose. It works because the human brain just isn’t likely to pick-up on the artifacts and imperfection</p><p>But even with a “lossless” file format there are tricks you can lean into that rely on the limits of human perception. One of the primary ways you can do that with a PNG or GIF is reducing the number of unique colors in the palette. You’d be surprised by how often a palette of 64 colors or fewer might actually be enough and perceived as significantly more.</p><p>There’s also a parallel in my head between this and the brain’s ability to still comprehend text with spelling mistakes, dropped words and other errors, i.e. <a href=\"https://en.wikipedia.org/wiki/Transposed_letter_effect\">transposed letter effects</a>. Our brains have a knack for filling in the gaps, and when you go looking through the world with magnifying glass you'll start to notice lots of them.</p><p>Speeding up the audio starts to drop the more subtle sounds and occasionally shorter words from the audio, but it doesn’t seem to hurt my ability to  what I’m hearing—even if I do have to focus. These audio transcription models seem to be pretty good at this as well.</p><h3>Wait—how far can I push this? Does It Actually Save Money?</h3><p>Turns out yes. OpenAI <a href=\"https://platform.openai.com/docs/pricing\">charges for transcription</a> based on audio tokens, which scale with the duration of the input. Faster audio = fewer seconds = fewer tokens.</p><p>Here are some rounded numbers based on the 40-minute audio file breaking down the audio input and text output token costs:</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>That’s a solid 33% price reduction on input tokens at 3x! However the bulk of your costs for these transcription models are still going to be the output tokens. Those are priced at $10 per 1M tokens whereas audio input tokens are priced at $6 per 1M token as of the time of this writing.</p><p>Also interesting to note—my output tokens for the 2x and 3x versions were exactly the same: 2,048. This kind of makes sense, I think? To the extent the output tokens are a reflection of that model's ability to understand and summarize the input, my takeaway is a “summarized” (i.e. reduced-token) version of the same audio yields the same amount of comprehensibility.</p><p>This is also probably a reflection of the 4,096 token ceiling on transcriptions generally when using the  model. I suspect half the context window is reserved for the output tokens and this is basically reflecting our request using it up in its entirety. I suspect we might get diminishing results with longer transcriptions.</p><p>So the back-of-the-envelope calculator for a single transcription looks something like this:</p><pre><code>6 * (audio_input_tokens / 1_000_000) + 10 * (text_output_tokens / 1_000_000);\n</code></pre><p>That does  quite seem to jibe with the estimated cost of $0.006 per minute stated on the pricing page, at least for the 2x speed. That version (19-20 minutes) seemed to cost about $0.09 whereas the 3x version (13 minutes) cost about $0.07 (pretty accurate actually), if I’m adding up the tokens correctly.</p><pre><code># Pricing for 2x speed\n6 * (11_856 / 1_000_000) + 10 * (2_048 / 1_000_000) = 0.09\n\n# Pricing for 3x speed\n6 * (7_904 / 1_000_000) + 10 * (2_048 / 1_000_000) = 0.07\n</code></pre><p>It would seem that estimate isn’t just based on the length of the audio but also some assumptions around how many tokens per minute are going to be generated from a normal speaking cadence.</p><p>Comparing these costs to  is easy because the pricing table more confidently advertises the cost—not “estimated” cost—as a flat $0.006 per minute. I’m assuming that’s minute of audio processed, not minute of inference.</p><p>The  model actually compares pretty favorably.</p><table><tbody></tbody></table><p>In short, yes! It’s not particularly rigorous, but it seems like we reduced the cost of transcribing our 40-minute audio file by 23% from $0.09 to $0.07 simply by speeding up the audio.</p><p>If we could compare to a 1x version of the audio file trimmed to the 25-minute limit, I bet we could paint an even more impressive picture of cost reduction. We kind of can with the  chart. You could make the case this technique reduced costs by 67%!</p><p>I don’t know—I didn’t watch it, lol. That was the whole point. And if that answer makes you uncomfortable, buckle-up for this future we're hurtling toward. Boy, howdy.</p><p>More helpfully, I didn’t compare word-for-word, but spot checks on the 2x and 3x versions looked solid. 4x speed was too fast—the transcription started getting hilariously weird. So, 2x and 3x seem to be the sweet spot between efficiency and fidelity, though it will obviously depend on how fast the people are speaking in the first place.</p><p>That sure didn't stop my call to summarize from <a href=\"https://gist.github.com/georgemandis/1ec4ef084789f92ee06ac6283338a194#file-summarization-md\">trying</a> though.</p><p>Hey, not the worst talk I've been to!</p><p>Always, in short, to save time and money, consider doubling or tripling the speed of the audio you want to transcribe. The trade-off is, as always, fidelity, but it’s not an insignificant savings.</p><p>Simple, fast, and surprisingly effective.</p><ul><li>OpenAI charges for transcriptions based on audio duration () or tokens ().</li><li>You can  with  before uploading to save time and money.</li><li>This reduces audio tokens (or duration), lowering your bill.</li><li> works well.</li><li>? Probably too much—but fun to try.</li></ul><p>If you find problems with my math, have questions, found a more rigorous study qualitatively comparing different output speeds please <a href=\"https://george.mand.is/contact\">get in touch</a>! Or if you thought this was so cool you want to <a href=\"https://george.mand.is/hire\">hire me</a> for something fun...</p>--","contentLength":10697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44376989"},{"title":"Gemini CLI","url":"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/","date":1750857046,"author":"sync","guid":267,"unread":true,"content":"<div>\n      [{\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 9, \"fields\": {\"name\": \"AA - Google AI product use - I/O\", \"survey_id\": \"aa-google-ai-product-use-io_250519\", \"scroll_depth_trigger\": 50, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank You!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"e83606c3-7746-41ea-b405-439129885ead\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How often do you use Google AI tools like Gemini and NotebookLM?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"32ecfe11-9171-405a-a9d3-785cca201a75\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Daily\\\"}, {\\\"id\\\": \\\"29b253e9-e318-4677-a2b3-03364e48a6e7\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Weekly\\\"}, {\\\"id\\\": \\\"5c5bb2ba-19b7-41dd-9000-2e3741878d19\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Monthly\\\"}, {\\\"id\\\": \\\"697372e1-80b1-4901-81eb-48bf090a6a05\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Hardly Ever\\\"}, {\\\"id\\\": \\\"b8e1604d-1146-4f2c-9184-6ed0f06fd863\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Unsure\\\"}]}}]\", \"target_article_pages\": true}}, {\"model\": \"blogsurvey.survey\", \"pk\": 7, \"fields\": {\"name\": \"Article Improvements - March 2025\", \"survey_id\": \"article-improvements-march-2025_250321\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 75, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"5a12fd89-d978-4a1b-80e5-2442a91422be\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"How could we improve this article?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"30122b0d-1169-4376-af7c-20c9de52c91c\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Make it more concise\\\"}, {\\\"id\\\": \\\"18f3016a-7235-468b-b246-ffe974911ae9\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Add more detail\\\"}, {\\\"id\\\": \\\"5d19c11d-6a61-49d3-9f1d-dad5d661ba4f\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Make it easier to understand\\\"}, {\\\"id\\\": \\\"97064d1f-d9af-4a83-a44f-a84f8ed899d6\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Include more images or videos\\\"}, {\\\"id\\\": \\\"a9ec2a70-c7c5-4f00-a179-31a7b5641879\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"It's fine the way it is\\\"}]}}]\", \"target_article_pages\": true}}]\n    </div>","contentLength":6055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44376919"},{"title":"Growing Expert Generalists","url":"https://martinfowler.com/articles/expert-generalist.html#GrowingExpertGeneralists","date":1750855680,"author":"Martin Fowler","guid":203,"unread":true,"content":"<p>To <a href=\"https://martinfowler.com/articles/expert-generalist.html#GrowingExpertGeneralists\">grow Expert Generalists</a> we need to focus attention on fundamentals\n      rather tools. As an example, Unmesh, Gitanjali, and I describe a workshop we've used to\n      break silos of application development, data engineering, and devops</p>","contentLength":238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Intro to ty – The Extremely Fast Python type checker","url":"https://www.blog.pythonlibrary.org/2025/06/25/an-intro-to-ty-the-extremely-fast-python-type-checker/","date":1750855546,"author":"Mike","guid":314,"unread":true,"content":"<p><a href=\"https://github.com/astral-sh/ty\">Ty is a brand new, extremely fast Python type checker</a> written in Rust from the fine folks at Astral, the makers of Ruff. Ty is in preview and is not ready for production use, but you can still try it out on your code base to see how it compares to Mypy or other popular Python type checkers.</p><p>If you prefer to install ty, you can use pip:</p><h2>Using the ty Type Checker</h2><p>Want to give ty a try? You can run it in much the same way as you would Ruff. Open up your terminal and navigate to your project’s top-level directory. Then run the following command:</p><p>If ty finds anything, you will quickly see the output in your terminal.</p><p>Astral has also provided a way to exclude files from type checking. By default, ty ignores files listed in an&nbsp;&nbsp;or&nbsp;&nbsp;file.</p><p>Ruff is a great tool and has been adopted by many teams since its release. Ty will likely follow a similar trajectory if it as fast and useful as Ruff has been. Only time will tell. However, these new developments in Python tooling are exciting and will be fun to try. If you have used ty, feel free to jump into the comments and let me know what you think.</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Scream to Unlock – Blocks social media until you scream “I'm a loser”","url":"https://news.ycombinator.com/item?id=44375761","date":1750848596,"author":"madinmo","guid":222,"unread":true,"content":"Hi all,<p>I kept wasting time on social media, even though I’d promised myself I’d stay focused. Regular site blockers didn’t help.</p><p>I needed something that felt annoying enough to break the habit. That’s how the idea came up: make the blocker ask me to say something embarrassing out loud before it lets me back in. If I actually have to yell “I’m a loser” into my mic. Even better - the louder I screamed, the more time I’d get.</p><p>So I put together Scream to Unlock. It’s silly, but so far it’s done its job. My social feeds stay locked unless I really want them.</p>","contentLength":577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44375761"},{"title":"Cloud Native Live: One Environment to Rule Them All - How mirrord Eliminates Dev Environment Chaos","url":"https://www.youtube.com/watch?v=pyY4xfh4ObQ","date":1750827961,"author":"CNCF [Cloud Native Computing Foundation]","guid":387,"unread":true,"content":"<article>Do you really need to spin up dozens of near-prod environments to support your developers?\n\nSpoiler: you don't.\n\nMost teams spin up isolated development environments for each developer which are expensive, hard to maintain, and often not even faithful to production. In this webinar, we'll show you how mirrord lets every developer safely treat your shared staging environment as their own, directly from their local machine. No more sky-high infra bills. No more surprise bugs in prod.\n\nJoin us to learn how you can\n\n- Let devs test against staging safely, without affecting each other's work\n- Run code locally but have it behave like it was running in your staging environment\n- Reduce infra complexity and cost by consolidating environments</article>","contentLength":744,"flags":null,"enclosureUrl":"https://www.youtube.com/v/pyY4xfh4ObQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"A new PNG spec","url":"https://www.programmax.net/articles/png-is-back/","date":1750823683,"author":"tbillington","guid":266,"unread":true,"content":"<ul><li><h3>Proper HDR support (future‐proof, too!)</h3><p>\n\t\t\t\t\t\t\t\tFigure 1 shows the colors our eyes can see.\n\t\t\t\t\t\t\t\tThe smaller, inner triangle represents the color space of most images.<p>\n\t\t\t\t\t\t\t\tThe larger, outer triangle represents the colors that are typical with a High Dynamic Range (HDR) image.\n\t\t\t\t\t\t\t</p></p><p>This new HDR support uses only 4 bytes (plus the usual PNG chunk overhead).</p></li><li><h3>Finally recognizes APNGs (animations!)</h3><p>Animated PNGs were proposed by Mozilla quite some time ago. Support was added to Firefox, but other programs hesitated to adopt them.</p><p>Today, animated PNGs are widely supported. It is time for the spec to reflect reality.</p></li><li><h3>Officially supports Exif data</h3><p>Exif stores additional information such as copyright information and even camera lens and GPS location of a photograph.</p></li><li><h3>General tidying up—fixing errata, clarifications, etc.</h3></li></ul><p>The last PNG spec was released over 20 years ago. Technology has advanced a lot since then. We're talking 3.5 years before the first iPhone.</p><p>In fact, technological advancement is what resurrected PNG. The W3C Timed Text Working Group (think:&nbsp;subtitles) needed HDR support in PNG. A proposal was made, but a few experts decided we could do better.</p><p>Momentum built, and additional parties became interested. Before we knew it, we had representation from (in alphabetical order) Adobe, Apple, BBC, Comcast / NBCUniversal, Google, MovieLabs, and of course W3C, who maintains the spec. It's quite the dream team.</p><p>With these titans behind it, the image format is back with full momentum. Work has already begun on the  PNG spec updates.</p><p>Many of the programs you use already support the new PNG spec: Chrome, Safari, Firefox, iOS/macOS, Photoshop, DaVinci Resolve, Avid Media Composer...</p><p>Plus, you saw some broadcast companies in that list above. Behind the scenes, hardware and tooling are being updated to support the new PNG spec. The next time you see a news ticker scrolling  or the score banner update as your team pulls out a clutch play, check if it is HDR.</p><p>I know you all immediately wondered, . We're already working on that. And parallel encoding/decoding, too! Just like this update, we want to make sure we do it right.</p><p>We expect the next PNG update (Fourth Edition) to be short. It will improve HDR &amp; Standard Dynamic Range (SDR) interoperability. While we work on that, we'll be researching compression updates for PNG Fifth Edition.</p>","contentLength":2363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44373504"},{"title":"Thnickels","url":"https://thick-coins.net/?_bhlid=8a5736885893b7837e681aa73f890b9805a4673e","date":1750810407,"author":"jxmorris12","guid":265,"unread":true,"content":"<p>Hello this is Theodore Nichols and welcome to my world wide web page.</p><p> I have partnered with someone to help me take <a href=\"https://www.legboot.com/thnickels-pre-order/\">pre-orders</a> for thnickels. Slots limited.</p><p> I have received hundreds of e-Mails from my flyer. I told my grandson that flyers still work in this day and age. He said societal media is the only way, but I was right. And he is still very thin. <b>Thank you all for the interest, I will be in touch.</b>-Theo</p><p> Thank you for the huge interest in my coins. Yes there are some pre order slots left but going fast.</p><p>Nickels are too thin we need a much heftier coin. That is why I am making thicker coins in my new <a href=\"https://thick-coins.net/?_bhlid=8a5736885893b7837e681aa73f890b9805a4673e#mint\">minting facility</a>.</p><p>Some burglars recently entered my garage to pilfer my things. The only weapon nearby was a , which I brandished at the ruffians.</p><img src=\"https://thick-coins.net/ten%20cents.jpg\" alt=\"two nickels was not an effective weapon\"><p>The burglars  about the slight bag of coins. They did not  me or .</p>They stole the nickels and my best push broom. <b>I have never felt so humiliated.</b><p>That's when I resolved to NEVER be disrespected about my pocketchange again.</p><h2>Introducing: Thicker Coins</h2><p>My new coins have a  when compared to currency from the U.S. Mint.</p><p>The enhanced weight is both  to good guys and  to bad guys.</p><img src=\"https://thick-coins.net/size%20comparison.jpg\" alt=\"wimpy nickel vs beefy thnickel\"><p>You can see in the photograph above how  a regular \"nickel\" is between my fingers.</p><p>The \"thnickel\" is , there is much more coin per coin.</p><p>Feel free to review my designs below. <i>I welcome feedback on the designs as long as it is positive and respectful of me.</i></p><p>I have converted my garage into a mint. I am dedicated to producing enough thnickels for everyone in need of respect.</p><img src=\"https://thick-coins.net/in%20garage.jpg\" alt=\"theodore in garage\"><p>My new state of some art facility is equipped with several tools and a powerful workhorse (me)!</p><p>Please help spread the word about my coins.</p><p>Post it in your neighborhood or city to let the people know about thnickels.</p><h3>NUMMOS CRASSIORES OMNIBUS</h3><p>If you want a thnickel coin or have any questions about my new mint please e-Mail me or call my phone.</p><img src=\"https://thick-coins.net/thnickel%20rotate.gif\" alt=\"coin2\">","contentLength":1839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44372424"},{"title":"Microsoft Edit","url":"https://github.com/microsoft/edit","date":1750810024,"author":"ethanpil","guid":264,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44372380"},{"title":"Image Compatibility In Cloud Native Environments","url":"https://kubernetes.io/blog/2025/06/25/image-compatibility-in-cloud-native-environments/","date":1750809600,"author":"","guid":741,"unread":true,"content":"<p>In industries where systems must run very reliably and meet strict performance criteria such as telecommunication, high-performance or AI computing, containerized applications often need specific operating system configuration or hardware presence.\nIt is common practice to require the use of specific versions of the kernel, its configuration, device drivers, or system components.\nDespite the existence of the <a href=\"https://opencontainers.org/\">Open Container Initiative (OCI)</a>, a governing community to define standards and specifications for container images, there has been a gap in expression of such compatibility requirements.\nThe need to address this issue has led to different proposals and, ultimately, an implementation in Kubernetes' <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html\">Node Feature Discovery (NFD)</a>.</p><p><a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html\">NFD</a> is an open source Kubernetes project that automatically detects and reports <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/customization-guide.html#available-features\">hardware and system features</a> of cluster nodes. This information helps users to schedule workloads on nodes that meet specific system requirements, which is especially useful for applications with strict hardware or operating system dependencies.</p><h3>Dependencies between containers and host OS</h3><p>A container image is built on a base image, which provides a minimal runtime environment, often a stripped-down Linux userland, completely empty or distroless. When an application requires certain features from the host OS, compatibility issues arise. These dependencies can manifest in several ways:</p><ul><li>:\nHost driver versions must match the supported range of a library version inside the container to avoid compatibility problems. Examples include GPUs and network drivers.</li><li>:\nThe container must come with a specific version or range of versions for a library or software to run optimally in the environment. Examples from high performance computing are MPI, EFA, or Infiniband.</li><li><strong>Kernel Modules or Features:</strong>:\nSpecific kernel features or modules must be present. Examples include having support of write protected huge page faults, or the presence of VFIO</li></ul><p>While containers in Kubernetes are the most likely unit of abstraction for these needs, the definition of compatibility can extend further to include other container technologies such as Singularity and other OCI artifacts such as binaries from a spack binary cache.</p><h3>Multi-cloud and hybrid cloud challenges</h3><p>Containerized applications are deployed across various Kubernetes distributions and cloud providers, where different host operating systems introduce compatibility challenges.\nOften those have to be pre-configured before workload deployment or are immutable.\nFor instance, different cloud providers will include different operating systems like:</p><ul></ul><p>Each OS comes with unique kernel versions, configurations, and drivers, making compatibility a non-trivial issue for applications requiring specific features.\nIt must be possible to quickly assess a container for its suitability to run on any specific environment.</p><p>An effort was made within the <a href=\"https://github.com/opencontainers/wg-image-compatibility\">Open Containers Initiative Image Compatibility</a> working group to introduce a standard for image compatibility metadata.\nA specification for compatibility would allow container authors to declare required host OS features, making compatibility requirements discoverable and programmable.\nThe specification implemented in Kubernetes Node Feature Discovery is one of the discussed proposals.\nIt aims to:</p><ul><li><strong>Define a structured way to express compatibility in OCI image manifests.</strong></li><li><strong>Support a compatibility specification alongside container images in image registries.</strong></li><li><strong>Allow automated validation of compatibility before scheduling containers.</strong></li></ul><p>The concept has since been implemented in the Kubernetes Node Feature Discovery project.</p><h3>Implementation in Node Feature Discovery</h3><p>The solution integrates compatibility metadata into Kubernetes via NFD features and the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/custom-resources.html#nodefeaturegroup\">NodeFeatureGroup</a> API.\nThis interface enables the user to match containers to nodes based on exposing features of hardware and software, allowing for intelligent scheduling and workload optimization.</p><p>The compatibility specification is a structured list of compatibility objects containing .\nThese objects define image requirements and facilitate validation against host nodes.\nThe feature requirements are described by using <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/customization-guide.html#available-features\">the list of available features</a> from the NFD project.\nThe schema has the following structure:</p><ul><li> (string) - Specifies the API version.</li><li> (array of objects) - List of compatibility sets.\n<ul><li> (int, optional) - Node affinity weight.</li><li> (string, optional) - Categorization tag.</li><li> (string, optional) - Short description.</li></ul></li></ul><p>An example might look like the following:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Client implementation for node validation</h3><p>To streamline compatibility validation, we implemented a <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/reference/node-feature-client-reference.html\">client tool</a> that allows for node validation based on an image's compatibility artifact.\nIn this workflow, the image author would generate a compatibility artifact that points to the image it describes in a registry via the referrers API.\nWhen a need arises to assess the fit of an image to a host, the tool can discover the artifact and verify compatibility of an image to a node before deployment.\nThe client can validate nodes both inside and outside a Kubernetes cluster, extending the utility of the tool beyond the single Kubernetes use case.\nIn the future, image compatibility could play a crucial role in creating specific workload profiles based on image compatibility requirements, aiding in more efficient scheduling.\nAdditionally, it could potentially enable automatic node configuration to some extent, further optimizing resource allocation and ensuring seamless deployment of specialized workloads.</p><ol><li><p><strong>Define image compatibility metadata</strong>\nA <a href=\"https://kubernetes.io/docs/concepts/containers/images/\">container image</a> can have metadata that describes its requirements based on features discovered from nodes, like kernel modules or CPU models.\nThe previous compatibility specification example in this article exemplified this use case.</p></li><li><p><strong>Attach the artifact to the image</strong>\nThe image compatibility specification is stored as an OCI artifact.\nYou can attach this metadata to your container image using the <a href=\"https://oras.land/\">oras</a> tool.\nThe registry only needs to support OCI artifacts, support for arbitrary types is not required.\nKeep in mind that the container image and the artifact must be stored in the same registry.\nUse the following command to attach the artifact to the image:</p></li></ol><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><ol start=\"3\"><li><strong>Validate image compatibility</strong>\nAfter attaching the compatibility specification, you can validate whether a node meets the image's requirements.\nThis validation can be done using the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/reference/node-feature-client-reference.html\">nfd client</a>:</li></ol><p><code>nfd compat validate-node --image &lt;image-url&gt;</code></p><ol start=\"4\"><li><strong>Read the output from the client</strong>\nFinally you can read the report generated by the tool or use your own tools to act based on the generated JSON report.</li></ol><p>The addition of image compatibility to Kubernetes through Node Feature Discovery underscores the growing importance of addressing compatibility in cloud native environments.\nIt is only a start, as further work is needed to integrate compatibility into scheduling of workloads within and outside of Kubernetes.\nHowever, by integrating this feature into Kubernetes, mission-critical workloads can now define and validate host OS requirements more efficiently.\nMoving forward, the adoption of compatibility metadata within Kubernetes ecosystems will significantly enhance the reliability and performance of specialized containerized applications, ensuring they meet the stringent requirements of industries like telecommunications, high-performance computing or any environment that requires special hardware or host OS configuration.</p><p>Join the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/contributing/\">Kubernetes Node Feature Discovery</a> project if you're interested in getting involved with the design and development of Image Compatibility API and tools.\nWe always welcome new contributors.</p>","contentLength":7628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deaf in Cloud Native: KubeCon + CloudNativeCon EU 2025 Keynote Experience","url":"https://www.youtube.com/watch?v=NtKrOlTjjhk","date":1750804603,"author":"CNCF [Cloud Native Computing Foundation]","guid":386,"unread":true,"content":"<article>Join the Deaf in Cloud Native community as we look back at our wonderful experience at KubeCon + CloudNativeCon Europe 2025 in London. This recap celebrates a landmark moment for our community - the first solo keynote delivered by one of our own, Rob Koch, CNCF DHHWG Co-Chair. In this session, Rob shares the behind-the-scenes story of his presentation, “Empowering Accessibility Through Kubernetes: The Future of Real-Time Sign Language Interpretation.” Following Rob’s story, community leaders Milad Vafaeifard and Anastasiia Gubska, a CNCF Ambassador, share their key takeaways, leading into a community roundtable where everyone shares their highlights from the conference.</article>","contentLength":684,"flags":null,"enclosureUrl":"https://www.youtube.com/v/NtKrOlTjjhk?version=3","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Releases Classic MS-DOS Editor For Linux","url":"https://linux.slashdot.org/story/25/06/24/2039231/microsoft-releases-classic-ms-dos-editor-for-linux?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1750803000,"author":"BeauHD","guid":295,"unread":true,"content":"Microsoft has released a modern, open-source version of its classic MS-DOS Editor -- built with Rust and compatible with Windows, macOS, and Linux. It's now simple called \"Edit.\" Ars Technica reports: \nAside from ease of use, Microsoft's main reason for creating the new version of Edit stems from a peculiar gap in modern Windows. \"What motivated us to build Edit was the need for a default CLI text editor in 64-bit versions of Windows,\" writes [Christopher Nguyen, a product manager on Microsoft's Windows Terminal team] while referring to the command-line interface, or CLI. \"32-bit versions of Windows ship with the MS-DOS editor, but 64-bit versions do not have a CLI editor installed inbox.\" [...]\n \nLinux users can download Edit from the project's GitHub releases page or install it through an unofficial snap package. Oh, and if you're a fan of the vintage editor and crave a 16-bit text-mode for your retro machine that actually runs MS-DOS, you can download a copy on the Internet Archive. [...]\n \nAt 250KB, the new Edit maintains the lightweight philosophy of its predecessor while adding features the original couldn't dream of: Unicode support, regular expressions, and the ability to handle gigabyte-sized files. The original editor was limited to files smaller than 300KB depending on available conventional memory -- a constraint that seems quaint in an era of terabyte storage. But the web publication OMG! Ubuntu found that the modern Edit not only \"works great on Ubuntu\" but noted its speed when handling gigabyte-sized documents.","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fun with uv and PEP 723","url":"https://www.cottongeeks.com/articles/2025-06-24-fun-with-uv-and-pep-723","date":1750790486,"author":"deepakjois","guid":263,"unread":true,"content":"<p>For the longest time, I have been frustrated with Python because I couldn’t use it for one-off scripts. I had to first ensure it was running in an environment where it could find the right Python version and the dependencies installed. That is now a thing of the past.</p><p>If you are not a Pythonista (or one possibly living under a rock), <a href=\"https://docs.astral.sh/uv/\">uv</a> is <em>an extremely fast Python package and project manager, written in Rust.</em></p><p>uv also provides this nifty tool called  (kinda like  from the Node/NPM ecosystem for Javascript/Typescript packages) which can be used to invoke a Python tool inside a package.  takes care of creating a (cached) disposable virtual environment, setting up the right Python version and installing all the dependencies before running.</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p><a href=\"https://peps.python.org/pep-0723/\">PEP 723</a> is a Python Enhancement Proposal that <em>specifies a metadata format that can be embedded in single-file Python scripts to assist launchers, IDEs and other external tools which may need to interact with such scripts.</em></p><p>Here is the example directly lifted from the proposal:</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Combining uv and the PEP-723 metadata inside a Python script, we can run the script in the previous section as follows:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>We can combine things we covered in the previous sections to create a simple executable script that can extract YouTube transcripts.</p><p>First we create a Python script with a shebang and inline metadata.</p><pre tabindex=\"0\" data-language=\"python\"><code></code></pre><p>Note the shebang line: <code>#!/usr/bin/env -S uv run --script</code>. It is important to specify  with the  flag when used on the shebang line.</p><p>We save this script as  and then make it executable with .</p><p>We can now run the script like:</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>This opens up a lot of possibilities for running Python code more seamlessly. Before this I used to prefer <a href=\"https://go.dev/\">Go</a> for one-off scripts because it was easy to create a self-contained binary executable. But now that I could use uv, I coded up a quick MCP server in Python for extracting YouTube transcripts. Check it out on Github at <a href=\"https://github.com/cottongeeks/ytt-mcp\">cottongeeks/ytt-mcp</a>.</p>","contentLength":1905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44369388"}],"tags":["dev"]}