{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Rust in Production at 1Password: 500k lines of Rust, 600 crates, 100 engineers - How they secure millions of passwords","url":"https://corrode.dev/podcast/s04e06-1password/","date":1750954316,"author":"/u/mre__","guid":639,"unread":true,"content":"<p>Handling secrets is extremely hard.\nYou have to keep them safe (obviously), while at the same time you need to integrate with a ton of different systems and always provide a great user-experience, because otherwise people will just find a way around your system.\nWhen talking to peers, a lot of people mention 1Password as a company that nailed this balance.</p><p>In today’s episode, I talk to Andrew about how 1Password uses Rust to build critical systems that must never fail, how Rust helps them handle secrets for millions of users, and the lessons they learned when adopting Rust in their stack.</p><div><p>\n    CodeCrafters helps you become proficient in Rust by building real-world,\n    production-grade projects. Learn hands-on by creating your own shell, HTTP\n    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.\n  </p><p>\n    Start for free today and enjoy 40% off any paid plan by using\n    <a href=\"https://app.codecrafters.io/join?via=mre\">this link</a>.\n  </p></div><p>1Password is a password manager that helps users securely store and manage their passwords, credit card information, and other sensitive data. It provides a user-friendly interface and strong security features to protect users’ secrets across multiple devices.</p><p>Andrew is a Senior Rust Developer at 1Password in the Product Foundations org, on the Frameworks team and specifically on the Core Platform squad handling the asynchronous frameworks other developers use to build features (i.e. requests into the Rust core from the Native clients, data sync, etc.).\nHe specifically specialized in that synchronization process, getting data federated from cloud to clients to native apps and back.</p><ul><li><a href=\"https://github.com/1Password/typeshare\">typeshare</a> - Generate types for multiple languages from Rust code</li><li><a href=\"https://github.com/1Password/zeroizing-alloc\">zeroizing-alloc</a> - 1Password’s minimal secure heap zero-on-free implementation for Rust</li><li><a href=\"https://github.com/1Password/arboard\">arboard</a> - Cross-platform clipboard manager written in Rust</li><li><a href=\"https://github.com/1password/passkey-rs\">passkey-rs</a> - Pure Rust implementation of the WebAuthn Passkey specification</li><li><a href=\"https://tokio.rs/\">tokio</a> - The de facto standard async runtime for Rust</li><li><a href=\"https://github.com/rust-lang/rust-clippy\">Clippy</a> - A collection of lints to catch common mistakes in Rust</li><li><a href=\"https://github.com/EmbarkStudios/cargo-deny\">cargo-deny</a> - Cargo plugin for linting dependencies, licenses, and security advisories</li><li><a href=\"https://nixos.org/\">Nix</a> - Purely functional package manager for reproducible builds</li><li><a href=\"https://nixos.wiki/wiki/Flakes\">Nix Flakes</a> - Experimental feature for hermetic, reproducible Nix builds</li><li><a href=\"https://direnv.net/\">direnv</a> - Load and unload environment variables based on current directory</li><li><a href=\"https://github.com/tokio-rs/axum\">axum</a> - Ergonomic and modular web framework built on tokio and tower</li><li><a href=\"https://github.com/tower-rs/tower\">tower</a> - Library for building robust networking clients and servers</li><li><a href=\"https://github.com/tokio-rs/tracing\">tracing</a> - Application-level tracing framework for async-aware diagnostics</li><li><a href=\"https://github.com/rusqlite/rusqlite\">rusqlite</a> - Ergonomic wrapper for SQLite in Rust</li><li><a href=\"https://docs.rs/mockall/latest/mockall/\">mockall</a> - Powerful mock object library for Rust</li><li><a href=\"https://neon-rs.dev/\">neon</a> - Library for writing native Node.js modules in Rust</li><li><a href=\"https://docs.rs/nom-supreme/latest/nom_supreme/\">nom-supreme</a> - Parser combinator additions and utilities for nom</li><li><a href=\"https://github.com/ipetkov/crane\">crane</a> - Nix library for building Cargo projects</li><li><a href=\"https://github.com/rust-lang/rustlings\">Rustlings</a> - Small exercises to get you used to reading and writing Rust code</li></ul>","contentLength":2833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ll46he/rust_in_production_at_1password_500k_lines_of/"},{"title":"Rust 1.88: 'If-Let Chain' syntax stabilized","url":"https://releases.rs/docs/1.88.0/","date":1750939066,"author":"/u/thurn2","guid":641,"unread":true,"content":"<blockquote><ul><li>Released on: </li><li>Branched from master on: </li></ul></blockquote><p>These previously stable APIs are now stable in const contexts:</p>","contentLength":100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lky9d2/rust_188_iflet_chain_syntax_stabilized/"},{"title":"\"Why is the Rust compiler so slow?\"","url":"https://sharnoff.io/blog/why-rust-compiler-slow","date":1750935847,"author":"/u/jahmez","guid":638,"unread":true,"content":"<div><p>I spent a month repeatedly building my website in Docker, and now have horrors to share.</p></div><p>My website (the one you're reading right now) is mainly served by a single Rust binary.\nFor  now, every time I wanted to make a change, I would:</p><ol><li>Build a new statically linked binary (with <code>--target=x86_64-unknown-linux-musl</code>)</li></ol><p>So instead, I'd like to switch to deploying my website with containers (be it Docker, Kubernetes, or otherwise),\nmatching the vast majority of software deployed any time in the last decade.</p><p>The only issue is that fast Rust builds with Docker are not simple.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#basics\"></a><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-simple\"><h3>Rust in Docker, the simple way</h3></a><p>To get your Rust program in a container, the typical approach you might find would be something\nlike:</p><pre><code></code></pre><p><strong>Unfortunately, this will rebuild everything from scratch whenever there's any change</strong>.</p><p>In my case, building from scratch takes about 4 minutes (including 10s to download the crates every time).</p><pre><code></code></pre><p>Sure, it could be worse. But I've grown accustomed to speedy local builds, thanks to incremental compilation — I don't\nwant to wait that long on every tiny change!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rust-in-docker-cargo-chef\"><h3>Rust in Docker, with better caching</h3></a><p>Thankfully, there's a tool to help with this!</p><p>Luca Palmieri's <a href=\"https://github.com/LukeMathWalker/cargo-chef\"></a> makes it easy to pre-build all of the dependencies as a separate layer in the docker\nbuild cache, so that changes in your codebase only trigger re-compilation of your codebase (and not your dependencies).</p><p>I'll save the detailed explanation for <a href=\"https://lpalmieri.com/posts/fast-rust-docker-builds/\">Luca's blog post</a>, but broadly  creates a simplified \"recipe\" file from\nthe current workspace, which can be \"cooked\" to cache the dependencies without being invalidated by changes in the\nworkspace.</p><p>My website pulls in a few hundred dependencies, so this  help!</p><pre><code>...\n\n</code></pre><p>Unfortunately though, it doesn't have quite the speedup we're looking for — most of the time is still in the final\nbinary:</p><pre><code></code></pre><p>Weirdly, only 25% of the time is actually spent on the dependencies! As far as I could tell, my code isn't doing\nanything fundamentally unreasonable. It's ~7k lines of gluing together various larger dependencies (, ,\n, among others.)</p><p><em>(Just to double-check, I tried running  with . It really was just a single\ninvocation of  that took almost 3 minutes!)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-rustc-doing\"><h2>What's  doing for all that time?</h2></a><pre><code></code></pre><p>In addition to that <code>cargo-timing-&lt;timestamp&gt;.html</code> file, there's also a . We'll\njust copy out the canonical version:</p><pre><code>...\n\n</code></pre><p>And with a little bit of container wrangling...</p><pre><code> cargo-timing.html\n container </code></pre><p>... we should be able to see what's going on! Let's have a look:</p><p> There's not really much information there!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#cargo-timings-weirdness\"></a><p> shows a bunch of information about <em>how long each crate took to compile</em>. But here, we only care\nabout the compilation time of the final crate!</p><p>That aside, this does help give us more accurate timing. Measuring outside the compiler adds some extra moving\npieces, or requires searching the output of  — so using 's self-reported timings will make more\nprecise analysis a bit easier, later on.</p><p>Just to check, the value here of 174.1s roughly matches the \"2m 54s\" we saw from the  output.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#rustc-self-profile\"><h2>Actually asking  this time</h2></a><p>The post from fasterthanlime had one more tip we can use — 's self-profiling feature, via the \nflag.</p><p>Normally, you'd probably run something like:</p><pre><code> rustc  --  self-profile\n</code></pre><p><em>(note: This is using  to pass extra flags to , with  to allow using the \nunstable flags on a stable compiler.)</em></p><p>Unfortunately, this won't work here — the change in arguments will invalidate the cached dependencies from\n, and there's no equivalent way to pass additional  flags through .</p><p>Instead, we can funnel everything via the  environment variable:</p><pre><code> chef cook .\n\n build .\n</code></pre><p>This gives us files like <code>web_http_server-&lt;random-number&gt;.mm_profdata</code>, which we can move and extract from the image in\nthe same way as we did for .</p><p><em>(note: It's much easier to automate if we remove the profiling data that was added from  before the\nfinal build. That's omitted here, for brevity.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#using-rustc-profdata\"><h3>Actually using the profdata</h3></a><ul><li> – produces plaintext output summarizing the profiling data</li></ul><p>But let's install a couple of these to take a look at what we've got:</p><pre><code> https://github.com/rust-lang/measureme flamegraph summarize\n</code></pre><p>I personally use Firefox, so we'll hold off on the chrome tracing stuff for now.</p><p>First, with  (which itself has the  and  subcommands):</p><pre><code></code></pre><p>So at a high level, the two biggest things are <a href=\"https://www.llvm.org/docs/LinkTimeOptimization.html\">link-time optimization</a> (LTO) and\n<code>LLVM_module_codegen_emit_obj</code>, whatever that is.</p><p>Let's see if we can dig a bit deeper with the flamegraph:</p><pre><code></code></pre><p><em>(It's interactive! If you're curious, you can click through and play around with it yourself.)</em></p><p>So there's presumably some inter-mingling going on between codegen and LTO: <code>codegen_module_perform_lto</code> ends up falling\nthrough to both / and .</p><p>But either way, we've got a problem with LTO: <code>codegen_module_perform_lto</code> took ~80% of the total time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#about-lto\"><h2>It's time to talk about LTO</h2></a><p>The Rust compiler splits up crates into \"<a href=\"https://doc.rust-lang.org/rustc/codegen-options/index.html#codegen-units\">codegen units</a>\", handing each to LLVM as a separate module to compile.\n, optimizations take place within each codegen unit, and then they're linked together at the end.</p><p>LTO controls the set of optimizations that LLVM will make during that link-time — for example, inlining or\noptimization across codegen units.</p><ul><li>\"thin\" LTO — in theory, similar performance benefits to \"fat\" LTO, but less expensive to run</li><li>\"fat\" LTO — maximum amount of LTO, across all crates at the same time</li></ul><p>And if the LTO option is not specified,  uses \"thin local LTO\", which limits \"thin\" LTO only to a single crate at\na time.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-current-settings\"><h3>What are the current settings</h3></a><p>Turns out that a few years back, I had set  in my :</p><pre><code></code></pre><p>And, while we're at it, <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#debug\"></a> enables all debug symbols (where they'd normally be excluded by default for the\n profile). Maybe we should take a look at that as well.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#lto-tweaking-settings\"><h3>Tweaking the (normal) settings</h3></a><p>Let's take a look at the compile times and binary sizes for a variety of  and  settings (using\n like before, for more precise timing).</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>At a high level: It seems like the worst cases here are full debug symbols adding 30-50% to the compilation time, and\n\"fat\" LTO taking about  than with LTO fully disabled.</p><p>That mostly tracks with what we'd expect from the documentation — yeah, fat LTO takes longer. But when we disable\neverything, we're still looking at 50 seconds compiling the final binary!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note\"><h2>A brief note: 50 seconds is , actually!</h2></a><p>Look, 50 seconds is already a great improvement — and if it requires disabling LTO and debug symbols... my website\ngets approximately zero load. <em>It would be totally fine.</em> It would be perfectly sustainable, even!</p><p><strong>There's no practical reason to keep digging here.</strong></p><p>But where's the fun in leaving it there? We should be able to do better, right?</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#brief-note-2\"><h2>Another brief note: Can't we just use incremental compilation?</h2></a><p>It's slightly more complicated, but yes, absolutely — for local development, at least. Consistently loading the build\ncache isn't straightforward, but you'd want to make the  directory accessible with a <a href=\"https://docs.docker.com/build/cache/optimize/#use-cache-mounts\">\"cache mount\"</a> in the\ndockerfile, and persist that target directory between builds.</p><p>That said, I value that  have a clean environment every time, and I think it's worthwhile to go\nthrough docker's own caching system — which is why I'm using  in the first place.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#et-tu-llvm_module_optimize\"><h2>Digging deeper: Et tu, ?</h2></a><p>If we disable LTO and debug symbols, compiling the final binary still takes 50 seconds to do... something.</p><p>Let's re-run the self-profiling to check out what's going on.</p><p>It's ~70% just  — i.e. where LLVM is optimizing the code. Before diving into LLVM itself, let's\nsee if there's any easier knobs we can tune.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#tuning-optimization\"></a><p>The  profile uses  by default — maybe if we reduce the optimization level, we'll spend less\ntime on it.</p><p>We can actually do one better — since our dependencies are cached, and we only care about the final binary, we can get\nmost of the benefits by only reducing optimizations on the final binary:</p><pre><code></code></pre><p>Like the previous options, there's a handful of <a href=\"https://doc.rust-lang.org/cargo/reference/profiles.html#opt-level\">s</a> we can choose from:</p><ul><li>, , and  enable increasing levels of optimizations</li><li> and  are different flavors of prioritizing binary size</li></ul><p>Going through a handful of combinations here again:</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><ul><li>The baseline for  level of optimizations on the final binary is about 50 seconds</li><li>If we disable all optimizations, then it's pretty quick: only ~15s</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm\"></a><p>Rust relies pretty heavily on optimizations, and while it'd probably be fine to just blanket-disable them for the final\nbinary, it'd be pretty cool if we can at least keep  optimizations!</p><p><strong>So let's try to figure out what's taking so long.</strong>'s self-profiling doesn't give us any more detail though,\nso we'll have to get it from LLVM.</p><ul><li> – emit LLVM profiling information as plaintext</li><li> – emit LLVM profiling information in the chrome tracing format (again with that one!)</li></ul><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-text\"><h3>Profiling LLVM with  — plain text</h3></a><p>Like before, let's skip the chrome tracing format for now, and see what we can get from plain text.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>... Unfortunately if you try to  again, you'll immediately hit something like:</p><pre><code>[output clipped, log limit 2MiB reached]\n</code></pre><p>This is because <a href=\"https://docs.docker.com/build/buildkit/\">BuildKit</a> (if you're using ~recent Docker on Linux) has default output limits that are pretty small.</p><p>So after getting unlimited  output on the terminal, what's in it?\n — probably not what you want to be copying from your terminal, anyways.</p><p>So, redirecting to a file inside docker and copying that out like before, we get a bunch of pass/analysis timing\nreports. They each look something like this:</p><pre><code>===-------------------------------------------------------------------------===\n                          Pass execution timing report\n===-------------------------------------------------------------------------===\n  Total Execution Time: 0.0428 seconds (0.0433 wall clock)\n\n   ---User Time---   --System Time--   --User+System--   ---Wall Time---  — Name ---\n   0.0072 ( 19.2%)   0.0015 ( 27.4%)   0.0086 ( 20.2%)   0.0087 ( 20.0%)  InstCombinePass\n   0.0040 ( 10.8%)   0.0006 ( 10.8%)   0.0046 ( 10.8%)   0.0047 ( 10.8%)  InlinerPass\n   0.0024 (  6.4%)   0.0010 ( 18.0%)   0.0034 (  7.9%)   0.0034 (  7.8%)  SimplifyCFGPass\n   0.0022 (  5.9%)   0.0002 (  4.5%)   0.0025 (  5.7%)   0.0024 (  5.6%)  EarlyCSEPass\n   0.0021 (  5.5%)   0.0001 (  1.5%)   0.0021 (  5.0%)   0.0022 (  5.0%)  GVNPass\n   0.0015 (  4.0%)   0.0001 (  2.2%)   0.0016 (  3.8%)   0.0018 (  4.2%)  ArgumentPromotionPass\n\n   ... entries here continue, and more passes below, for hundreds of thousands of lines ...\n</code></pre><p>It certainly is  to parse and analyze these! But it's also hard to be certain about what you're looking at\nwhen each pass execution is emitted separately and multi-threading can interfere with timing.</p><p>Let's see if there's a better way to get good data.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#profiling-llvm-trace\"><h3>Profiling LLVM with  — actual tracing this time</h3></a><p>We skipped  earlier because it emits the chrome tracing format.</p><pre><code> chef cook .\n\n build .\n</code></pre><p>It produces a bunch of <code>$package-$hash.llvm_timings.json</code> files, alongside the normal compilation artifacts:</p><pre><code></code></pre><p><em>(Why ? Setting up rootless docker didn't work when I tried it a few years back, and I haven't bothered since)</em></p><p>So, deleting  between  and the final build, we can extract the singular profile for the\nfinal binary into <code>web_http_server.llvm_timings.json</code>.</p><p>There's just one minor hiccup:</p><pre><code></code></pre><p>It's . It's also all one single line!</p><p>In theory though, a wide variety of tools should be able to process this:</p><p>None of these options worked for me — but it's a big JSON file with a <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0\">known format</a>, how hard can it be?</p><p>Turns out, a 1.4GiB single line of JSON makes all the normal tools complain:</p><ul><li>If you try to view it with , scrolling blocks on processing the entire file</li><li>If you try to process it with , it has to load the entire 1.4GiB into 's internal format (which expectedly\ntakes up  more than the original 1.4GiB)</li><li>Vim hangs when you open it</li><li>And you probably don't want to just  it to the terminal — again, it's 1.4GiB!</li></ul><p>So instead, we can just look at a few hundred characters, at the start and end of the file:</p><pre><code></code></pre><p>Matching this to the <a href=\"https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview?tab=t.0#heading=h.q8di1j2nawlp\">\"JSON Object Format\"</a> from the chrome tracing spec, it seems we have a single JSON object like:</p><pre><code>\n    ...\n  </code></pre><p>We'd be able to process it with normal tools if we split each event into its own object. That could be something like:</p><pre><code> web_http_server.llvm_timings.json  web-http-server.llvm_timings.jsonl\n</code></pre><p><em>(i.e.: turn  into a newline, strip the start of the object, strip the end of the object)</em></p><p>And  we can process this.</p><pre><code></code></pre><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#whats-in-llvm-trace\"><h2>What's in LLVM's trace events?</h2></a><p>It looks like these events all have .</p><p>According to the spec, the  field gives the type of event, and  refers to \"complete\" events, recording how long a\nparticular piece of work took on a given thread (). The duration in microseconds is given by .</p><p>Aside from that, we also have  events:</p><pre><code></code></pre><p>These are \"metadata\" events — in our case, not much useful information.</p><p>And aside from these, there's nothing else:</p><pre><code></code></pre><p>Going back to those  events — there were a bunch of them with . What else do we have?</p><pre><code></code></pre><p>Neat! It looks like we might be able to demangle some of the symbols to get timings on individual functions.</p><p>If we track what's being run and how long it takes, we should be able to get a better sense of why our compile time is\nso long.</p><p>Later on, there's aggregate information for certain types of events, like . These are equivalent to\nthe sum of the duration for that event type (in this case, ). Let's see kind of operations are taking the\nmost time:</p><pre><code></code></pre><p>This particular run took ~110 seconds on a 16-core machine, so it's clear that some passes are being double-counted\n(which makes sense — we see both , and it looks like \nprobably just calls ).</p><p>But broadly, it seems like optimization () and inlining () are the two parts taking a lot of\ntime — let's see if we can do anything about it.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-inlinerpass-faster\"><h2>Can we make  any faster?</h2></a><p>LLVM has a bunch of arguments that can be configured, which  exposes through the  flag. At time of\nwriting (June 2025), there's somewhere in the region of ~100 options that mention inlining (via <code>rustc -C llvm-args='--help-list-hidden'</code>).\nIn particular, there's a <a href=\"https://github.com/llvm/llvm-project/blob/c7063380205d8776e281f7a6603119aa8ea28c12/llvm/lib/Analysis/InlineCost.cpp#L58-L176\">bunch of relevant options</a> in the file controlling the cost analysis.</p><p>Now, I'll be honest, I know  about LLVM's inlining. Most of the options refer to the \"cost\" associated\nwith the inlining, or with the function being inlined, etc. <strong>I'm flying mostly bind here.</strong> But there's a few arguments\nthat seem like decent candidates for tuning:</p><ul><li><code>--inlinedefault-threshold=225</code> — \"Default amount of inlining to perform\"</li><li> — \"Control the amount of inlining to perform\"</li><li><code>--inlinehint-threshold=325</code> — \"Threshold for inlining functions with inline hint\"</li></ul><p>For all of these, the \"threshold\" roughly means \"allow inlining functions with cost  the threshold\", so a higher\nthreshold means more inlining.</p><p>So if we set all of these to some value (e.g., ), we should see that there's less inlining, and in turn faster\ncompile times.</p><pre><code>.\n</code></pre><p><em>(Why separate ? I couldn't find a way to make the whitespace happy through the  environment\nvariable — maybe it's possible if you set  in , but this solution worked 🤷)</em></p><p>In any case, reducing to a threshold of 50  end up faster! About 42.2s, down from 48.8s.</p><p>Here's what that looks like across a handful of values:</p><p><em>(note: The smallest value is 1, and not zero. Why 1? Sometimes zero has special behavior – setting to one seemed like a safer bet.)</em></p><p>Of these, it's hard to say exactly what the best value is, but for my use case (remember: my website gets ~zero load!),\nsetting the thresholds to 10 looks promising. We'll hold off on that for now though.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#making-optfunction-faster\"><h2>Can we make  any faster?</h2></a><p>Optimizing functions was the other expensive task we saw.</p><p>The knobs here are much less clear to me (we're already at , and  compeltely disables\noptimizations). So, let's see what exactly is taking so long.</p><p>First, a brief look at the event format:</p><pre><code></code></pre><p>In its raw form, each of the events'  field has the mangled symbol of the function being optimized. We can\n\"demangle\" these back to the original Rust symbols with <a href=\"https://github.com/luser/rustfilt\"></a> — for example:</p><pre><code></code></pre><p>It's worth noting that in the list above, while there's several <code>serde_json::value::to_value</code> items, they actually have\ndistinct hashes:</p><pre><code></code></pre><p>... which makes sense, given that <code>serde_json::value::to_value</code> is a generic function — it might be that it's being\noptimized with different generic parameters (\"monomorphizations\").</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#why-optimizing-other-crates\"><h3>Wait, why are we optimizing functions from other crates?</h3></a><p>The short answer is that optimization is done <em>in the context of the crate where a function is monomorphized</em>. So if we\ndefine a type  and then call methods on , those methods  will first exist in the\ncontext of our crate — meaning it gets compiled and optimized with the same configuration as our crate.</p><p>With some knowledge about how the compiler works under the hood, this should hopefully make some sense — but from the\noutside, it's certainly a little odd!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-are-we-optimizing\"><h3>What's actually taking so long?</h3></a><p>Now that we know what we're looking at, we can start doing some analysis. For example, by finding the individual\nfunctions we spent the most time optimizing:</p><pre><code></code></pre><p><em>(Why two separate  invocations? If we did just one, the / call would load the entire file\ninto a single array before any processing, which is one of the key operations we're trying to avoid)</em></p><p>This is a surprising amount of time on individual functions! Profiling roughly doubled the total time to compile, but\neven 1 second optimizing a single function is quite a long time!</p><p>But let's look into more detail here. We've got:</p><ul><li><code>web_http_server::photos::PhotosState::new::{{closure}}</code> — this is  closure inside a giant, 400-line async\nfunction that does the setup for <a href=\"https://sharnoff.io/photos\">https://sharnoff.io/photos</a></li><li><code>web_http_server::run::{{closure}}</code> — this is inside the main entrypoint (also async), but all the closures are small\nerror-handling, like <code>.wrap_err_with(|| format!(\"failed to bind address {addr:?}\"))</code><ul><li>Maybe there's something weird going on here!</li></ul></li></ul><p>... and a handful of dependencies that also took a while:</p><p>, we could break it down by the outermost crate:</p><pre><code></code></pre><p>This is, of course, a very imperfect measure — the outermost crate isn't necessarily the best one to attribute the\ncompilation time to, and there's a lot of items like  that aren't captured by this simple filtering.\nBut all that aside, it's still surprising that there's so much from !</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#closures-mangling-v0\"><h3>Digging more into closures, with mangling v0</h3></a><p>The long compile times for closures seems very suspicious — maybe it's worth digging further. There's just one\nproblem: the symbols all end with  without saying  is taking all the time.</p><p>As it turns out, there's an easy fix! As of June 2025,  currently uses the \"legacy\" symbol mangling format by\ndefault, but there's a newer option with more information: the <a href=\"https://doc.rust-lang.org/rustc/symbol-mangling/v0.html\">v0 format</a>.</p><p>We can enable it by adding <code>RUSTFLAGS=\"-C symbol-mangling-version=v0\"</code> to our existing flags, which now look something\nlike:</p><pre><code>RUSTC_BOOTSTRAP=1 RUSTFLAGS=\"-Csymbol-mangling-version=v0 -Zllvm-time-trace\" cargo build --timings ...\n</code></pre><p><em>(aside: The issue for that feature's been open for 6 years, why hasn't it been merged yet? Turns out, there's a lot of\nupstream work required to add support in common tools like  and . A lot of that has been done, but not yet\neverything.)</em></p><p>The end result of this is that we get  better symbols coming out of the LLVM trace. As an example, here's what\nthose <code>serde_json::value::to_value</code> symbols look like now:</p><pre><code></code></pre><p>So not only do we get better closure labeling (see e.g. ) but we also get full generics for everything!</p><p>Exactly what's taking so long  be much clearer now:</p><pre><code></code></pre><p>... but those first few closures are :</p><pre><code> is_jpg  path s</code></pre><pre><code> app  feed </code></pre><p>And if we remove these closures, replacing them with separately defined functions where possible, LLVM  reports\ntaking a long time to optimize  in the outer function.</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#async-closure0\"><h3>So where are those closures coming from?</h3></a><p>After dumping the LLVM IR with <code>RUSTFLAGS=\"--emit=llvm-ir\"</code> (which places it into ) and searching\nthrough the generated functions, I found a line like:</p><p>That  function was a nested async function, defined directly inside  — so why did the\nsymbol say it was defined inside a closure?</p><p>It's because <em>internally represents async functions/blocks with a nested closure</em>. So all of these places that\nwe had async functions where compiling  took a long time were actually just referring to the function itself!</p><p>With some quick github searching (<code>is:issue state:open async fn closure mangle</code>), it turned out there was already an\n<a href=\"https://github.com/rust-lang/rust/issues/104830\">open issue about this</a>!</p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#big-async-functions-considered-harmful\"><h3>Big async functions considered harmful?</h3></a><p>Going back to our list from before – those async functions where LLVM takes a long time to optimize  are\nreally just spending a long time on the body of the function itself. It would make sense that big functions are hard to\noptimize, and async functions doubly so.</p><p>It's  straightforward to identify all of the functions inside the main crate that are taking a long time:</p><pre><code></code></pre><p>Some of the most expensive functions here are around setup.</p><p><strong>Let's try breaking up just one function, to see if it helps.</strong> We'll start with .</p><p>On the first attempt, I tried breaking it up while also preserving the number of s – it's easy to do both\naccidentally, and this would hopefully isolate which type of complexity is causing problems.</p><p>Interestingly, this didn't help all that much: only reducing the total time from 5.3s to 4.7s.</p><p>So to add to that, I tried merging a handful of neighboring s into their own functions — reducing the total\nnumber from 10 to 3.</p><p>But that took substantially longer! It increased from 4.66s to 6.24s!</p><p>At this point, it seemed like there was something strange happening with async functions. Otherwise, why would splitting\ninto more functions make things worse?</p><p>Under the hood, async functions desugar to a complex state machine. There might be something odd happening there, so if\nwe want to make that simpler in the caller, we can turn the  into a trait object to obscure the implementation\nbehind it (typically ).</p><p>So this time, let's add a new function like:</p><pre><code>\n    futfut</code></pre><p>and using it everywhere we . For example:</p><pre><code> candidates  candidates </code></pre><p><strong>This one worked — down to 2.14s.</strong></p><p>So, a reduction from 5.3s to 2.14s – a notable improvement, albeit with a lot of effort to get there. (and, for the\nrecord, when I wrapped the futures with  instead of a fresh function, it didn't make a difference here).</p><p>Re-running the build without profiling, this gives a total reduction from 48.8s to 46.8s. It's pretty small, but that's\nfrom just a single function!</p><p><em>(Aside: What about ? I tried it with and without – after boxing, compile times weren't any better\nwith inlining disabled for those functions, but it's still helpful for ensuring better attribution on the LLVM\ntimings.)</em></p><p><em>(Aside: What about disabling inlining on the  functions? I also tried wrapping the async functions with a\n implementation having  on its poll function. That helped , but wasn't as good as\nboxing.)</em></p><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#putting-it-together\"></a><p>There's a number of approaches available — let's try:</p><ol><li>Reducing inlining with LLVM args;</li><li>Breaking up expensive functions in the main crate; and</li><li>Removing generics from dependencies to prevent needing to compile it in the main crate</li></ol><p>So, updating the final Dockerfile commands to read:</p><pre><code>\n\n...\n\n</code></pre><p>... and many more small changes to the main crate:</p><pre><code></code></pre><p>... alongside some changes to larger dependencies:</p><p>... gives us a final compile time of .</p><ol><li>Disabling LTO (and debug symbols!) got us to 51s (-71%) </li><li>Changing to  on the final crate got us to 48.8s (-4%)</li><li>Reducing inlining with  got us to 40.7s (-16%) </li><li>Local changes got us to 37.7s (-7%) </li><li>And changes with dependencies got us to 32.3s (-14%) </li></ol><a href=\"https://sharnoff.io/blog/why-rust-compiler-slow#what-now\"></a><p>While I did hit a lot of issues here, the tooling honestly worked really well – and the documentation was sufficient for\nsomeone with relatively little experience to make meaningful improvements to their codebase.</p><p>Some of the issues are straightforward: bugfixes to provide a nicer experience for the next person that finds themselves\nin a similar mess.</p><p>Others are more complicated:</p><ul><li><p>The compile time of deep call graphs of async functions needs to be improved – perhaps LLVM has a degenerate edge\ncase that's easy to trigger with what  generates, or maybe it's as simple as a bad heuristic that's\nunder-utilized in other languages.</p></li><li><p>It  be worthwhile for  to special-case <code>core::ptr::drop_in_place&lt;T&gt;</code> so that it's compiled in the crate\nthat defines . That approach wouldn't work for everything – for example, generic types – but would prevent\ndownstream crates from needing to re-compile the same destructor multiple times.</p></li><li><p>There might also be room for tooling to help with isolating which parts of a codebase are taking up the most time\nduring compilation (and providing recommendations to mitigate) – although that's a longer project than just this post.</p></li></ul><p><strong>In the meantime, setting  might be just fine :)</strong></p><div><p><em>(questions? comments? Feel free to reach out below!)</em></p></div>","contentLength":24235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lkxad3/why_is_the_rust_compiler_so_slow/"},{"title":"How much code does that proc macro generate?","url":"https://nnethercote.github.io/2025/06/26/how-much-code-does-that-proc-macro-generate.html","date":1750916451,"author":"/u/nnethercote","guid":637,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lks869/how_much_code_does_that_proc_macro_generate/"},{"title":"How do you see the current state and future of Rust? And, will Rust get popular in game dev?","url":"https://www.reddit.com/r/rust/comments/1lkmloc/how_do_you_see_the_current_state_and_future_of/","date":1750898741,"author":"/u/lettsten","guid":636,"unread":true,"content":"<p>I'm a hobbyist who've been eyeing Rust for a while, dabbled a bit. As a hobbyist I don't have my finger on the industrial pulse and would like to hear your thoughts and insights about the current state of Rust in general—things that are hard for me to look up on a wiki page and that requires the insights of those of you who work with it regularly or semi-regularly.</p><p><strong>What do you think about the current state of Rust as a language, ecosystem and community?</strong></p><p>I've seen some flak about async in Rust. Do you agree with it? How happy are you about the current state of the language? Is Rust your favourite language? What are your biggest gripes with the language, and do you think they will be resolved within the next 2-5 years?</p><p>From what I understand, Rust jobs are rare. Is your impression that they are becoming more common? Do you think Rust will become more prevalent than C or C++ at some point?</p><p>Are you happy with the Rust ecosystem, tooling, library availability and so on? Which areas shine, and which are most lacking? What are your opinions on the Rust community, in terms of demographics, friendliness, activity, open-source work and so on?</p><p>My impression is that Rust is most suited to systems level programming, especially critical components where correctness is essential. Do you see Rust taking over other segments or domains?</p><p>Reason I ask these questions is honestly because I would love to get psyched about Rust again, and because I would like an honest and well-informed impression of the current state of the language.</p><p>Any and all insights are very welcome!</p><p>Edit: <strong><em>I'm mostly interesting in the state of Rust as a whole</em></strong>, the gamedev question from the subject is secondary.</p>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI is Ditching TypeScript to Rebuild Codex CLI with Rust","url":"https://analyticsindiamag.com/global-tech/openai-is-ditching-typescript-to-rebuild-codex-cli-with-rust/","date":1750876865,"author":"/u/GeneReddit123","guid":640,"unread":true,"content":"<p>When OpenAI launched Codex CLI, the aim was to make it easier for developers to interact with AI in the terminal using a familiar stack—TypeScript and React-based Ink.&nbsp;</p><p>While it tries to compete with Claude Code and <a href=\"https://analyticsindiamag.com/ai-features/why-openais-codex-is-not-as-good-as-devin-or-replit/\">similar tools</a>, the team decided to revamp its foundation for better performance. “We’ve been working on a rewrite of Codex CLI into Rust,” <a href=\"https://www.linkedin.com/in/fouadmatin/\">Fouad Matin</a>, a member of technical staff at OpenAI, said in a <a href=\"https://github.com/openai/codex/discussions/1174\">GitHub discussion thread</a>.&nbsp;</p><p>Although the TypeScript version was productive for fast prototyping, it started to show its limits as the Codex CLI matured with various use cases.&nbsp;</p><p>Now, OpenAI plans to retire the TypeScript CLI entirely in favour of Rust. Matin mentioned that the TypeScript version will continue to receive bugfixes for now and that the focus is on bringing the native Rust build to feature parity and eventually making it the default.</p><h2>4 Benefits of Rust at Its Core</h2><p>While <a href=\"https://analyticsindiamag.com/ai-features/when-rust-rusts-the-developer-experience/\">Rust has its own set of problems</a> and benefits, the switch isn’t about language ideology. As Matin put it, “We want to use the best tool for the job.” Codex CLI may have launched with “a neat terminal UI” built on React.&nbsp;</p><p>However, he said that, at its core, the CLI functions as a tool working in a loop that keeps talking to the AI model and working with the system, instead of simply displaying a nice terminal interface. For that kind of repeated interaction with local system resources and APIs, TypeScript began to fall short.</p><p>“We wanted to improve a few areas,” Matin explained.&nbsp;</p><p>First, the installation experience required improvement, as the current version “requires Node v22+, which can be frustrating or a blocker for some users”. Second, improvements were to be made in native security bindings. “We already ship a Rust for Linux sandboxing since the bindings were available.” And third, the focus was on runtime performance. “No runtime garbage collection, resulting in lower memory consumption.”</p><p>Beyond performance, Rust offered architectural breathing room. Matin said OpenAI is developing a “wire protocol” for Codex CLI, which will enable developers to extend the agent using various languages, such as TypeScript/JavaScript and Python. Rust is already supported for MCPs.</p><p>In other words, Codex CLI isn’t just a tool; it aims to evolve into a cross-language, plug-in-friendly runtime for model-based automation.</p><h2>Work in Progress, but ‘Butter Smooth’</h2><p>While the new Rust version is still under development, the response so far has been optimistic. One developer <a href=\"https://github.com/rizwankce\">reported</a> that “codex native is butter smooth so far”, even though there are still some discrepancies between the TypeScript and native versions. This includes configuration file support and the ability to use the free tier mode or log in with an OpenAI account.</p><p>OpenAI is systematically addressing those gaps. In a separate <a href=\"https://github.com/openai/codex/issues/1262\">GitHub thread</a>, <a href=\"https://www.linkedin.com/in/michael-bolin-7632712/\">Michael Bolin</a>, a member of technical staff at OpenAI, categorised the remaining work as P0 (must-fix), P1 (feature parity), and P2 (quality-of-life).&nbsp;</p><p>High-priority features for the native Rust version include ‘Sign in with ChatGPT’ and improved interruption handling. Other features, like session management and prompt suggestions, are set to follow after the feature parity is handled.</p><p>“We will ultimately be retiring the TypeScript version of the CLI in favour of the Rust one,” Bolin wrote in the <a href=\"https://github.com/openai/codex/issues/1262\">GitHub thread</a>. The roadmap reflects a methodical upgrade path rather than a rushed rewrite.</p><h2>Native is the New Normal?</h2><p>The move fits a larger industry narrative. On Hacker News, a <a href=\"https://news.ycombinator.com/item?id=44150814\">user</a> called it part of a “recent resurgence of tools going native”.&nbsp;</p><p>The user explained that the notion of JIT (just-in-time) interpreters becoming better and eliminating the need for native languages is increasingly being challenged.</p><p>Meanwhile, another user <a href=\"https://news.ycombinator.com/item?id=44154144\">noted</a> that Rust and Go have made native development far more accessible. “The package management is better, and statically linked native binaries eliminate so many deployment headaches,” the user wrote.</p><p>With Rust, OpenAI isn’t just changing the codebase; it’s changing what kind of software Codex CLI can be. From terminal utility to programmable agent harness, the CLI is being rebuilt not just for speed, but also for flexibility, portability, and long-term maintainability.&nbsp;</p><p>If the TypeScript version was deployed for a playground, the Rust rewrite is planned to be ready for the real world.</p>","contentLength":4406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1lkdu2m/openai_is_ditching_typescript_to_rebuild_codex/"}],"tags":["dev","reddit","rust"]}