{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Image Compatibility In Cloud Native Environments","url":"https://kubernetes.io/blog/2025/06/25/image-compatibility-in-cloud-native-environments/","date":1750809600,"author":"","guid":741,"unread":true,"content":"<p>In industries where systems must run very reliably and meet strict performance criteria such as telecommunication, high-performance or AI computing, containerized applications often need specific operating system configuration or hardware presence.\nIt is common practice to require the use of specific versions of the kernel, its configuration, device drivers, or system components.\nDespite the existence of the <a href=\"https://opencontainers.org/\">Open Container Initiative (OCI)</a>, a governing community to define standards and specifications for container images, there has been a gap in expression of such compatibility requirements.\nThe need to address this issue has led to different proposals and, ultimately, an implementation in Kubernetes' <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html\">Node Feature Discovery (NFD)</a>.</p><p><a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/stable/get-started/index.html\">NFD</a> is an open source Kubernetes project that automatically detects and reports <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/customization-guide.html#available-features\">hardware and system features</a> of cluster nodes. This information helps users to schedule workloads on nodes that meet specific system requirements, which is especially useful for applications with strict hardware or operating system dependencies.</p><h3>Dependencies between containers and host OS</h3><p>A container image is built on a base image, which provides a minimal runtime environment, often a stripped-down Linux userland, completely empty or distroless. When an application requires certain features from the host OS, compatibility issues arise. These dependencies can manifest in several ways:</p><ul><li>:\nHost driver versions must match the supported range of a library version inside the container to avoid compatibility problems. Examples include GPUs and network drivers.</li><li>:\nThe container must come with a specific version or range of versions for a library or software to run optimally in the environment. Examples from high performance computing are MPI, EFA, or Infiniband.</li><li><strong>Kernel Modules or Features:</strong>:\nSpecific kernel features or modules must be present. Examples include having support of write protected huge page faults, or the presence of VFIO</li></ul><p>While containers in Kubernetes are the most likely unit of abstraction for these needs, the definition of compatibility can extend further to include other container technologies such as Singularity and other OCI artifacts such as binaries from a spack binary cache.</p><h3>Multi-cloud and hybrid cloud challenges</h3><p>Containerized applications are deployed across various Kubernetes distributions and cloud providers, where different host operating systems introduce compatibility challenges.\nOften those have to be pre-configured before workload deployment or are immutable.\nFor instance, different cloud providers will include different operating systems like:</p><ul></ul><p>Each OS comes with unique kernel versions, configurations, and drivers, making compatibility a non-trivial issue for applications requiring specific features.\nIt must be possible to quickly assess a container for its suitability to run on any specific environment.</p><p>An effort was made within the <a href=\"https://github.com/opencontainers/wg-image-compatibility\">Open Containers Initiative Image Compatibility</a> working group to introduce a standard for image compatibility metadata.\nA specification for compatibility would allow container authors to declare required host OS features, making compatibility requirements discoverable and programmable.\nThe specification implemented in Kubernetes Node Feature Discovery is one of the discussed proposals.\nIt aims to:</p><ul><li><strong>Define a structured way to express compatibility in OCI image manifests.</strong></li><li><strong>Support a compatibility specification alongside container images in image registries.</strong></li><li><strong>Allow automated validation of compatibility before scheduling containers.</strong></li></ul><p>The concept has since been implemented in the Kubernetes Node Feature Discovery project.</p><h3>Implementation in Node Feature Discovery</h3><p>The solution integrates compatibility metadata into Kubernetes via NFD features and the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/custom-resources.html#nodefeaturegroup\">NodeFeatureGroup</a> API.\nThis interface enables the user to match containers to nodes based on exposing features of hardware and software, allowing for intelligent scheduling and workload optimization.</p><p>The compatibility specification is a structured list of compatibility objects containing .\nThese objects define image requirements and facilitate validation against host nodes.\nThe feature requirements are described by using <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/usage/customization-guide.html#available-features\">the list of available features</a> from the NFD project.\nThe schema has the following structure:</p><ul><li> (string) - Specifies the API version.</li><li> (array of objects) - List of compatibility sets.\n<ul><li> (int, optional) - Node affinity weight.</li><li> (string, optional) - Categorization tag.</li><li> (string, optional) - Short description.</li></ul></li></ul><p>An example might look like the following:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h3>Client implementation for node validation</h3><p>To streamline compatibility validation, we implemented a <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/reference/node-feature-client-reference.html\">client tool</a> that allows for node validation based on an image's compatibility artifact.\nIn this workflow, the image author would generate a compatibility artifact that points to the image it describes in a registry via the referrers API.\nWhen a need arises to assess the fit of an image to a host, the tool can discover the artifact and verify compatibility of an image to a node before deployment.\nThe client can validate nodes both inside and outside a Kubernetes cluster, extending the utility of the tool beyond the single Kubernetes use case.\nIn the future, image compatibility could play a crucial role in creating specific workload profiles based on image compatibility requirements, aiding in more efficient scheduling.\nAdditionally, it could potentially enable automatic node configuration to some extent, further optimizing resource allocation and ensuring seamless deployment of specialized workloads.</p><ol><li><p><strong>Define image compatibility metadata</strong>\nA <a href=\"https://kubernetes.io/docs/concepts/containers/images/\">container image</a> can have metadata that describes its requirements based on features discovered from nodes, like kernel modules or CPU models.\nThe previous compatibility specification example in this article exemplified this use case.</p></li><li><p><strong>Attach the artifact to the image</strong>\nThe image compatibility specification is stored as an OCI artifact.\nYou can attach this metadata to your container image using the <a href=\"https://oras.land/\">oras</a> tool.\nThe registry only needs to support OCI artifacts, support for arbitrary types is not required.\nKeep in mind that the container image and the artifact must be stored in the same registry.\nUse the following command to attach the artifact to the image:</p></li></ol><div><pre tabindex=\"0\"><code data-lang=\"bash\"></code></pre></div><ol start=\"3\"><li><strong>Validate image compatibility</strong>\nAfter attaching the compatibility specification, you can validate whether a node meets the image's requirements.\nThis validation can be done using the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/reference/node-feature-client-reference.html\">nfd client</a>:</li></ol><p><code>nfd compat validate-node --image &lt;image-url&gt;</code></p><ol start=\"4\"><li><strong>Read the output from the client</strong>\nFinally you can read the report generated by the tool or use your own tools to act based on the generated JSON report.</li></ol><p>The addition of image compatibility to Kubernetes through Node Feature Discovery underscores the growing importance of addressing compatibility in cloud native environments.\nIt is only a start, as further work is needed to integrate compatibility into scheduling of workloads within and outside of Kubernetes.\nHowever, by integrating this feature into Kubernetes, mission-critical workloads can now define and validate host OS requirements more efficiently.\nMoving forward, the adoption of compatibility metadata within Kubernetes ecosystems will significantly enhance the reliability and performance of specialized containerized applications, ensuring they meet the stringent requirements of industries like telecommunications, high-performance computing or any environment that requires special hardware or host OS configuration.</p><p>Join the <a href=\"https://kubernetes-sigs.github.io/node-feature-discovery/v0.17/contributing/\">Kubernetes Node Feature Discovery</a> project if you're interested in getting involved with the design and development of Image Compatibility API and tools.\nWe always welcome new contributors.</p>","contentLength":7628,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Changes to Kubernetes Slack","url":"https://kubernetes.io/blog/2025/06/16/changes-to-kubernetes-slack/","date":1750032000,"author":"","guid":740,"unread":true,"content":"<p>: We’ve received notice from Salesforce that our Slack workspace  on June 20th. Stand by for more details, but for now, there is no urgency to back up private channels or direct messages.</p><p>. Sometime later this year, our community may move to a new platform. If you are responsible for a channel or private channel, or a member of a User Group, you will need to take some actions as soon as you can.</p><p>For the last decade, Slack has supported our project with a free customized enterprise account. They have let us know that they can no longer do so, particularly since our Slack is one of the largest and more active ones on the platform. As such, they will be downgrading it to a standard free Slack while we decide on, and implement, other options.</p><p>On Friday, June 20, we will be subject to the <a href=\"https://slack.com/help/articles/27204752526611-Feature-limitations-on-the-free-version-of-Slack\">feature limitations of free Slack</a>. The primary ones which will affect us will be only retaining 90 days of history, and having to disable several apps and workflows which we are currently using. The Slack Admin team will do their best to manage these limitations.</p><p>Responsible channel owners, members of private channels, and members of User Groups should <a href=\"https://github.com/kubernetes/community/blob/master/communication/slack-migration-faq.md#what-actions-do-channel-owners-and-user-group-members-need-to-take-soon\">take some actions</a> to prepare for the upgrade and preserve information as soon as possible.</p><p>The CNCF Projects Staff have proposed that our community look at migrating to Discord. Because of existing issues where we have been pushing the limits of Slack, they have already explored what a Kubernetes Discord would look like. Discord would allow us to implement new tools and integrations which would help the community, such as GitHub group membership synchronization. The Steering Committee will discuss and decide on our future platform.</p>","contentLength":1684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Kubernetes Event Management with Custom Aggregation","url":"https://kubernetes.io/blog/2025/06/10/enhancing-kubernetes-event-management-custom-aggregation/","date":1749513600,"author":"","guid":739,"unread":true,"content":"<p>Kubernetes <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/\">Events</a> provide crucial insights into cluster operations, but as clusters grow, managing and analyzing these events becomes increasingly challenging. This blog post explores how to build custom event aggregation systems that help engineering teams better understand cluster behavior and troubleshoot issues more effectively.</p><h2>The challenge with Kubernetes events</h2><p>In a Kubernetes cluster, events are generated for various operations - from pod scheduling and container starts to volume mounts and network configurations. While these events are invaluable for debugging and monitoring, several challenges emerge in production environments:</p><ol><li>: Large clusters can generate thousands of events per minute</li><li>: Default event retention is limited to one hour</li><li>: Related events from different components are not automatically linked</li><li>: Events lack standardized severity or category classifications</li><li>: Similar events are not automatically grouped</li></ol><p>To learn more about Events in Kubernetes, read the <a href=\"https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/\">Event</a> API reference.</p><p>Consider a production environment with tens of microservices where the users report intermittent transaction failures:</p><p><strong>Traditional event aggregation process:</strong> Engineers are wasting hours sifting through thousands of standalone events spread across namespaces. By the time they look into it, the older events have long since purged, and correlating pod restarts to node-level issues is practically impossible.</p><p><strong>With its event aggregation in its custom events:</strong> The system groups events across resources, instantly surfacing correlation patterns such as volume mount timeouts before pod restarts. History indicates it occurred during past record traffic spikes, highlighting a storage scalability issue in minutes rather than hours.</p><p>The beneﬁt of this approach is that organizations that implement it commonly cut down their troubleshooting time significantly along with increasing the reliability of systems by detecting patterns early.</p><h2>Building an Event aggregation system</h2><p>This post explores how to build a custom event aggregation system that addresses these challenges, aligned to Kubernetes best practices. I've picked the Go programming language for my example.</p><p>This event aggregation system consists of three main components:</p><ol><li>: Monitors the Kubernetes API for new events</li><li>: Processes, categorizes, and correlates events</li><li>: Stores processed events for longer retention</li></ol><p>Here's a sketch for how to implement the event watcher:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h3>Event processing and classification</h3><p>The event processor enriches events with additional context and classification:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h3>Implementing Event correlation</h3><p>One of the key features you could implement is a way of correlating related Events.\nHere's an example correlation strategy:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h2>Event storage and retention</h2><p>For long-term storage and analysis, you'll probably want a backend that supports:</p><ul><li>Efficient querying of large event volumes</li><li>Flexible retention policies</li><li>Support for aggregation queries</li></ul><p>Here's a sample storage interface:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h2>Good practices for Event management</h2><ol><li><ul><li>Implement rate limiting for event processing</li><li>Use efficient filtering at the API server level</li><li>Batch events for storage operations</li></ul></li><li><ul><li>Distribute event processing across multiple workers</li><li>Use leader election for coordination</li><li>Implement backoff strategies for API rate limits</li></ul></li><li><ul><li>Handle API server disconnections gracefully</li><li>Buffer events during storage backend unavailability</li><li>Implement retry mechanisms with exponential backoff</li></ul></li></ol><p>Implement pattern detection to identify recurring issues:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>With this implementation, the system can identify recurring patterns such as node pressure events, pod scheduling failures, or networking issues that occur with a specific frequency.</p><p>The following example provides a starting point for building an alerting system based on event patterns. It is not a complete solution but a conceptual sketch to illustrate the approach.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>A well-designed event aggregation system can significantly improve cluster observability and troubleshooting capabilities. By implementing custom event processing, correlation, and storage, operators can better understand cluster behavior and respond to issues more effectively.</p><p>The solutions presented here can be extended and customized based on specific requirements while maintaining compatibility with the Kubernetes API and following best practices for scalability and reliability.</p><p>Future enhancements could include:</p><ul><li>Machine learning for anomaly detection</li><li>Integration with popular observability platforms</li><li>Custom event APIs for application-specific events</li><li>Enhanced visualization and reporting capabilities</li></ul>","contentLength":4496,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Gateway API Inference Extension","url":"https://kubernetes.io/blog/2025/06/05/introducing-gateway-api-inference-extension/","date":1749081600,"author":"","guid":738,"unread":true,"content":"<p>Modern generative AI and large language model (LLM) services create unique traffic-routing challenges\non Kubernetes. Unlike typical short-lived, stateless web requests, LLM inference sessions are often\nlong-running, resource-intensive, and partially stateful. For example, a single GPU-backed model server\nmay keep multiple inference sessions active and maintain in-memory token caches.</p><p>Traditional load balancers focused on HTTP path or round-robin lack the specialized capabilities needed\nfor these workloads. They also don’t account for model identity or request criticality (e.g., interactive\nchat vs. batch jobs). Organizations often patch together ad-hoc solutions, but a standardized approach\nis missing.</p><h2>Gateway API Inference Extension</h2><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">Gateway API Inference Extension</a> was created to address\nthis gap by building on the existing <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a>, adding inference-specific\nrouting capabilities while retaining the familiar model of Gateways and HTTPRoutes. By adding an inference\nextension to your existing gateway, you effectively transform it into an , enabling you to\nself-host GenAI/LLMs with a “model-as-a-service” mindset.</p><p>The project’s goal is to improve and standardize routing to inference workloads across the ecosystem. Key\nobjectives include enabling model-aware routing, supporting per-request criticalities, facilitating safe model\nroll-outs, and optimizing load balancing based on real-time model metrics. By achieving these, the project aims\nto reduce latency and improve accelerator (GPU) utilization for AI workloads.</p><p>The design introduces two new Custom Resources (CRDs) with distinct responsibilities, each aligning with a\nspecific user persona in the AI/ML serving workflow​:</p><ol><li><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencepool/\">InferencePool</a>\nDefines a pool of pods (model servers) running on shared compute (e.g., GPU nodes). The platform admin can\nconfigure how these pods are deployed, scaled, and balanced. An InferencePool ensures consistent resource\nusage and enforces platform-wide policies. An InferencePool is similar to a Service but specialized for AI/ML\nserving needs and aware of the model-serving protocol.</p></li><li><p><a href=\"https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencemodel/\">InferenceModel</a>\nA user-facing model endpoint managed by AI/ML owners. It maps a public name (e.g., \"gpt-4-chat\") to the actual\nmodel within an InferencePool. This lets workload owners specify which models (and optional fine-tuning) they\nwant served, plus a traffic-splitting or prioritization policy.</p></li></ol><p>In summary, the InferenceModel API lets AI/ML owners manage what is served, while the InferencePool lets platform\noperators manage where and how it’s served.</p><p>The flow of a request builds on the Gateway API model (Gateways and HTTPRoutes) with one or more extra inference-aware\nsteps (extensions) in the middle. Here’s a high-level example of the request flow with the\n<a href=\"https://gateway-api-inference-extension.sigs.k8s.io/#endpoint-selection-extension\">Endpoint Selection Extension (ESE)</a>:</p><ol><li><p>\nA client sends a request (e.g., an HTTP POST to /completions). The Gateway (like Envoy) examines the HTTPRoute\nand identifies the matching InferencePool backend.</p></li><li><p>\nInstead of simply forwarding to any available pod, the Gateway consults an inference-specific routing extension—\nthe Endpoint Selection Extension—to pick the best of the available pods. This extension examines live pod metrics\n(queue lengths, memory usage, loaded adapters) to choose the ideal pod for the request.</p></li><li><p><strong>Inference-Aware Scheduling</strong>\nThe chosen pod is the one that can handle the request with the lowest latency or highest efficiency, given the\nuser’s criticality or resource needs. The Gateway then forwards traffic to that specific pod.</p></li></ol><p>This extra step provides a smarter, model-aware routing mechanism that still feels like a normal single request to\nthe client. Additionally, the design is extensible—any Inference Gateway can be enhanced with additional inference-specific\nextensions to handle new routing strategies, advanced scheduling logic, or specialized hardware needs. As the project\ncontinues to grow, contributors are encouraged to develop new extensions that are fully compatible with the same underlying\nGateway API model, further expanding the possibilities for efficient and intelligent GenAI/LLM routing.</p><p>We evaluated ​this extension against a standard Kubernetes Service for a <a href=\"https://docs.vllm.ai/en/latest/\">vLLM</a>‐based model\nserving deployment. The test environment consisted of multiple H100 (80 GB) GPU pods running vLLM (<a href=\"https://blog.vllm.ai/2025/01/27/v1-alpha-release.html\">version 1</a>)\non a Kubernetes cluster, with 10 Llama2 model replicas. The <a href=\"https://github.com/AI-Hypercomputer/inference-benchmark\">Latency Profile Generator (LPG)</a>\ntool was used to generate traffic and measure throughput, latency, and other metrics. The\n<a href=\"https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\">ShareGPT</a>\ndataset served as the workload, and traffic was ramped from 100 Queries per Second (QPS) up to 1000 QPS.</p><ul><li><p>: Throughout the tested QPS range, the ESE delivered throughput roughly on par with a standard\nKubernetes Service.</p></li><li><ul><li>: The ​ESE showed significantly lower p90 latency at higher QPS (500+), indicating that\nits model-aware routing decisions reduce queueing and resource contention as GPU memory approaches saturation.</li><li>: Similar trends emerged, with the ​ESE reducing end‐to‐end tail latencies compared to the\nbaseline, particularly as traffic increased beyond 400–500 QPS.</li></ul></li></ul><p>These results suggest that this extension's model‐aware routing significantly reduced latency for GPU‐backed LLM\nworkloads. By dynamically selecting the least‐loaded or best‐performing model server, it avoids hotspots that can\nappear when using traditional load balancing methods for large, long‐running inference requests.</p><p>As the Gateway API Inference Extension heads toward GA, planned features include:</p><ol><li><strong>Prefix-cache aware load balancing</strong> for remote caches</li><li> for automated rollout</li><li> between workloads in the same criticality band</li><li> for scaling based on aggregate, per-model metrics</li><li><strong>Support for large multi-modal inputs/outputs</strong></li><li> (e.g., diffusion models)</li><li><strong>Heterogeneous accelerators</strong> (serving on multiple accelerator types with latency- and cost-aware load balancing)</li><li> for independently scaling pools</li></ol><p>By aligning model serving with Kubernetes-native tooling, Gateway API Inference Extension aims to simplify\nand standardize how AI/ML traffic is routed. With model-aware routing, criticality-based prioritization, and\nmore, it helps ops teams deliver the right LLM services to the right users—smoothly and efficiently.</p><p> Visit the <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/\">project docs</a> to dive deeper,\ngive an Inference Gateway extension a try with a few <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/guides/\">simple steps</a>,\nand <a href=\"https://gateway-api-inference-extension.sigs.k8s.io/contributing/\">get involved</a> if you’re interested in\ncontributing to the project!</p>","contentLength":6365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Start Sidecar First: How To Avoid Snags","url":"https://kubernetes.io/blog/2025/06/03/start-sidecar-first/","date":1748908800,"author":"","guid":737,"unread":true,"content":"<p>From the <a href=\"https://kubernetes.io/blog/2025/04/22/multi-container-pods-overview/\">Kubernetes Multicontainer Pods: An Overview blog post</a> you know what their job is, what are the main architectural patterns, and how they are implemented in Kubernetes. The main thing I’ll cover in this article is how to ensure that your sidecar containers start before the main app. It’s more complicated than you might think!</p><p>I'd just like to remind readers that the <a href=\"https://kubernetes.io/blog/2023/12/13/kubernetes-v1-29-release/\">v1.29.0 release of Kubernetes</a> added native support for\n<a href=\"https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/\">sidecar containers</a>, which can now be defined within the  field,\nbut with . You can see that illustrated in the following example Pod manifest snippet:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>What are the specifics of defining sidecars with a  block, rather than as a legacy multi-container pod with multiple ?\nWell, all  are always launched  the main application. If you define Kubernetes-native sidecars, those are terminated  the main application. Furthermore, when used with <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\">Jobs</a>, a sidecar container should still be alive and could potentially even restart after the owning Job is complete; Kubernetes-native sidecar containers do not block pod completion.</p><p>Now you know that defining a sidecar with this native approach will always start it before the main application. From the <a href=\"https://github.com/kubernetes/kubernetes/blob/537a602195efdc04cdf2cb0368792afad082d9fd/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L827-L830\">kubelet source code</a>, it's visible that this often means being started almost in parallel, and this is not always what an engineer wants to achieve. What I'm really interested in is whether I can delay the start of the main application until the sidecar is not just started, but fully running and ready to serve.\nIt might be a bit tricky because the problem with sidecars is there’s no obvious success signal, contrary to init containers - designed to run only for a specified period of time. With an init container, exit status 0 is unambiguously \"I succeeded\". With a sidecar, there are lots of points at which you can say \"a thing is running\".\nStarting one container only after the previous one is ready is part of a graceful deployment strategy, ensuring proper sequencing and stability during startup. It’s also actually how I’d expect sidecar containers to work as well, to cover the scenario where the main application is dependent on the sidecar. For example, it may happen that an app errors out if the sidecar isn’t available to serve requests (e.g., logging with DataDog). Sure, one could change the application code (and it would actually be the “best practice” solution), but sometimes they can’t - and this post focuses on this use case.</p><p>I'll explain some ways that you might try, and show you what approaches will really work.</p><p>To check whether Kubernetes native sidecar delays the start of the main application until the sidecar is ready, let’s simulate a short investigation. Firstly, I’ll simulate a sidecar container which will never be ready by implementing a readiness probe which will never succeed. As a reminder, a <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">readiness probe</a> checks if the container is ready to start accepting traffic and therefore, if the pod can be used as a backend for services.</p><p>(Unlike standard init containers, sidecar containers can have <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">probes</a> so that the kubelet can supervise the sidecar and intervene if there are problems. For example, restarting a sidecar container if it fails a health check.)</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><div><pre tabindex=\"0\"><code data-lang=\"console\"></code></pre></div><p>From these logs it’s evident that only one container is ready - and I know it can’t be the sidecar, because I’ve defined it so it’ll never be ready (you can also check container statuses in ). I also saw that myapp has been started before the sidecar is ready. That was not the result I wanted to achieve; in this case, the main app container has a hard dependency on its sidecar.</p><p>To ensure that the sidecar is ready before the main app container starts, I can define a . It will delay the start of the main container until the command is successfully executed (returns  exit status). If you’re wondering why I’ve added it to my , let’s analyse what happens If I’d added it to myapp container. I wouldn’t have guaranteed the probe would run before the main application code - and this one, can potentially error out without the sidecar being up and running.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>This results in 2/2 containers being ready and running, and from events, it can be inferred that the main application started only after nginx had already been started. But to confirm whether it waited for the sidecar readiness, let’s change the  to the exec type of command:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>and run  to watch in real time whether the readiness of both containers only changes after a 15 second delay. Again, events confirm the main application starts after the sidecar.\nThat means that using the  with a correct  request helps to delay the main application start until the sidecar is ready. It’s not optimal, but it works.</p><h2>What about the postStart lifecycle hook?</h2><p>Fun fact: using the  lifecycle hook block will also do the job, but I’d have to write my own mini-shell script, which is even less efficient.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>An interesting exercise would be to check the sidecar container behavior with a <a href=\"https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/\">liveness probe</a>.\nA liveness probe behaves and is configured similarly to a readiness probe - only with the difference that it doesn’t affect the readiness of the container but restarts it in case the probe fails.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>After adding the liveness probe configured just as the previous readiness probe and checking events of the pod by  it’s visible that the sidecar has a restart count above 0. Nevertheless, the main application is not restarted nor influenced at all, even though I'm aware that (in our imaginary worst-case scenario) it can error out when the sidecar is not there serving requests.\nWhat if I’d used a  without lifecycle ? Both containers will be immediately ready: at the beginning, this behavior will not be different from the one without any additional probes since the liveness probe doesn’t affect readiness at all. After a while, the sidecar will begin to restart itself, but it won’t influence the main container.</p><p>I’ll summarize the startup behavior in the table below:</p><table><thead><tr><th>Sidecar starts before the main app?</th><th>Main app waits for the sidecar to be ready?</th><th>What if the check doesn’t pass?</th></tr></thead><tbody><tr><td>, but it’s almost in parallel (effectively )</td><td>Sidecar is not ready; main app continues running</td></tr><tr><td>Yes, but it’s almost in parallel (effectively )</td><td>Sidecar is restarted, main app continues running</td></tr><tr></tr><tr><td>, main app container starts after  completes</td><td>, but you have to provide custom logic for that</td></tr></tbody></table><p>To summarize: with sidecars often being a dependency of the main application, you may want to delay the start of the latter until the sidecar is healthy.\nThe ideal pattern is to start both containers simultaneously and have the app container logic delay at all levels, but it’s not always possible. If that's what you need, you have to use the right kind of customization to the Pod definition. Thankfully, it’s nice and quick, and you have the recipe ready above.</p>","contentLength":6796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gateway API v1.3.0: Advancements in Request Mirroring, CORS, Gateway Merging, and Retry Budgets","url":"https://kubernetes.io/blog/2025/06/02/gateway-api-v1-3/","date":1748883600,"author":"","guid":736,"unread":true,"content":"<p>Join us in the Kubernetes SIG Network community in celebrating the general\navailability of <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a> v1.3.0! We are\nalso pleased to announce that there are already a number of conformant\nimplementations to try, made possible by postponing this blog\nannouncement. Version 1.3.0 of the API was released about a month ago on\nApril 24, 2025.</p><p>Gateway API v1.3.0 brings a new feature to the  channel\n(Gateway API's GA release channel): <em>percentage-based request mirroring</em>, and\nintroduces three new experimental features: cross-origin resource sharing (CORS)\nfilters, a standardized mechanism for listener and gateway merging, and retry\nbudgets.</p><h2>Graduation to Standard channel</h2><p>Graduation to the Standard channel is a notable achievement for Gateway API\nfeatures, as inclusion in the Standard release channel denotes a high level of\nconfidence in the API surface and provides guarantees of backward compatibility.\nOf course, as with any other Kubernetes API, Standard channel features can continue\nto evolve with backward-compatible additions over time, and we (SIG Network)\ncertainly expect\nfurther refinements and improvements in the future. For more information on how\nall of this works, refer to the <a href=\"https://gateway-api.sigs.k8s.io/concepts/versioning/\">Gateway API Versioning Policy</a>.</p><h3>Percentage-based request mirroring</h3><p><em>Percentage-based request mirroring</em> is an enhancement to the\nexisting support for <a href=\"https://gateway-api.sigs.k8s.io/guides/http-request-mirroring/\">HTTP request mirroring</a>, which allows HTTP requests to be duplicated to another backend using the\nRequestMirror filter type. Request mirroring is particularly useful in\nblue-green deployment. It can be used to assess the impact of request scaling on\napplication performance without impacting responses to clients.</p><p>The previous mirroring capability worked on all the requests to a .\nPercentage-based request mirroring allows users to specify a subset of requests\nthey want to be mirrored, either by percentage or fraction. This can be\nparticularly useful when services are receiving a large volume of requests.\nInstead of mirroring all of those requests, this new feature can be used to\nmirror a smaller subset of them.</p><p>Here's an example with 42% of the requests to \"foo-v1\" being mirrored to \"foo-v2\":</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>You can also configure the partial mirroring using a fraction. Here is an example\nwith 5 out of every 1000 requests to \"foo-v1\" being mirrored to \"foo-v2\".</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><h2>Additions to Experimental channel</h2><p>The Experimental channel is Gateway API's channel for experimenting with new\nfeatures and gaining confidence with them before allowing them to graduate to\nstandard. Please note: the experimental channel may include features that are\nchanged or removed later.</p><p>Starting in release v1.3.0, in an effort to distinguish Experimental channel\nresources from Standard channel resources, any new experimental API kinds have the\nprefix \"\". For the same reason, experimental resources are now added to the\nAPI group <code>gateway.networking.x-k8s.io</code> instead of <code>gateway.networking.k8s.io</code>.\nBear in mind that using new experimental channel resources means they can coexist\nwith standard channel resources, but migrating these resources to the standard\nchannel will require recreating them with the standard channel names and API\ngroup (both of which lack the \"x-k8s\" designator or \"X\" prefix).</p><p>The v1.3 release introduces two new experimental API kinds: XBackendTrafficPolicy\nand XListenerSet. To be able to use experimental API kinds, you need to install\nthe Experimental channel Gateway API YAMLs from the locations listed below.</p><p>Cross-origin resource sharing (CORS) is an HTTP-header based mechanism that allows\na web page to access restricted resources from a server on an origin (domain,\nscheme, or port) different from the domain that served the web page. This feature\nadds a new HTTPRoute  type, called \"CORS\", to configure the handling of\ncross-origin requests before the response is sent back to the client.</p><p>Here's an example of a simple cross-origin configuration:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>In this case, the Gateway returns an  of \"*\", which means that the\nrequested resource can be referenced from any origin, a \n(<code>Access-Control-Allow-Methods</code>) that permits the , , and \nverbs, and a  allowing , ,\n, , and .</p><div><pre tabindex=\"0\"><code data-lang=\"text\"></code></pre></div><p>The complete list of fields in the new CORS filter:</p><ul></ul><h3>XListenerSets (standardized mechanism for Listener and Gateway merging)</h3><p>This release adds a new experimental API kind, XListenerSet, that allows a\nshared list of  to be attached to one or more parent Gateway(s). In\naddition, it expands upon the existing suggestion that Gateway API implementations\nmay merge configuration from multiple Gateway objects. It also:</p><ul><li>adds a new field  to the  of a Gateway. The\n field defines from which Namespaces to select XListenerSets\nthat are allowed to attach to that Gateway: Same, All, None, or Selector based.</li><li>increases the previous maximum number (64) of listeners with the addition of\nXListenerSets.</li><li>allows the delegation of listener configuration, such as TLS, to applications in\nother namespaces.</li></ul><p>The following example shows a Gateway with an HTTP listener and two child HTTPS\nXListenerSets with unique hostnames and certificates. The combined set of listeners\nattached to the Gateway includes the two additional HTTPS listeners in the\nXListenerSets that attach to the Gateway. This example illustrates the\ndelegation of listener TLS config to application owners in different namespaces\n(\"store\" and \"app\"). The HTTPRoute has both the Gateway listener named \"foo\" and\none XListenerSet listener named \"second\" as .</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Each listener in a Gateway must have a unique combination of , ,\n(and  if supported by the protocol) in order for all listeners to be\n and not conflicted over which traffic they should receive.</p><p>Furthermore, implementations can  separate Gateways into a single set of\nlistener addresses if all listeners across those Gateways are compatible. The\nmanagement of merged listeners was under-specified in releases prior to v1.3.0.</p><p>With the new feature, the specification on merging is expanded. Implementations\nmust treat the parent Gateways as having the merged list of all listeners from\nitself and from attached XListenerSets, and validation of this list of listeners\nmust behave the same as if the list were part of a single Gateway. Within a single\nGateway, listeners are ordered using the following precedence:</p><ol><li>Single Listeners (not a part of an XListenerSet) first,</li><li>Remaining listeners ordered by:\n<ul><li>object creation time (oldest first), and if two listeners are defined in\nobjects that have the same timestamp, then</li><li>alphabetically based on \"{namespace}/{name of listener}\"</li></ul></li></ol><h3>Retry budgets (XBackendTrafficPolicy)</h3><p>This feature allows you to configure a  across all endpoints\nof a destination Service. This is used to limit additional client-side retries\nafter reaching a configured threshold. When configuring the budget, the maximum\npercentage of active requests that may consist of retries may be specified, as well as\nthe interval over which requests will be considered when calculating the threshold\nfor retries. The development of this specification changed the existing\nexperimental API kind BackendLBPolicy into a new experimental API kind,\nXBackendTrafficPolicy, in the interest of reducing the proliferation of policy\nresources that had commonalities.</p><p>The following example shows an XBackendTrafficPolicy that applies a\n that represents a budget that limits the retries to a maximum\nof 20% of requests, over a duration of 10 seconds, and to a minimum of 3 retries\nover 1 second.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Unlike other Kubernetes APIs, you don't need to upgrade to the latest version of\nKubernetes to get the latest version of Gateway API. As long as you're running\nKubernetes 1.26 or later, you'll be able to get up and running with this version\nof Gateway API.</p><p>To try out the API, follow the <a href=\"https://gateway-api.sigs.k8s.io/guides/\">Getting Started Guide</a>.\nAs of this writing, four implementations are already conformant with Gateway API\nv1.3 experimental channel features. In alphabetical order:</p><p>Wondering when a feature will be added? There are lots of opportunities to get\ninvolved and help define the future of Kubernetes routing APIs for both ingress\nand service mesh.</p><p>The maintainers would like to thank  who's contributed to Gateway\nAPI, whether in the form of commits to the repo, discussion, ideas, or general\nsupport. We could never have made this kind of progress without the support of\nthis dedicated and active community.</p><h2>Related Kubernetes blog articles</h2>","contentLength":8284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}