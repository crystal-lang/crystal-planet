{"id":"F7pXcmPnWkLEHkvQBb3zXkgdiHy14GQcP2RQp","title":"Aphyr: Posts","displayTitle":"Dev - Aphyr","url":"http://aphyr.com/posts.atom","feedLink":"http://aphyr.com/posts.atom","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":1,"items":[{"title":"The Future of Comments is Lies, I Guess","url":"https://aphyr.com/posts/388-the-future-of-comments-is-lies-i-guess","date":1748540176,"author":"Aphyr","guid":309,"unread":true,"content":"<p>I’ve been involved in content moderation since roughly 2004. I’ve built spam prevention for corporate and personal e-mail, moderated open-source mailing lists and IRC channels, worked at a couple social media networks, and help moderate a Mastodon instance for a few hundred people. In the last few years I’ve wasted more time fighting blog comment spam, and I’m pretty sure Large Language Models (LLMs) are to blame.</p><p>I think of spam as a space with multiple equilibria. Producing spam takes work. Spammers are willing to invest that work because each message has a small chance to make money, or achieve political or emotional goals. Some spam, like the endless identical Viagra scams in my email spam folder, or the PHPBB comment spam I filter out here on aphyr.com, is cheap to generate and easy to catch. I assume the spammers make it up in volume. Other spam, like <a href=\"https://www.ibm.com/think/topics/spear-phishing\">spear phishing attacks</a>, is highly time-consuming: the spammer must identify a target, carefully craft a plausible message using, say, the identity of the target’s co-workers, or construct a facade of a bank’s log-in page, and so on. This kind of spam is more likely to make it through filters, but because it takes a lot of human work, is generally only worth it for high-value targets.</p><p>LLMs seem to be changing these equilibria. Over the last year I’ve seen a new class of comment spam, using what I’m fairly sure is LLM-generated text. These comments make specific, plausible remarks about the contents of posts and images, and work in a link to some web site or mention a product. Take this one I caught a few months back:</p><blockquote><p>\"Walking down a sidewalk lined with vibrant flowers is one of life’s simple joys! It reminds me of playing the [link redacted] slope game, where you have to navigate through colorful landscapes while dodging obstacles.</p></blockquote><p>Before 2023, you’d likely have paid a human a few cents to write and post that. Now, thanks to LLMs, this sort of thing is trivially automated. The model will happily fabricate relatable personal experiences in service of a spam campaign:</p><blockquote><p>That photo reminds me of the first time I tried macro photography in my backyard. I spent an hour trying to get a clear shot of a red flower, experimenting with angles and lighting. It was so much fun discovering the little details up close! If you ever need a break from photography, I recommend playing Snow Rider 3D for a bit of quick, light-hearted fun.</p></blockquote><p>Other spam seems to glue together LLM remarks with what I think is a hand-written snippet of ad copy. Note the abrupt shift in grammar, diction, and specificity.</p><blockquote><p>This piece masterfully blends technical depth with mythological storytelling, transforming a standard Haskell programming interview into an epic narrative. It cleverly critiques the complexity and absurdity of some technical interviews by illustrating how type-level Haskell can be pushed to esoteric extremes beautiful, powerful, and largely impractical. A fascinating and relevant read for anyone interested in the intersection of programming, language design, and narrative. I’m James Maicle, working at Cryptoairhub where we run a clear and insightful crypto blog. I’ll be bookmarking your site and following the updates. Glad to see so much valuable information shared here looking forward to exploring more strategies together. Thanks for sharing. If you interest about Crypto please visit my website and read my article [link redacted] Crypto Blog.</p></blockquote><p>The same thing is happening on aggregators like Hacker News, where commenters post more-or-less-obviously LLM output for… I’m not sure, exactly. Karma? Weirder still are bots like <a href=\"https://bsky.app/profile/hackerbriefs.bsky.social\">Hacker Briefs</a>, which I suspect use an LLM to summarize trending HN posts. Here’s <a href=\"https://bsky.app/profile/hackerbriefs.bsky.social/post/3lnz4kirkpv25\">its summary of a recent article I wrote</a>:</p><blockquote><p>“Jepsen: Amazon RDS for PostgreSQL 17.4”</p><p>New multi-AZ clusters in Amazon RDS for PostgreSQL offer better failure recovery but may return outdated data when reading after writes. Caution is needed.</p></blockquote><p>This is a totally plausible summary of <a href=\"https://jepsen.io/analyses/amazon-rds-for-postgresql-17.4\">the article</a>, and it is utterly, laughably wrong. Multi-AZ clusters are not new, and they do not return outdated data when reading after writes. As the abstract succinctly explains, they allow Long Fork, a different anomaly which does not involve real-time orders at all. The bot ignored the actual problem and invented a different one. This sort of spam isn’t obviously motivated by commercial interest, but it is nevertheless depressing: one more drop in the misinformation slurry.</p><p>Of course this is not news. Product reviews are inundated with LLM slop, as are social media networks. LLMs allow for cheap, fast, and automated generation of unique spam which is difficult for machines and humans to identify. The cost falls on me and other moderators, who must sift through LLM bullshit trying to sieve “awkward but sincere human” from “automated attack”. Thanks to OpenAI et al I read more spam, and each message takes longer to check.</p><p>This problem is only going to get worse as LLMs improve and spammers develop more sophisticated ways to use them. In recent weeks I’ve received vague voice messages from strangers with uncanny speech patterns just asking to catch up—a sentence which, had I uttered it prior to 2023, would have been reasonably interpreted as a sign of psychosis. I assume these too are LLM-generated scams, perhaps a <a href=\"https://dfpi.ca.gov/wp-content/uploads/2025/03/Pig-Butchering-Scam-Playbook.pdf\">pig butchering</a> scheme. So far these are from strangers, but it’s not hard to imagine an attacker using text and voice synthesis to impersonate a friend, colleague, or lover in a detailed conversation. Or one’s doctor, or bank.</p><p>As the cost of generating slop decreases, it’s easy to imagine LLMs generating personae, correspondence, even months-long relationships with real humans before being deployed for commercial or political purposes. <a href=\"https://www.techtransparencyproject.org/articles/for-sale-on-facebook-accounts-that-can-run-u.s.-election-ads\">Creating plausible accounts and selling them</a> has been a successful business model in social media for some time; likewise, we have strong clues that <a href=\"https://arxiv.org/pdf/2307.16336\">LLMs are already used for social media bots</a>. Social networks have responded to these attacks via out-of-band mechanisms: IP reputation analysis, javascript and mobile app fingerprinting, statistical correlation across multiple accounts, and so on. I’m not sure how to translate these defenses to less centralized and more privacy-oriented networks, like email or blog spam. I strongly suspect the only reason Mastodon hasn’t been eaten alive by LLM spambots is because we’re just not big enough to be lucrative. But those economics are shifting, and even obscure ecological niches can be worth filling.</p><p>As a moderator, that keeps me up at night.</p>","contentLength":6585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}