{"id":"4RDcb1QCVpCZpB54ijAYcT1W2","title":"Recent Discussions","displayTitle":"Recent Discussions","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Kemal now uses a LRU Cache for Faster Routing Performance","url":"https://old.reddit.com/r/crystal_programming/comments/1olhfag/kemal_now_uses_a_lru_cache_for_faster_routing/","date":1761980645,"author":"/u/sdogruyol","guid":52,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Annother Spinoff Shard: ralsina/rate_limiter","url":"https://forum.crystal-lang.org/t/annother-spinoff-shard-ralsina-rate-limiter/8525","date":1761949159,"author":"ralsina","guid":37,"unread":true,"content":"<p>I am sorry, I know I create a lot of topics all at once, but it happens when I actually have some time to work on my projects <img src=\"https://emoji.discourse-cdn.com/google_classic/smiley.png?v=15\" title=\":smiley:\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"></p><p>Basically, it’s a generic in-memory sliding window composible rate limiter. Usage looks like this:</p><pre data-code-wrap=\"crystal\"><code># Per user rate limiting\nuser_limiter = RateLimiter.new(50, 3600)  # 50 requests per hour per user\n\n# Per IP rate limiting\nip_limiter = RateLimiter.new(200, 3600)  # 200 requests per hour per IP\n\n# Per endpoint rate limiting\nendpoint_limiter = RateLimiter.new(20, 60)  # 20 requests per minute per endpoint\n\n# Per user + endpoint rate limiting\nuser_endpoint_limiter = RateLimiter.new(10, 60)  # 10 requests per minute per user+endpoint\n\n# Check request (example: user trying to access API)\nusername = \"alice\"\nip = \"10.0.0.1\"\nendpoint = \"/api/create_note\"\n\n# Check all applicable rate limits\nlimits = [\n  user_limiter.allow?(username),\n  ip_limiter.allow?(ip),\n  endpoint_limiter.allow?(endpoint),\n  user_endpoint_limiter.allow?(\"#{username}::#{endpoint}\")\n]\n\nif limits.any?\n  # Request is allowed by all rate limiters\n  process_request(username, ip, endpoint)\nelse\n  # Request exceeds at least one rate limit\n  render_error(\"Rate limit exceeded\")\nend\n</code></pre>","contentLength":1177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Some performance testing","url":"https://forum.crystal-lang.org/t/some-performance-testing/8524","date":1761941070,"author":"ralsina","guid":36,"unread":true,"content":"<p>As I worked on <a href=\"https://tocry.ralsina.me\" rel=\"noopener nofollow ugc\">ToCry</a> I got curious about how well it performed.</p><p>It was  built with performance as the main goal. In fact it’s sort of guaranteed not to be the fastest way to do things. It uses <a href=\"https://github.com/ralsina/sepia\" rel=\"noopener nofollow ugc\">sepia</a> as data storage, which is the opposite of efficient, and I implemented it .</p><p>On the other hand, the best thing was to just write some scripts and measure things!</p><p>Turns out … it’s pretty fast? It can have about 20 simultaneous users and 300 RPS on a Pi 4 with 4GB of RAM, and industry standard says that equates to about 200 users.</p><p>200 users? On an old SBC!</p><p> This is not scientific. What a “realistic” load is needs to be discovered by having actual users :-)</p>","contentLength":661,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Second fiber does not complete","url":"https://forum.crystal-lang.org/t/second-fiber-does-not-complete/8523","date":1761929399,"author":"axd99","guid":35,"unread":true,"content":"<p>Hi, I am experimenting with crystal’s fibers and to test with a buffered channel I wrote this:</p><pre><code># generate a list of file names.\n# This is the data source.\nlist = [] of String\n(1..4).each do |i|\n  list &lt;&lt; \"file#{i}\"\nend\n\n# provides communication between the producing fiber and the\n# consuming one.\nch = Channel(String).new(3)\n\n# fiber1\n# this fiber loads the data into the channel\n# This is the producer.\nspawn do\n  puts \"--- Entering fiber1\"\n  list.each do |val|\n    puts \"fiber1, before send #{val}\"\n    ch.send val\n    puts \"fiber1, after send #{val}\"\n  end\n  puts \"--- Exiting fiber1\"\nend\n\n# fiber2\n# this fiber empties the channel\n# This is the consumer.\nspawn do\n  puts \"---- Entering fiber 2\"\n  while val2 = ch.receive\n    puts \"Received: #{val2}\"\n  end\n  puts \"---------------------\"   # The code does not get to here\n  puts \"---- Exiting fiber2\"\n  puts \"---------------------\"\nend\n\nputs \"Starting...\\n\"\n\n# Start the fibers\nFiber.yield\n\n# When the control gets back here all data has been exhausted.\nputs \"Goodbye!!!\"\n\n# shouldn't either of these 2 restart fiber2?\nFiber.yield\nch.close \np ch.closed?\n\n\n</code></pre><p>Now, when I run the above, all data is produced and consumed as expected.\nHowever the ‘---- Exiting Fiber 2’ never executed.</p><pre><code>$ crystal list-files.cr\nStarting...\n--- Entering fiber1\nfiber1, before send file1\nfiber1, after send file1\nfiber1, before send file2\nfiber1, after send file2\nfiber1, before send file3\nfiber1, after send file3\nfiber1, before send file4\n---- Entering fiber 2\nReceived: file1\nReceived: file2\nReceived: file3\nReceived: file4\nfiber1, after send file4\n--- Exiting fiber1\nGoodbye!!!\ntrue\n</code></pre><p>Any pointer would be most welcome.\nMany thanks.</p>","contentLength":1667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wanna do an MCP? Try this :-)","url":"https://forum.crystal-lang.org/t/wanna-do-an-mcp-try-this/8520","date":1761767213,"author":"ralsina","guid":34,"unread":true,"content":"<p><a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\" rel=\"noopener nofollow ugc\">MCP</a> is a mechanism to expose things to AI agents.</p><p>Turns out it’s super easy to do, and it gives you some amazing leverage for tools! For example, I gave my <a href=\"https://tocry.ralsina.me\" rel=\"noopener nofollow ugc\">kanban board</a> a MCP server and now I can just say “move the note “whatever” to done” and it works ;-)</p><p>It supports stdio MCP servers (nice way to expose a CLI tool to AI agents) and web MCP servers (with kemal at least)</p><p>Here is a FULL example of a MCP with a single tool, in stdio mode:</p><pre data-code-wrap=\"crystal\"><code>require \"mcp\"\n\n# A simple tool that returns the answer to any question\nclass AnswerTool &lt; MCP::AbstractTool\n  @@tool_name = \"get_answer\"\n  @@tool_description = \"Returns 42 as the answer to any question you ask\"\n  @@tool_input_schema = {\n    \"type\"       =&gt; \"object\",\n    \"properties\" =&gt; {\n      \"question\" =&gt; {\n        \"type\"        =&gt; \"string\",\n        \"description\" =&gt; \"The question you want answered\",\n      },\n    },\n    \"required\" =&gt; [\"question\"],\n  }.to_json\n\n  def invoke(params : Hash(String, JSON::Any), env : HTTP::Server::Context? = nil)\n    question = params[\"question\"]?.try(&amp;.as_s) || \"unknown question\"\n    {\n      \"answer\"   =&gt; 42,\n      \"question\" =&gt; question,\n    }\n  end\nend\n\n# Start the stdio server - that's it! One line and you have a complete MCP server.\nMCP::StdioHandler.start_server\n</code></pre><p>Have fun and let me know if something is not ergnomic, I am trying to make this as boilerplate-free as possible.</p><p><small>11 posts - 5 participants</small></p>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Airsailer - open source Cloud orchestrator in Crystal","url":"https://forum.crystal-lang.org/t/airsailer-open-source-cloud-orchestrator-in-crystal/8516","date":1761731144,"author":"paulocoghi","guid":33,"unread":true,"content":"<p>Missed you all during my cancer treatment, and I’m finally cancer-free!</p><p>I’m back full-time at my cloud computing startup, and going fully open source (MIT license) for the entire platform.</p><ul><li>LXC (Linux/system containers) for bare-metal performance, no VM overhead (PoC working)</li><li>elastic bare-metal hardware resources that can be changed without reboot</li><li>elastic LVM storage with thin-provisioning (manually handled today)</li><li>routing for local networks + WireGuard for clusters (manually handled today)</li><li><a href=\"https://blog.cloudflare.com/workerd-open-source-workers-runtime/#homogeneous-deployment\" rel=\"noopener nofollow ugc\">Homogeneous deployment</a> as the strategy for automatic horizontal scaling and load-balancing (will be on a future release with PaaS automation)</li><li>automatic proxy + SSL for apps, no need for 1 public IP per app (<a href=\"https://github.com/admiracloud/narnia\" rel=\"noopener nofollow ugc\">proxy lib</a> temporarily made in JS because of acme-client lib)</li></ul><p> IaaS+PaaS with 20% of features that cover 80% of cloud demand.</p><p>I’m refactoring and I will release soon at <a href=\"https://github.com/airsailer/airsailer\" rel=\"noopener nofollow ugc\">github.com/airsailer/airsailer</a>. Since I sustain my family only through this work, I expect to release a new production version soon.</p><p>Happy to talk to you again. <img src=\"https://emoji.discourse-cdn.com/google_classic/slightly_smiling_face.png?v=15\" title=\":slightly_smiling_face:\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>","contentLength":1026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["community"]}